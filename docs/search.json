{
  "articles": [
    {
      "path": "about.html",
      "title": "Alex Stephenson",
      "author": [],
      "contents": "\n\n          \n          \n          Methods Tutoring\n          \n          \n          Home\n          Creator\n          \n          \n          Lecture Notes\n           \n          ▾\n          \n          \n          Calculus\n          Causal Inference\n          Game Theory\n          Linear Algebra\n          Machine Learning\n          Probability\n          Statistics\n          Survey Design and Sampling\n          \n          \n          Math Camp Solutions\n          \n          \n          Coding\n           \n          ▾\n          \n          \n          Python\n          R\n          Applied Linear Algebra\n          \n          \n          ☰\n          \n          \n      \n        \n          \n            Alex Stephenson\n          \n          \n            \n              I am a PhD Candidate at the University of California,\n              Berkeley.\n            \n            \n              I am a PhD Candidate at the University of California,\n              Berkeley.\n            \n          \n\n          \n            \n              \n                  \n                    \n                      Github\n                    \n                  \n                \n                                \n                  \n                    \n                      Twitter\n                    \n                  \n                \n                                \n                  \n                    \n                      My Website\n                    \n                  \n                \n                              \n          \n\n          \n            \n              \n                                \n                  \n                    Github\n                  \n                \n                                \n                  \n                    Twitter\n                  \n                \n                                \n                  \n                    My Website\n                  \n                \n                              \n            \n          \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2023-05-24T22:02:40-07:00"
    },
    {
      "path": "causal.html",
      "title": "Causal Inference",
      "description": "Notes on Causal Inference.\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2023-05-24T22:02:41-07:00"
    },
    {
      "path": "game.html",
      "title": "Game Theory",
      "description": "Notes on Game Theory.\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2023-05-24T22:02:42-07:00"
    },
    {
      "path": "index.html",
      "title": "Methods Tutoring",
      "description": "A depository of quantitative methods notes\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\nWelcome to the Methods Tutoring Notes site. I put this together based on my methods training and years being a graduate quantitative methods tutor at UC Berkeley.\nThe site contains several different resources. First and foremost, it includes notes on the primary quantitative methods topics common to a graduate course sequence in political science. Second, there are coding notes for both R and Python.1I have endeavored to provide an introduction to both languages, focusing on topics that have proved more difficult for graduate students at UC Berkeley. Third, I provide code for standard research techniques and methods currently in use in top political science journals. For pedagogical reasons, every single example and application on this website has both an R and a Python implementation.\nTexts\nI have benefited greatly in my own methods education from the teaching of professors at Berkeley. All errors on this website are entirely my own and should in no way reflect their excellent guidance.\nIn addition to classroom teaching and tutoring, the underlying materials on this website are drawn from many resources. Below is a non-exhaustive list of books from which these materials are drawn by subject. I encourage individuals who reach this page for self-study to consult them for more detail. I have put in bold the text or sets of texts that strike me as the most reasonable entry point in each category.\nCalculus\nSimon, Carl P., and Lawrence E. Blume. 1994. Mathematics for Economists. 1. ed. New York: Norton.\nStewart, J. 2016. Calculus: Early Transcendentals. 8th ed. Boston, MA, USA: Cengage Learning.\nCausal Inference\nAngrist, J, & Pischke J.S. 2009. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton: Princeton University Press.\nAngrist, J., & Pischke J.S. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton: Princeton University Press.\nCunningham, S. 2021. Causal Inference: The Mixtape. New Haven: Yale University Press. https://mixtape.scunning.com/\nDunning, T. 2012. Natural Experiments in the Social Sciences: A Design-Based Approach. Cambridge: Cambridge University Press.\nGerber, Alan S, and Donald P Green. 2012. Field Experiments: Design, Analysis and Interpretation. New York: W.W. Norton & Co.\nImbens, G., & Rubin, D. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge: Cambridge University Press.\nHernán, M.A, & Robins,J.A.. 2021. Causal Inference: What If. Boca Raton: Chapman & Hall/CRC Press\nMorgan, S.L., & Winship, C. 2014. Counterfactuals and Causal Inference: Methods and Principles for Social Research. 2nd ed. Cambridge: Cambridge University Press.\nRosenbaum, P.R., 2020. Design of Observational Studies. Cham: Springer International Publishing.\nGame Theory\nFudenberg, Drew, and Jean Tirole. 1991. Game Theory. Cambridge: MIT Press.\nGehlbach, S. 2013. Formal Models of Domestic Politics. Cambridge: Cambridge University Press.\nMcCarty, N., & Meirowitz, A. 2007. Political Game Theory: An Introduction. Cambridge: Cambridge University Press.\nTadelis, Steve. 2013. Game Theory: An Introduction. Princeton; Oxford: Princeton University Press.\nLinear Algebra\nBoyd, S., & Vandenberghe, L. 2018. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares. 1st ed. Cambridge University Press. https://web.stanford.edu/~boyd/vmls/\nHarville, D.A., 1997. Matrix Algebra from a Statistician’s Perspective. New York: Springer.\nMachine Learning\nGoodfellow, I., Bengio, Y. & Courville, A. 2016. Deep Learning. Cambridge: MIT Press.\nHastie, T., Tibshirani, R., & Friedman, J. 2013. Elements of Statistical Learning. 2nd ed. Springer.\nJames, G., Witten, D., Hastie, T, & Tibshirani, R. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. New York: Springer. https://www.statlearning.com/\nKneusel, R. T., 2021. Practical Deep Learning. No Starch Press.\nMurphy, K. 2020. Probablistic Machine Learning. MIT Press.\nZhang, Aston, Zachary C Lipton, Mu Li, and Alexander J Smola. 2022. Dive into Deep Learning.\nMath Prefreshers\nGill, J. 2006. Essential Mathematics for Political and Social Research. Cambridge: Cambridge University Press.\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\nProbability\nPitman, J. 1993. Probability. New York, NY: Springer New York.\nStatistics\nAronow, P., & Miller, B. 2019. Foundations of Agnostic Statistics. 1st ed. Cambridge University Press.\nCasella, G., & Berger, R. 2002. Statistical Inference. 2nd ed. Pacific Grove: Duxbury.\nDavidson, R., & MacKinnon, J.G. 2004. Econometric Theory and Methods. New York: Oxford University Press.\nFreedman, D. (2009). Statistical Models: Theory and Practice (2nd ed.). Cambridge: Cambridge University Press. doi:10.1017/CBO9780511815867\nFreedman, David, Robert Pisani, and Roger Purves. 2007. Statistics. 4th ed. New York: W.W. Norton & Co.\nGailmard, S. 2014. Statistical Modeling and Inference for Social Science: 1st ed. Cambridge University Press.\nGreene, W. 2017. Econometric Analysis. 8th ed. Upper Saddle River: Prentice Hall.\nWooldridge, J. M. 2010. Econometric Analysis of Cross Section and Panel Data. 2nd ed. Cambridge: MIT Press.\nWooldridge, J. M. 2016. Introductory Econometrics: A Modern Approach. 7th ed. Adrian MI: South-Western Cengage Learning.\nSurvey Design and Sampling\nLohr, S. 2021. Sampling: Design and Analysis. 3rd ed. Boca Raton: Chapman and Hall/CRC.\nThompson, Steven K. 2012. Sampling. 3rd ed. Hoboken, N.J: Wiley.\n\nThere are no resources for Stata by choice. Stata is not free software, rarely used in industry, and not needed to do research.↩︎\n",
      "last_modified": "2023-05-24T22:02:43-07:00"
    },
    {
      "path": "intro-seq.html",
      "title": "Math Camp Pre-track Solutions",
      "description": "A set of solutions for UC Berkeley's Math Camp Pre-track\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nSelf-Guided Summer Methods Program\nLinear Algebra Sequence\nM+S Chapter 1: Preliminaries\nM + S Chapter 2: Arithmetic Review\nM+S Chapter 12: Vectors and Matrices\nM+S Chapter 13: Vector Spaces and Systems of Equations\n\nCalculus Sequence\nM+S Chapter 3: Functions\nM+S Chapter 4: Limits and Continuity, Sequences and Series, and More on Sets\nM+S Chapter 5: Introduction to Derivatives\nM+S Chapter 6: Rules of Derivatives\nM+S Chapter 7: Integrals\nM+S Chapter 8: Extrema in One Dimension\nM+S Chapter 15: Multivariate Calculus\n\nRandom Variables and Probability Sequence\nM+S Chapter 1: Preliminaries\nFPP Chapter 4,6\nFPP Chapter 13, 14\nM+S Chapter 9: Introduction to Probability\nM+S Chapter 10: Introduction to Discrete Distributions\nFPP Chapter 8, 9\nM+S Chapter 11: Continuous Distributions\nFPP Chapter 10,11,12,16,17\n\nR Sequence\n\nSelf-Guided Summer Methods Program\nThe self-guided summer methods program is based on a suggested syllabus for preparing for the Political Science program at UC Berkeley’s formal and quantitative methods course. The purpose of the self-guided course is to help students prepare for the summer math camp and the introductory courses. The primary books for the self-guided course are A Mathematics Course for Political and Social Research herein dubbed M+S and Statistics: Fourth Edition herein dubbed as FPP.\nLinear Algebra Sequence\nThe Linear Algebra sequence suggested timeline is 2-4 weeks. It is entirely composed of chapters from M+S and comprises mathematical preliminaries, vectors and matrices, and vector spaces and systems of equation.\nM+S Chapter 1: Preliminaries\nExercises\nExercise 1\nParty identification of delegates at a political convention is a constant\nWar participation of the Great Powers is a variable.\nVoting record of members of Congress relative to the stated position of the president is a variable.\nRevolutions in France, Russia, China, Iran, and Nicaragua is a variable.\nAn individual voter’s vote choice in the 1992 presidential election is a constant.\nAn individual voter’s vote choice in the 1960-1992 presidential elections is a variable.\nVote choice in the 1992 presidential election is a variable.\nExercise 4\n\n\n\nExercise 5\nLet \\(A = \\{1,5,10\\}\\) and \\(B = \\{1,2,...,10\\}\\)\n\\(A \\subset B\\) is true. All elements of A are contained in B. The reverse is not true. For example \\(3 \\not\\subset A\\) but is in \\(B\\).\n\\(A \\cup B = \\{1,2,3,4,5,6,7,8,9,10\\}\\) because A is entirely contained within B, so the union of the two sets is just the set B itself.\n\\(A \\cap B = A\\) because A is entirely contained within B, so the intersection between the two sets is just the set A itself.\nPartition \\(B\\) into two sets \\(A = \\{1,5,10\\}\\) and \\(C = \\{2,3,4,6,7,8,9\\}\\). \\(C = A^C\\) or the set of elements in B that are not in A. \\(A^C\\) is known as the complement of A.\nPerhaps unsurprisingly \\(A \\cup C = A \\cup A^C = B\\).\n\\(A \\cap C = \\emptyset\\) or the empty set. There are no elements simultaneously within both A and C.\nExercise 8\nProve that the sum of any two even numbers is even, the sum of any two odd numbers is even, and the sum of any odd number with any even number is odd.\nA\nDefine the set of even numbers as the set of integers multiplied by 2 so the set of even numbers can be described as \\(\\{2n: \\forall n \\in \\mathbb{I}\\}\\). Any two even numbers can be written as \\(2n + 2m\\). For example, we can write \\(4+2\\) as \\(2(1) + 2(2)\\) where \\(n=1, m = 2\\). Since both numbers have the common factor 2, we can pull it out and rearrange \\(2(n+m) = 2n + 2m\\). Now we can simply rewrite \\(n+m = k\\) and the expression simplifies to \\(2k\\) which is the equivalent definition of an even number since we have proved that this is also even.\nB\nThe same set up as above for evens also reveals that any odd number is an even number + 1. For example, 3 is odd and \\(3 = 2+1\\), or more generally any odd number \\(\\{k : 2n +1, \\forall i \\in \\mathbb{I}\\). The sum of any two odd numbers can be written \\((2n + 1) + (2m +1) = 2(n+m) + 2\\). To get a little intuition that this is correct, consider \\(1 + 1 = 2(0+0) + 2\\), while \\(1 + 3 = 2(0+1)+2\\). Since any even number added to another even number yields an even number, and the first part of the expression \\(2(n+m)\\) we have already proven to be even, we are done.\nC\nWe now have \\(2n + 2m + 1\\), for example \\(2 + 3 = 2(1) + 2(1)+1\\). We know from part A, that \\(2n + 2m\\) is even, and we can note that the definition of an odd number is an even number plus 1. Note that we can equivalently define the odd numbers as \\(2k - 1\\) (think 3 as either being \\(2(1) + 1 \\mid 2(2) - 1\\))\nAssume that any even number plus an odd number is equal to some even number \\(2p\\). Then:\n\\[\\begin{aligned}\n2n + 2m + 1 &= 2p \\\\\n2(n + m) + 1 &= 2p \\\\\n2(n + m) &= 2p - 1 \\\\\n2k &= 2p - 1\n\\end{aligned}\\]\nwhich yields a contradiction because this states that an even number is equal to an odd number.\nWhile not explicitly required, it’s worth pointing out why the above is a contradiction.\nSuppose for the sake of contradiction that an integer \\(x\\) was both odd and even. Then \\(x = 2y\\) for some integer \\(y\\) by the definition of evenness and \\(x = 2y + 1\\) by the definition of oddness. Thus, \\(2y = 2y +1\\) or \\(1 = 2(y - z) \\implies \\frac{1}{2} = y-z\\) but we know that \\(\\frac{1}{2}\\) is not an integer and any integer added to another integer must always yield an integer.\nReturn to Top\nM + S Chapter 2: Arithmetic Review\nExercises\nExercise 1-4\nThe expression on the left hand side is the problem. The goal is to complete the following equations.\n\\(x^1 = x\\)\n\\(-a \\times (-b)^2 = -ab^2\\)\n\\(\\sum_{i=1}^4x_i = x_1 + x_2 + x_3 + x_4\\)\n\\(\\prod_{m=6}^9x_m = x_5x_6x_7x_8x_9\\)\nExercise 10\nRepresent the following as a ratio, a proporition, and a percentage:\nLatinos relative to all others: African American 98,642, Asian 62,346, Caucasian 436,756, Latino 105,342, Other 32,654\nThe ratio is 105342:630398. The proportion is \\(\\approx.14\\). The percentage is \\(\\approx14.\\%\\)\nIndependent registered voters relative to Republicans: Democrats 432, Independent 221; Republicans 312.\nThe ratio is 221:312. The proportion is \\(\\approx.41\\). The percentage is \\(41\\%\\).\nFollowing B but now Republican relative to Democrats\nThe ratio is 312:432. The proportion is \\(\\approx.42\\). The percentage is \\(42\\%\\).\nExercise 13\nIf voter turnout in the United States in 1996 was \\(56\\%\\) and in 2000 it was \\(62\\%\\), what was the percentage change in turnout from 1996 to 2000?\nThe formula for percentage change is \\(\\frac{x_2 - x_1}{x_1}\\). Plugging in values we get \\(\\approx 10.7\\%\\)\nExercise 15\nSimplify the following expressions into one term.\n\\(xz + yz = z(x+y)\\).\n\\(mn + ln - pn = n(m + l - p)\\)\n\\(zyx - 2yx = yx(z-2)\\)\n\\((z+x)y\\frac{1}{x} = \\frac{y(x+z)}{x}\\)\nExercise 17\nSimplify \\(\\frac{5 + 17x + 4x + 7}{42x}\\)\n\\(\\frac{1}{2} + \\frac{2}{7x}\\). Group and add like terms. Split the fraction. Cancel out the common terms and reduce both fractions.\nExercise 22\nFactor and reduce \\(\\frac{\\beta - \\alpha}{\\alpha^2- \\beta^2}\\)\n\\(\\frac{-1}{\\alpha+\\beta}\\). The denominator can be reduced to \\((\\alpha+\\beta)(\\alpha-\\beta)\\). Multiply both the numerator and the denominator by \\(-1\\) makes the numerator \\(\\alpha - \\beta\\) and the denominator \\(-(\\alpha+\\beta)(\\alpha-\\beta)\\). We cancel the common factor to complete the reduction.\nExercise 23\nSolve\n\\[\\begin{aligned}\n15\\delta + 45 - 6\\delta &= 36 \\\\\n15\\delta + 45 - 6\\delta - 36 &= 0 \\\\\n15\\delta + 6\\delta - 9 &= 0 \\\\\n9\\delta - 9 &= 0 \\\\\n\\delta &= -1\n\\end{aligned}\\]\nExercise 29\nSolve using the quadratic formula \\(2x^2 + 5x - 7\\)\nThe quadratic formula is \\(\\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\).\nPlugging in values we get \\(x = 1 \\mid x = \\frac{-7}{2}\\)\nExercise 30\nDerive the quadratic formula by completing the sequare for the equation \\(ax^2 + bx + c = 0\\)\nStart with a quadratic of interest and divide through by the coefficient on on \\(x^2\\) yielding \\(x^2 - \\frac{bx}{a} = -\\frac{c}{a}\\) after rearranging.\nDivide the coefficient on x by 2 and then square it. Add that value to both sides of the equation. \\(x^2 - \\frac{bx}{a} + (\\frac{b}{2a})^2 = \\frac{c}{a} + (\\frac{b}{2a})^2\\).\nFactor the left hand side into a \\((x \\pm z)^2\\) form and simplify the right hand side. \\((x + \\frac{b}{2a})^2 = \\frac{c}{a} + (\\frac{b}{2a})^2\\).\n\\((x + \\frac{b}{2a})^2 = \\frac{b^2 - 4ac}{4a^2}\\) where we’ve multiplied \\(\\frac{-c}{a}\\) by \\(4a^2\\) and eliminated an a.\n\\(x+\\frac{b}{2a} = \\pm \\frac{\\sqrt{b^2 - 4ac}}{2a}\\). Taking the square root of both sides.\nRearrange and combine terms \\(x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\)\nExercise 31\nSolve\n\\[\\begin{aligned}\n-\\delta &> \\frac{\\delta + 4}{7} \\\\\n-7\\delta &> \\delta + 4 \\\\\n-8\\delta &> 4 \\\\\n\\delta &< -\\frac{1}{2}\n\\end{aligned}\\]\nReturn to Top\nM+S Chapter 12: Vectors and Matrices\nThere are discussions of the essentials of Linear Algebra in the Lecture Notes and in the Linear Algebra computation section.\nExercises\nExercise 1\nLet \\(\\textbf{a} = \\begin{pmatrix} 10\\\\ 2\\\\ 5 \\\\ 2\\end{pmatrix}\\), \\(\\textbf{b} = \\begin{pmatrix} 4\\\\ 15\\\\ 6 \\\\ 8\\end{pmatrix}\\), \\(\\textbf{c} = (2,6,8), \\textbf{d} = (1,15,12), \\textbf{e} = (14, 17, 11, 10)^T, \\textbf{f} = (20, 4, 10, 4)^T\\). Calculate each of the following, indicating that it’s not possible if there is a calculation you cannot perform.\n\\(\\textbf{a} + \\textbf{b} = (14, 17, 11, 10)^T\\)\nThis is not possible because the vectors are not of equal length.\n\\(\\textbf{b} - \\textbf{e} = (-10, -2, -5, -2)^T\\)\n\\(15\\textbf{c} = (30, 90, 120)\\)\n\\(-3\\textbf{f} = (-60, -12, -30, -12)^T\\)\n\\(\\lVert b \\rVert = \\sqrt{\\sum_{i=1}^n b_i^2} = \\approx 18.46\\).\n\\(\\lVert c + d \\rVert= \\sqrt{\\sum_{i=1}^n (c_i + d_i)^2} = \\sqrt{3^2 + 21^2 + 20^2} = \\approx 29.2\\). The triangle inequality states \\(\\lVert c + d \\rVert \\leq \\lVert c \\rVert + \\lVert d \\rVert\\). Focusing on the right hand side, the first term is \\(\\sqrt{4+36 + 64}= \\sqrt{104} \\approx 10.2\\) and the second term is \\(\\sqrt{1 + 15^2 + 12^2} = \\sqrt{370} \\approx 19.2\\) or \\(29.4\\) showing the triangle inequality holds.\n\\(\\lVert c - d\\rVert = \\sqrt{\\sum_{i=1}^n (c_i - d_i)^2} = \\sqrt{1 + 9^2 + 4^2} \\approx 9.9\\)\n\\(a \\cdot b = a^Tb = 10(4) + 2(15) + 5(6) + 2(8) = 116\\)\n\\(c \\cdot d = c^Td = 2(1) + 6(15) + 8(12) = 188\\)\nExercise 2\nIdentify the following matrices as diagonal, identity, square, symmetric, triangular, or none of the above. Note all that apply.\n\n[1] \"The matrix A is\"\n     [,1] [,2] [,3]\n[1,]    0    1    5\n[2,]    1   -2   -1\n[3,]    5   -1    2\n[1] \"The matrix B is\"\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n[1] \"The matrix C is\"\n     [,1] [,2]\n[1,]    1    1\n[2,]    3   -2\n[1] \"The matrix D is\"\n     [,1] [,2] [,3]\n[1,]    0    1    2\n[2,]    5    1   -1\n[3,]    2    4    0\n[4,]    1    1    0\n\nSquare and symmetric matrix\nThe identity matrix is square, symmetric, triangular, and of course the identity.\nThis is just a square matrix.\nThe matrix identified is none of the above.\nExercise 3\n\n[1] \"The transpose of A is\"\n     [,1] [,2] [,3]\n[1,]    0    1    5\n[2,]    1   -2   -1\n[3,]    5   -1    2\n[1] \"The transpose of B is\"\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n[1] \"The transpose of C is\"\n     [,1] [,2]\n[1,]    1    3\n[2,]    1   -2\n[1] \"The transpose of D is\"\n     [,1] [,2] [,3] [,4]\n[1,]    0    5    2    1\n[2,]    1    1    4    1\n[3,]    2   -1    0    0\n\nExercise 4\nGiven the matrices in the problem, perform the following calculations.\n\n\nA = rbind(c(5,1,2), c(6,2,3))\nB = rbind(c(3,4,5), c(-2,-3,6))\nC = cbind(c(1,-5,-3), c(2,3,1))\nD = rbind(c(2,1),c(4,3))\nA\n\n     [,1] [,2] [,3]\n[1,]    5    1    2\n[2,]    6    2    3\n\nB\n\n     [,1] [,2] [,3]\n[1,]    3    4    5\n[2,]   -2   -3    6\n\nC\n\n     [,1] [,2]\n[1,]    1    2\n[2,]   -5    3\n[3,]   -3    1\n\nD\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    4    3\n\n\\(A + C\\) is not possible because there are not the same dimensions.\n\\(A-B\\)\n\n     [,1] [,2] [,3]\n[1,]    2   -3   -3\n[2,]    8    5   -3\n\n\\(A + 5B\\)\n\n     [,1] [,2] [,3]\n[1,]   20   21   27\n[2,]   -4  -13   33\n\n\\(3A\\)\n\n     [,1] [,2] [,3]\n[1,]   15    3    6\n[2,]   18    6    9\n\n\\(2B - 5A\\)\n\n     [,1] [,2] [,3]\n[1,]  -19    3    0\n[2,]  -34  -16   -3\n\n\\(B^T - C\\)\n\n     [,1] [,2]\n[1,]    2   -4\n[2,]    9   -6\n[3,]    8    5\n\n\\(BA\\) is not possible because the dimensions are not correct \\(2 \\times 3\\) and \\(2 \\times 3\\)\n\\(DA\\)\n\n     [,1] [,2] [,3]\n[1,]   16    4    7\n[2,]   38   10   17\n\n\\(AD\\) is not possible because the dimensions are not correct.\n\\(CD\\)\n\n     [,1] [,2]\n[1,]   10    7\n[2,]    2    4\n[3,]   -2    0\n\n\\(BC\\)\n\n     [,1] [,2]\n[1,]  -32   23\n[2,]   -5   -7\n\n\\(CB\\)\n\n     [,1] [,2] [,3]\n[1,]   -1   -2   17\n[2,]  -21  -29   -7\n[3,]  -11  -15   -9\n\nNote that \\(BC \\neq CB\\) This is common with matrix multiplication whenever both \\(BC\\) and \\(CB\\) exist.\nExercise 6\nFind the inverse of the following two matrices or explain why it does not exist.\n\n     [,1] [,2]\n[1,]    4    2\n[2,]    6    3\n     [,1] [,2]\n[1,]    1    4\n[2,]    3    2\n\n\\(A\\) does not have full rank because the first column is just the second column muliplied by 2. As a result, the matrix is not square (and singular) so there is no inverse.\n\\(B^{-1}\\) does have an inverse. The determinant is \\(ad - bc = 2 - 12 = -10\\). We swap the two diagonal elements and flip the signs on the off diagonal and then multiply by 1 over the determinant \\(\\frac{-1}{10}\\).\n\n     [,1] [,2]\n[1,] -0.2  0.4\n[2,]  0.3 -0.1\n\nExercise 9\nTrue or False?\n\\(BA = AB\\) for all matrices \\(A,B\\).\nFalse. We’ve already shown this to be the case in 4 so the matrices B and C from that problem serve as a counter example.\n\\(XX^-1 \\neq I\\). False. Plug in the identity matrix for X as a counter example. More generally, we can produce the identity matrix by either left or right multiplying any matrix \\(X\\) with the inverse matrix provided the inverse matrix exists.\n\\(M_{i \\times j}N_{j\\times k} = (MN)_{i \\times k}\\). True.\nExercise 10\nWhy are the following useful?\nThe determinant is useful because it determines whether a matrix is invertible. If the determinant is 0, then a matrix is not invertible.\nThe inverse is useful because we can use it to solve the system of equations represented by the matrix. An inverse indicates whether the system has a solution.\nExercise 11\nWhat does it mean if you have a singular matrix?\nIt means that the matrix is not invertible and has less thank full rank. For most applied political science research, it means that you have a perfect collinearity between two of your variables, and the error will show up when you try to run a regression via OLS.\nExercise 12\nFill in the blanks:\nAB indicates that you right-multiply A by B.\nAB indicates that you left-multiply B by A\nReturn to Top\nM+S Chapter 13: Vector Spaces and Systems of Equations\nExercises\nExercise 1\nLet \\(\\textbf{a} = \\begin{pmatrix} 10\\\\ 2\\\\ 5 \\\\ 2\\end{pmatrix}\\), \\(\\textbf{b} = \\begin{pmatrix} 4\\\\ 15\\\\ 6 \\\\ 8\\end{pmatrix}\\), \\(\\textbf{e} = (14, 17, 11, 10)^T, \\textbf{f} = (20, 4, 10, 4)^T\\). Calculate each of the following, indicating that it’s not possible if there is a calculation you cannot perform.\nWrite the most general vector that is a lineaer combination of \\(\\textbf{a}\\) and \\(\\textbf{b}\\).\n\\[\\begin{aligned}\ns\\textbf{a} + t\\textbf{b} &= \\begin{pmatrix} 10s + 4t \\\\ 2s + 15t \\\\ 5s + 6t \\\\ 2s + 8t \\end{pmatrix}\n\\end{aligned}\\]\nNo. At most there are 2 that linearly dependent. For example \\(\\textbf{e} = \\textbf{a} + \\textbf{b}\\), while \\(\\textbf{f} = 2\\textbf{a}\\)\nSince there are only two linearly independent vectors, the vectors span a two-dimensional space.\nExercise 2\nSolve the following systems of equations using substitution or elimination or both.\nExercise 4\nFrom the previous chapter, consider the following matrices.\n\n[1] \"D\"\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[1] \"E\"\n     [,1] [,2]\n[1,]    3    1\n[2,]    6    2\n[1] \"g\"\n     [,1]\n[1,]    2\n[2,]    3\n[3,]    1\n[1] \"h\"\n     [,1] [,2] [,3]\n[1,]    1    2    3\n\nRank of D is 2.\nRank of E is 1 because the first column is 3 times the second.\nSolve \\(D\\textbf{x} = \\textbf{g}\\) for \\(\\textbf{x}\\) using matrix inversion.\nAs stated, this is not possible because \\(\\textbf{g}\\) has dimension 3 and \\(D\\) is of rank 2.\nSolve \\(\\textbf{x}E = \\textbf{h}\\) for \\(\\textbf{x}\\) using matrix inversion.\nOnce again, E is singular which means it is not a full rank matrix so we cannot use matrix inversion. There is in fact an infinite number of solutions to the system of equations.\nExercise 6\nWhy is Cramer’s rule useful?\nCramer’s rule is useful for solving for just one of the variables in a system of equations without having to solve the entire system of equations. It is a method of solving systems of linear equations by dividing the value of two determinants. To find whichever variable we are interested in, just evaluate the determinant quotient $.\nFor example, consider the system of equations\n\\[\\begin{pmatrix}\n2x + y + z = 1 \\\\\nx - y + 4z = 0 \\\\\nx + 2y - 2z = 3\n\\end{pmatrix}\\]\nand imagine we only are interested in solving for \\(x\\). First, form find the value of the coefficient determinant.\n\\[D = \\begin{vmatrix}\n2 & 1 & 1 \\\\\n1 & -1 & 4 \\\\\n1 & 2 & 2\n\\end{vmatrix}\\]\nwhich here becomes \\(4 + 4 + 2 +1 -16 + 2 = -3\\)\nNext, form \\(D_z\\) by replacing the third column of values with the answer column.\n\\[D_z = \\begin{vmatrix}\n2 & 1 & 1 \\\\\n1 & -2 & 0 \\\\\n1 & 2 & 3\n\\end{vmatrix}\\]\nwhich comes out to \\(-6 + 0 + 2 +1 - 0 -3 = -6\\).\nNow just divide the two values \\(z = \\frac{D_z}{D} = \\frac{-6}{-3} = 2\\).\nNote that in a world in which \\(D = 0\\), we cannot use Cramer’s Rule. Incidentally, Cramer’s Rule is rarely used in computation because it is often unstable.\nExercise 7\nWhat does it mean if you have a singular matrix?\nSee the answer in Exercise 11 from Chapter 12\nReturn to Top\nCalculus Sequence\nThe Calculus sequence suggested timeline is 2-4 weeks. It is entirely composed of chapters from M+S and comprises mathematical preliminaries, functions, limits and continuity, sequences and series and sets, introduction to derivatives and integrals, and an introduction to extrema and multivariate calculus.\nReturn to Top\nM+S Chapter 3: Functions\nExercises\nSimplify \\(h(x) = g(f(x))\\) where \\(f(x) = x^2 +2\\) and \\(g(x) = \\sqrt{x-4}\\).\n\\[\\begin{aligned}\ng(f(x)) &= \\sqrt{(x^2 +2 ) - 4} \\\\\ng(f(x)) &= \\sqrt{x^2 - 2}\n\\end{aligned}\\]\nSimplify \\(h(x) = f(g(x))\\) with same \\(f\\) and \\(g\\) as previous answer. Is it the same as 2? No\n\\[\\begin{aligned}\nh(x) &= (\\sqrt{x-4})^2 + 2 \\\\\nh(x) &= x-2\n\\end{aligned}\\]\nFind the inverse function of \\(f(x) = 5x - 2\\).\n\\[\\begin{aligned}\n\\frac{x + 2}{5}\n\\end{aligned}\\]\nSimplify \\(x^{-2}x^3\\). \\(x\\)\nSimplify into one term \\(ln(3x) - 2ln(x +2)\\). \\(ln\\left(\\frac{3x}{x+2} \\right)\\)\nRewrite \\(y = \\alpha + x_1^{\\beta_1} + \\beta_2x_2 + \\beta_3x3\\) by taking the log of both sides. Is the result a linear (affine function)?\n\\[\\begin{aligned}\nln(y) &= ln(\\alpha + x_1^{\\beta_1} + \\beta_2x_2 + \\beta_3x_3) \\\\\nln(y) &= ln(\\alpha) + \\beta_1ln(x1) + ln(\\beta_2x_2) + ln(\\beta_3x_3)\n\\end{aligned}\\]\nRewrite \\(y = \\alpha + x_1^{\\beta_1} + x_2^{\\beta_2} + x_3^{\\beta_3}\\) by taking the log of both sides. Is the result a linear (affine) function? Yes it’s affine.\n\\[\\begin{aligned}\nln(y) &= ln(\\alpha) + \\beta_1 ln(x_1) + \\beta_2 ln(x_2) + \\beta_3 ln(x_3)\n\\end{aligned}\\]\nReturn to Top\nM+S Chapter 4: Limits and Continuity, Sequences and Series, and More on Sets\nExercises\nDraw a graph to show that the sequence {1,-1,1,-1,…} is divergent.\n\n\n\nFind the sum of the infinite series \\(\\sum_{t=0}^\\infty (\\delta^t)^2\\)\nRecall that \\(\\sum_{t=0}^\\infty \\delta^t = \\frac{1}{1-\\delta}, \\forall \\delta < 1\\). As long as this condition holds, we can use the same expression giving us \\(\\frac{1}{1-\\delta^2}\\)\nShow whether \\(f(x) = x + x^3\\) has a limit at x = 3 and if so, the value of the limit.\nYes this has a limit. The limit is 30. \\((3) + 3^3 = 30\\).\nShow whether \\(f(x) = \\frac{3x^2 - 12}{x -2}\\) has a limit at \\(x = 2\\) and if so the value of the limit.\nFactor the numerator. \\(f(x) = \\frac{3(x^2 - 4)}{x -2}\\). Note that we can factor the () expression as \\((x-2)(x+2)\\). Cancel out the common term. We are left to plug in the value 2 and the expression is 12.\nFor each of the following sets, state whether they are (a) open, closed both or neither, (b) bounded (c) compact (d) convex.\n\\([1,3]\\) closed, bounded, compact, convex\nIs the function\n\\[\\begin{aligned}f(x) = \\begin{cases}\n   x^3 - 3x + 4 & x &\\leq 3 \\\\\n   x^2 & x &> 3\n\\end{cases}\\end{aligned}\\]\ncontinuous? If so, why? If not what changes would make it continuous?\nThe function is not continuous because it does not have the same right hand side and left hand side limit. The function’s limit is 22 at 3, but 9 as it approaches 3 from the right. One could \\(\\pm 13\\) to the appropriate part to make it continous.\nReturn to Top\nM+S Chapter 5: Introduction to Derivatives\nExercises\n\n\\(y=6\\). The derivative of a constant is 0.\n\\(y = 3x^2\\). The derivative is \\(6x\\).\n\\(y = x^2 - 2x^2 - 1\\). The derivative is \\(3x^2 -4x\\).\n\\(y = x^4 + 5x\\). The derivative is \\(4x^3 + 5\\).\n\\(y = x^8\\). The derivative is \\(8x^7\\).\n\\(y = 4x^3 - x + 1\\). The derivative is \\(12x^2 -1\\).\nTo demonstrate on this problem the specific calculations.\n\\[\\begin{aligned}\nf'(x) &= lim_{x\\rightarrow 0}\\frac{4(x+h)^3 -(x+h) + 1 - 4x^3 + x -1}{h} \\\\\nf'(x) &=lim_{x\\rightarrow 0}\\frac{12x^2h + 12xh^2 + 4h3 -h}{h} \\\\\nf'(x) &=lim_{x\\rightarrow 0} 12x^2 + 12xh + 4h^2 = 1\\\\\nf'(x) &=12x^2 -1\n\\end{aligned}\\]\n\\(y = 2x^4 + x^2 - 1\\). The derivative is \\(8x^3 + 2x\\).\n\\(y = 5x^5 + 4x^4 + 3x^3 + 2x^2 + x + 1\\). The derivative is \\(25x^4 + 16x^3 + 9x^2 + 4x + 1\\)\n\\(y = 7x^4 - 9x^3 + 5x + 117\\). The derivative is \\(28x^3 - 27x^2 + 5\\).\n\\(y = 27x^3 + 5x^2 -x + 13\\). The derivative is \\(81x^2 + 10x -1\\)\nThe graphs are not included.\n\\(f(x) = 2x^2 + 7\\). The derivative is \\(4x\\).\n\\(f(x) = x^3 - x + 1\\). The derivative is \\(3x^2 -1\\)\nFor each of the following find the partial derivative with respect to x.\n\\(f(x,z) = 3zx + 2z\\). The partial derivative is \\(\\frac{\\partial_f}{\\partial_x} = 3z\\).\n\\(f(x,z) = x^2 + 2z^2\\). \\(\\frac{\\partial_f}{\\partial_x} = 2x\\).\n\\(f(x,z) = 3z^2 - z + 1\\). \\(\\frac{\\partial_f}{\\partial_x} = 0\\)\nReturn to Top\nM+S Chapter 6: Rules of Derivatives\nExercises\nFind the derivative of (every other) \\(y\\) with respect to \\(x\\) for the following.\n\\(y = 6\\). The derivative is \\(0\\).\n\\(y = x^3 - 2x^2 - 1\\). The derivative is \\(3x^2 - 4x\\).\n\\(y = x^8\\). The derivative is \\(8x^7\\).\n\\(y = ax^3 + 6\\). The derivative is \\(3ax^2\\)\n\\(y = ax^n - 1\\). The derivative is \\(nax^{n-1}\\)\n\\(f(x)\\cdot g(x) = (13x + 2x^3)(x^5 - 4x + r)\\)\nUse the product rule \\(f'g + fg'\\). \\(16x^7 + 78x^5 - 32x^3 + 6rx^2 - 104x + 13r\\)\n\\(y = (x-3)^3\\). The derivative is \\(3(x-3)^2\\)\n\\(y = 5x^7 + 7x^4 + 3x^2\\). The derivative is \\(35x^6 + 28x^3 + 6x\\).\n\\(y = x^3 + x^2 + 1\\). The derivative is \\(3x^2 + 2x\\).\n\\(y = (3x^2 + 4)(2x^3 + 3x + 5)\\). The derivative is \\(30x^4 + 51x^2 + 30x + 12\\)\n\\(y = (x+5)^2\\). The derivative is \\(2(x+5)\\)\n\\(y = (\\frac{x^2+1}{x+1})^2\\).\nUse the quotient rule. \\(\\frac{f'g - f'g}{g^2}\\). The derivative is \\(\\frac{2(x^2 +1)(x^2x -1)}{(x+1)^3}\\)\n\\(y = \\frac{x^3 + x^2 + x + 1}{x^2 + x +1}\\). The derivative is \\(\\frac{x^2(x^2 + 2x + 3)}{(x^2 + x + 1)^2}\\)\nUsing the rules in this chapter, differentiate the following:\n\\(f(x) = a_nx^n + a_{n-1}x^{n-1} + ... + a_0\\). Try also expressing the derivative as a series.\n\\(f(x) = \\frac{x^2-4}{x^5 - x^3 + x}\\). The derivative is \\(-\\frac{3x^6 - 21x^4 + 11x^2 -4}{x^2(x^4 -x^2 +1)^2}\\)\n\\(xg(x) - 7x^2\\) where \\(g(x) = e^xln(x)\\). The derivative is \\(e^xln(x) + \\frac{e^x}{x} - 14x\\)\n\\(f(x) = e^{5x}\\). The derivative is \\(5e^{5x}\\).\n\\(f(x) = \\frac{1}{2}e^{\\frac{x}{2}}\\). The derivative is \\(\\frac{e^{\\frac{x}{2}}}{4}\\)\n\\(f(x) = e^{ln(2x)}\\). The derivative is \\(2\\). \\(e^{ln(2x)} = 2x\\) by definition.\n\\(f(x) = x^2g(x) + 6x^2\\) where \\(g(x) = log_a(x) + x^7\\). The derivative is \\(\\frac{2x ln(x)}{ln(a)} +7x^6 + \\frac{x}{ln(a)} + 12x\\)\nShow that \\(\\frac{dlog_a(x)}{dx} = \\frac{1}{x(ln(a))}\\)\nReturn to Top\nM+S Chapter 7: Integrals\nExercises\nIntegrate the following derivatives to find \\(y\\)\n\\(f'(x) = 4x + 3\\)\n\\(f'(x) = -2x +3 - 4x^3\\)\n\\(f'(x) = -3 + 4x\\)\n\\(f'(x) = 4x^4 + 3x^2\\)\n\\(f'(x) = 4x^4 + 3x^3 + 2x^2 + x + 1\\)\n\\(f'(x) = x^{-1} + 3x^2\\)\n\\(f'(x) = 2e^{5x}\\)\n\\(f'(x) = ln(3x)\\)\nWhich of the following below best describes \\(\\int_{a}^b\\frac{dy}{dx}dx\\)? It is the integral of the derivative of y with respect to x over the range a to b.\n6 Compute the following integrals:\n\\(\\int (a_nx^n + a_{n-1}x^{n-1}+...+a_0)dx\\)\n\\(\\int (3x^{3/2} - 2x^{-5/4} + 4^x)dx\\)\n\\(\\int_{1}^{16} (5x^{3/2}-2x^{-5/4})dx\\)\n\\(\\int (-\\frac{1}{x}ln(\\frac{1}{x}))dx\\)\n\\(\\int (xe^{3x^2+1})dx\\)\n\\(\\int_{2}^{4}(3x^2 + x + 5)dx\\)\nReturn to Top\nM+S Chapter 8: Extrema in One Dimension\nExercises\nFind all extrema (local and global) of the following functions on the specified domains, and state whether each extremum is a minimum or maximum and whether each is only local or global on that domain.\n\\(f(x) = x^3 - x +1, x\\in[0,1]\\)\n\\(f(x) = \\frac{x^2}{e^x}, x\\in[0,\\infty)\\)\n\\(f(x) = 2 - 3x, x \\in [-3,10]\\)\n\\(f(x) = 4x^3 + x^2 - 2x + 3, x \\in [-1,1]\\)\n\\(f(x) = \\frac{1}{4}x^4 - \\frac{4}{3}x^3 + \\frac{1}{2}x^2 + 6x + 2, x \\in [-2,3]\\)\nThe utility that a legislator extracts from a policy in a one dimensional policy space varies as a quadratic loss function, \\(U(x)\\), of the form \\(U(x) = -a(x-x_0)^2\\), where \\(x_0\\) represents the ideal location of the legislator and \\(x\\) represents the location of the current policy in the one-dimensional policy space. Prove that the legislator’s utility is maximized when the policy is located at \\(x=x_0\\).\nFirst find the derivative and set it to 0. \\(U'(x) = -2a(x-x_0) = 0\\).\nWe can divide both sides by \\(-2a\\) to get \\(x-x_0 = 0 \\implies x^* = x_0\\).\nNow check to confirm that the second derivative is negative with respect to x. \\(U''(x) = -2a < 0, \\forall a > 0\\), which implies that the stationary point is a global maximum and the proof is completed.\nReturn to Top\nM+S Chapter 15: Multivariate Calculus\nThere are no suggested problems for this chapter, but it is good to read.\nRandom Variables and Probability Sequence\nThe Random Variables and Probability sequence suggested timeline is 1-2 weeks. It is entirely composed of chapters from M+S and FPP and comprises mathematical preliminaries, introduction to probability, and an introduction to discrete distributions and continuous distributions.\nReturn to Top\nM+S Chapter 1: Preliminaries\nThis material is already discussed in the Linear Algebra Sequence\nFPP Chapter 4,6\nExercises\nChapter 4 Exercise Set A (3,4,5,9)\nWhich of the following two lists has a bigger average?\n\\(10, 7, 8, 3,5,9\\)\nii) \\(10, 7, 8, 3,5,9,11\\)\nThe mean will always be pulled in the direction of outliers.\nTen people in a room have an average height of 5 feet 6 inches. An 11th person who is 6 feet 5 inches tall enters the room. Find the average height of all 11 people.\n67 inches or 5’7”. We can think of this as having 10 people in a room who are 5’6” and one person who is 6’5” and computing the average.\nSame as 4 except there are now 21 people in the room and a new person enters who is 6 feet 5 inches.\nThe new answer is now 66.5 inches or 5 feet 6 inches.\nAverage hourly earnings are computed each month by the BLS using payroll data from commercial establishments. The BLS figures the total wages paid out (to nonsupervisory personnel), and divides by the total hours worked. During recessions, average hourly earnings typically go up. When the recession ends, average hourly earnings often start going down. How can this be?\nDuring a recession, lower wage workers are often laid off. This will mechanically make the average wage rise. When the recession is over these workers are hired back, driving down the average wage.\nReturn to Top\nChapter 4 Exercise Set B (1,2,5)\nThere are three histograms. The answers are 50, 25, and 40.\nThe median is equal to the average in the first two histograms but to the left of the third.\nFor registered students at universities in the U.S., which is larger: average age or median age?\nAverage age because we expect most students to be between 18-22 but also older students which does a long right tail.\nTop\nChapter 4 Exercise Set C (1,3)\nFind the average and RMS size of the numbers on the list: \\(1, -3, 5, -6, 3\\).\nThe RMS is \\(\\sqrt{\\frac{1}{n}\\sum_{i=1}^n x_i^2}\\). Here that computation yields 0 for the average and 4 for the RMS.\nFind the RMS size of the list \\(7,7,7,7\\). Repeat for \\(7,-7,7,-7\\).\nIn both cases, the RMS is 7.\nReturn to Top\nChapter 4 Exercise Set D (1,2,3,6,8)\nThe public health service found that for boys age 11 in HANES2, the average height was 146cm and the SD was 8cm. Fill in the blanks below.\nOne boy was 170cm. He was above average by 3 SDs.\nAnother boy was 148cm tall. He was above average by \\(\\frac{1}{4}\\) SDs.\nA third boy was 1.5 SDs below average height. He was 134cm tall.\nIf a boy was within 2.25 SDs of average height, the shortest he could have been is 128cm tall and the tallest he could have been was 164cm tall.\nContinuing the previous exercise, there are four boys with heights 150, 130, 165, 140 cm. Match heights with descriptions “unusually short”, “about average”, “unusually tall”\n150 and 140: about average. 130: unusually short. 165: unusually tall.\nEach of the following lists has an average of 50. For which one is the spread of the numbers around the average biggest? Smallest?\n\\(0,20,40,50,60,80,100\\)\n\\(0, 48,49,50,51,52,100\\) Smallest\n\\(0,1,2,50, 98,99,100\\) Biggest\nThe problem has sketches of histograms and asks you to match them to choices. The correct choices are i, ii, v respectively.\nOne investigator takes a sample of 100 men age 18-24 in a certain town. Another takes a sample of 1000 such men. Which investigator will get a bigger average for heights or should the averages be about the same. Repeat with standard deviations. Which investigator is likely to get the tallest of the sample men or are the chances about the same? Repeat with shortest of the sample men.\nThe averages and standard deviations will be about the same. In both cases, the samples are drawn from the same underlying population and 100 is reasonably large. The range will be bigger in the larger sample. Here’s a little R simulation showing this principle.\n\n\nset.seed(123)\npop = rnorm(100000, mean = 72, 4)\nsmall = sample(pop, size = 100, replace = F)\nlarge = sample(pop, size = 1000, replace = F)\nprint(paste0(\"Mean of small: \", mean(small)))\n\n[1] \"Mean of small: 72.1097400452601\"\n\nprint(paste0(\"Mean of large: \", mean(large)))\n\n[1] \"Mean of large: 72.1080415819342\"\n\nprint(paste0(\"SD of small: \", sd(small)))\n\n[1] \"SD of small: 3.59422670373852\"\n\nprint(paste0(\"SD of large: \", sd(large)))\n\n[1] \"SD of large: 4.05933961830242\"\n\nprint(paste0(\"Min of small: \", min(small)))\n\n[1] \"Min of small: 63.2414773222852\"\n\nprint(paste0(\"Min of large: \",min(large)))\n\n[1] \"Min of large: 60.5438232355171\"\n\nprint(paste0(\"Max of small: \", max(small)))\n\n[1] \"Max of small: 82.6366279193488\"\n\nprint(paste0(\"Max of large: \",max(large)))\n\n[1] \"Max of large: 85.4581984517501\"\n\nChapter 4 Exercise Set E (5,8,11,12)\nFor each list below, work out the average, the deviations from average and the standard deviation.\n\\(1,3,4,5,7\\). The mean is is 4. The deviations are \\(-3,-1,0,1,3\\). The standard deviation is 2.\n\\(6,8,9,10,12\\). The mean is 9. The deviations are identical to i. The standard deviation is also 2.\nFor part b, the relationship is that the list in ii adds 5 to the values in i. This mechanically increases the average by 5 but has no effect on the deviations and standard deviation.\nThe Governor of California proposes to give all state employees a flat raise of $250 a month. What would this do the average monthly salary of state employees? to the SD?\nThe average would increase by $250. There would be no change in the SD.\nRepeat the same question but with 5% increases in salary across the board?\nThe average and the standard deviation would increase by 5%.\nCan the SD ever be negative?\nNo, and if you find a negative standard deviation you have a bug. Partially this is because the standard deviation is a function of a square root, which does not have negative numbers in the domain, and partially because the computation first requires squaring all values.\nFor a list of positive numbers, can the SD ever be larger than the average?\nYes. In a world where most numbers are clustered around a point with a big outlier. For example \\(1,1,25\\) will produce a standard deviation of \\(\\approx 14\\) but an average of 9.\nFPP Chapter 13, 14\nExercises\n13 Exercise Set A (2,4,5)\nA coin will be tossed 1000 times. About how many heads are expected?\nPresuming the coin is fair, about 500.\nIn five-card draw poker, the chance of being dealt a full house is 0.14 of 1%. If 10000 hands are dealt, about how many will be a full house.\n14\nOne hundred tickets will be drawn at random with replacement from one of the two boxes a) [1,2] or b) [1,3]. You’re paid the value of the tickets drawn. Which box is better and why?\nThe second box because it has a higher range.\n13 Exercise Set B (1,4)\nTwo tickets are drawn at random without replacement from the box [1,2,3,4]. What is the chance that the second ticket is 4? What is the chance that the second ticket is 4, given the first is 2?\nThe first is 1/4 because the tickets have not been drawn. There are 24 possible orderings, and in 6 of them the 4 is in the second position. The answer to the second is 1/3. Given that the first number is a 2, there are only 3 numbers left one of which is a 4 and all are equally likely to be drawn.\nFive cards are dealt off the top of a well-shuffled deck. What is the probability that the 5th card is the queen of spades? Find the chance that the 5th card is the queen of spades, given the first 4 cards are hearts.\nSimilar logic to the problem above. The first answer is 1/52. The second answer is 1/48.\n13 Exercise Set C (1,4,7)\nA deck is shuffled and two cards are dealt. What is the probability that the second card is a heart given the first card is a heart? What is the probability that the first card is a heart and the second card is a heart.\nFor the first part, we removed one heart from the deck. The chance is thus \\(\\frac{12}{51}\\). For the second part, there are thirteen hearts to pick on the first draw and 12 on the second. The result is \\(\\frac{13}{52}\\frac{12}{51} = \\approx 6\\ %\\)\nA die will be rolled six times. You have a choice to either win $1 if at least one ace shows up or to win $1 if an ace show up on all rolls. Which do you choose?\nThe former, the probability of at least one ace is \\(1-P(NA)\\) which is equivalent to \\(1-\\left(\\frac{5}{6}\\right)^6 \\approx 67\\%\\). The latter is functionally zero.\nA coin is tossed three times. What is chance of getting three heads? What is the chance of not getting three heads? What is the chance of getting at least 1 tail? At least 1 head?\nWe’ll assume that the coin is fair. The chance of getting three heads from three independent tosses is \\(\\left(\\frac{1}{2}\\right)^3 = \\frac{1}{8}\\). The chance of not getting three heads is the complement so \\(\\frac{7}{8}\\). The chance of getting at least 1 tail is equivalent to the the chance of not getting three heads. At least one head is the same answer just reversing what we want at least one of.\n13 Exercise Set D (1,2,8)\nThis exercise is a picture question. The first two are independent. The third is dependent.\nThis exercise is also a picture clue. The first two are again independent. The third is dependent.\nIn a certain psychology experiment, each subject is presented with three ordinary playing cards, face down. The subject takes one of these cards. The subject also takes one card at random from a separate full deck of playing cards. If the two cards are from the same suit, the subject wins a prize. What is the chance of winning? If more information is needed, explain what you need and why.\nThe answer is \\(\\frac{1}{4}\\). To see why, imagine that all three cards are the same card, say the King of Hearts. The probability that you draw a heart from the full deck is the same for each. Now imagine that the three cards are different, say the Kings of Hearts, Spades, and Diamonds. No matter which one you draw, there is no effect on the second full deck of cards, so once again the probability you win is the same.\n14 Exercise Set A (1,4)\nWhat is the chance of throwing a total of 5 spots with two fair die?\n\\(\\frac{1}{9}\\). There are four possible combinations of 5 spots \\((2,3),(3,2),(4,1),(1,4)\\) and 36 total possibilities.\nThis is a picture question. The correct probabilities are \\(\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{2}\\). It is no different than the dice problem conceptually.\n14 Exercise Set B (2,3,6)\nThis is a picture problem with three overlapping circles. Count the dots in the circles and be careful not to double count.\nTwo cards are dealt off the top of a well-shuffled deck. You have a choice: win $1 if the first card is an ace or the second card is an ace. win $1 if at least one of the two cards is an ace. Which option is better?\nThey’re the same probability. Or is equivalent to at least one here.\nA number is drawn at random from a box. There is a 20% chance for it to be 10 or less, a 10% chance for it to be 50 or more. True or False: the chance of getting a number between 10 and 50 (exclusive) is 70%. Explain briefly.\nTrue. This follows from the addition rule and is in some sense defining the probability density function of this variable.\n14 Exercise Set C (1,3)\nA large group of people are competing for all-expense-paid weekends in Philadelphia. The MC gives each contestant a well-shuffled deck of cards. The contestant deals two cards off the top of the deck, and wins a weekend in Philadelphia if the first card is the ace of hearts or the second card is the king of hearts\nAll of the contestants whose first card was the ace of hearts are asked to step forward. What fraction of the contestants do? \\(\\frac{1}{52}\\).\nThe contestants return to their original places. Then, the ones who got the king of hearts for their second card are asked to step forward. What fraction of the contestants do. \\(\\frac{1}{52}\\)\nDo any of the contestants step forward twice? Almost surely yes. While unlikely it is possible that someone got both the aces of hearts and then the king of hearts. \\(\\frac{1}{52}\\frac{1}{51}\\)\nTrue or false and explain. The chance of winning a weekend in Philadelphia is \\(\\frac{1}{52} + frac{1}{52}\\). False, we have double counting that we need to deal with \\(P(A) + P(B) - P(A \\cap B)\\)\nA deck of cards is shuffled. True or false and explain briefly.\nThe chance that the top card is the jack of clubs equals \\(\\frac{1}{52}\\). True, the jack has to go somewhere. It has 52 possible positions. One of them is the first card.\nThe chance that the bottom card is the jack of diamonds equals \\(\\frac{1}{52}\\). True for a similar argument as above.\nThe chance that the top card is the jack of clubs or the bottom card is the jack of diamonds equals \\(\\frac{2}{52}\\). False, these aren’t mutually exclusive. so we need to take care of the probability where the jacks are both in these places.\nThe chance that the top card is the jack of clubs or the bottom card is the jack of clubs is \\(\\frac{2}{52}\\). True. These are two independent events.\nThe chance that the top card is the jack of clubs and the bottom card is the jack of diamonds equals \\(\\frac{1}{52}\\frac{1}{52}\\). False, we have a conditional probability here..\nThe chance that the top card is the jack of clubs and the bottom card is the jack of clubs equals \\(\\frac{1}{52}\\frac{1}{52}\\). False, the probability here is 0. This event is impossible.\n14 Exercise Set D (3,5)\nA box contains four tickets, one of them is marked with a star and the other three are blanks. Two draws are made with replacement from the box.\nWhat is the chance of getting a blank ticket on the first draw? \\(\\frac{3}{4}\\).\nWhat is the chance of getting a blank ticket on the second draw? Same as a.\nWhat is the chance of getting a blank ticket on the first draw and getting a blank ticket on the second draw? \\(\\frac{3}{4}\\frac{3}{4} = \\frac{9}{16}\\)\nWhat is the chance of not getting the star in the two draws? Same as c.\nWhat is the chance of getting the star at least once in the two draws? \\(1-\\frac{9}{16} = \\frac{7}{16}\\).\nA pair of dice is rolled 36 times. What is the chance of getting at least one double ace? The answer is \\(1 -\\left(\\frac{35}{36}\\right) \\approx 64\\%\\). You might be tempted to think because there’s a chance on the first roll and the same on the second, but this would be incorrect because we are interested in an event happening over time, not the likelihood on any given roll.\nM+S Chapter 9: Introduction to Probability\nThere are no exercises assigned in this chapter, but it is good to read.\nM+S Chapter 10: Introduction to Discrete Distributions\nThere are no exercises assigned in this chapter, but it is good to read.\nReturn to Top\nFPP Chapter 8, 9\nExercises\nFPP Chapter 8 Exercise Set B (1,2,6,9)\nWould the correlation between the age of a second hand car and its price be positive or negative? Why? Negative, in general older cars are cheaper.\nWhat about the correlation between weight and miles per gallon? Negative, heavier cars are less fuel efficient.\nThese are picture questions\nThese are picture questions\nTrue or False, and explain. If the correlation coefficient is .9 then 90% of the points are highly correlated? False. The correlation coefficient is a measure of linear association between two variables.\nReturn to Top\nFPP Chapter 8 Exercise Set C(1) and D(1)\nTrue or False:\nThe standard deviation line always goes through the point of averages. True\nThe standard deviation always goes through the point (0,0). False.\nFor each of the datasets below calculate R.\n\\(r = \\frac{1}{n}\\sum_{i=1}^n x_iy_i\\) where \\(x,y\\) are standardized units. To standardize a variable, we compute \\(\\frac{x_i - \\bar{x}}{\\sqrt{\\frac{1}{N}\\sum_{i=1}^N(x_i - \\bar{x})^2}}\\) for all values of \\(x\\). Inspection reveals that the third set are perfectly negatively correlated, but we’ll do the calculation anyway.\n\n\nstandardize = function(v){\n  sdv = sqrt((1/length(v))*sum((v-mean(v))^2))\n  return((v-mean(v))/sdv)\n}\n\nr = function(y, x){\n  xs = standardize(x)\n  ys = standardize(y)\n  round(mean(xs*ys),2)\n}\n\nx = 1:7\ny = list(c(6,7,5,4,3,1,2), c(2,1,4,3,7,5,6), 7:1)\nlapply(y, r, y = x)\n\n[[1]]\n[1] -0.93\n\n[[2]]\n[1] 0.82\n\n[[3]]\n[1] -1\n\nReturn to Top\nFPP Chapter 9 Exercise Set A (2,6,9)\nA small dataset of two variables has a correlation coefficient of \\(\\approx .76\\). If you switch the two columns does this change \\(r\\)?\nNo, like covariance, correlation does not depend on the order of the variables.\nSuppose the correlation between \\(x\\) and \\(y\\) is 0.73.\nDoes the scatter diagram slope up or down? Slopes up.\nIf you multiply all the values of \\(y\\) by \\(-1\\), would the new scatter diagram slope up or down? It would now slope down.\nWhat happens to the correlation? Correlation stays the same but the sign flips.\nThis problem provides several datasets and asks you to give the correlations. You’ll find that the four datasets are just column switched versions of the first two or multiples of the first two.\nFPP Chapter 9 Exercise Set B(4)\nB.4. This is a picture clue that asks about calculating \\(r\\) without labels. You’ll find that the answer is yes you can. The points are \\(\\{(1,1), (2,1), (2,2), (3,2)\\}\\) and the same flipped in the other direction. In both cases, the \\(r\\) value is the same, though the sign will flip.\n\n[1] 0.71\n[1] -0.71\n\nReturn to Top\nM+S Chapter 11: Continuous Distributions\nThere are no exercises for this chapter, but it is good to read.\nFPP Chapter 10,11,12,16,17\nReturn to Top\nExercises\nFPP Chapter 10\nA.1 In a certain class, midterm scores average out to 60 with an SD of 15, as do scores on the final. The correlation between midterm scores and final scores is about 0.5. Estimate the average final score for the students whose midterm scores were \\([75, 30, 60]\\).\nFor the first answer the value is one standard deviation above so \\(75-7.5 = 67.5\\), for the second \\(30 + 15 = 45\\), and for the third it is \\(60 - .5(0) = 60\\).\nA.4. For women age 25-34 in the US in 2005, with full-time jobs, the relationship between education (years of schooling completed) and personal income can be summarized as follows: average education is approximately 14 years with a SD of 2.4 years. Average income is approximately $32000 with a SD of $26000 and a correlation of approximately .34. Estimate the average income of those women who have finished high school but have not gone on to college.\nThe women in the problem have completed 12 years of schooling, which is two years below average. That implies that they are \\(\\frac{2}{2.4} \\approx .83\\) SD below average in schooling. The estimate is then \\(.83r \\approx .28\\) SD in income or \\(.28(26000) \\approx 7300\\). Their average income is estimated as \\(32000-7300 = 24700\\)\nA.5 Suppose \\(r = -1\\). Can you explain why a one-SD increase in \\(x\\) is matched by a one-SD decrease in y?\nIn this case, the variables are perfectly negatively correlated. Since in a bivariate regression, the coefficient of interest is \\(\\rho\\frac{\\sigma_y}{\\sigma_x}\\), that means that any increase in the standard deviation of \\(x\\) must be meant by a corresponding decrease in the standard deviation of \\(y\\)\nB.1 From the picture. True or false: There is a positive association between husband’s income and wife’s income?\nTrue. We can imagine that individuals pair with other individuals who are higher earners.\nWhy is the dot at 127,500 so far below the regression line?\nChance error. There are few couples behind this dot.\nIf you use the regression line to estimate wife’s income from husband’s income would your estimates generally be a little to high, little too low, or just right for the couples in the sample with husbands income in the range 65k to 80k.\nB.3 This is a picture clue\nD.1 The criticism of all landings is not warranted. This is a textbook case of regression to the mean.\nD.2 No, tutoring appears to have an effect in this case because students scored above the standardized average. The regression effect would simply pull their scorers closer to the standardized average.\nD.3. This is a picture clue.\nE.1 For the men age 18-24 in the HANES5 sample, the ones who were 63 inches tall averaged 138 pounds in weight. True or false, the ones who weighed 138 pounds must have averaged 63 inches in height.\nFalse. This is a different group of men.\nE.2 In Pearson’s study, the sons of the 72 inch fathers only averaged 71 inches in height. True or false, if you take the 71 inch sons, their fathers will average about 72 inches in height.\nFalse, this is mixing up the regression lines.\nE.3. This question will also be false for similar reasons as above.\nReturn to Top\nFPP Chapter 11\nA.3 Compute the prediction errors and rms error of the regression line for the following dataset.\n\n[1] -7  1  3 -1  4\n[1] 3.898718\n\nA.4 This is a picture clue\nA.6 An admissions officer is trying to choose between two methods of predicting first year scores. One method has a RMS error of 12, the other 7. All else being equal he should choose the one with a lower RMS\nB.1 The RMS error for a regresion line of y on x can be figured as \\(\\sqrt{1-r^2}\\sigma_y\\). In this problem, that means \\(10\\sqrt{1-.6^2}\\).\nB.2 If you have no information about x, you guess the mean of y, which here is 65, your rms would be 10. If you have information, you’d use the regression line and your rms would be the same as B.1.\nB.3 At a certain college, first year GPAs average around 3.0 with an SD of about 0.5; they are correlated about 0.6 with high-school GPA. Person A predicts first year GPAs using just the average. Person B predicts via regression using high school GPAs.\nIt is better to use the second method as it will be smaller by a factor of \\(\\sqrt{1-.6^2}\\).\nD.3 You have average education = 13, SD = 3.4. Average income = 18000, SD 20000, r \\(\\approx .37\\). Find the RMSE of the regression line for predicting income from education.\n\\(20000\\sqrt{1-.37^2} = 18580.64\\)\nPredict the income of a woman with 16 years of education. Using regression you’ll find this is 24,500.\nThe prediction is likely off by can’t be determined with this information.\nD.7 In one study of identical male twins, the average height was found to be about 68 inches with a SD of 3. The correlation between the heights of the twins was about .95 and the scatter diagram was football shaped.\nYou have to guess the height of one of the twins without any further information. Guess the average\nFind the RMS for the method in (a). 3, the RMS is just the SD.\nOne twin of the pair is standing in front of you. You have to guess the height of the other twin. Use the regression line\nFind the RMS for the method in c. \\(3\\sqrt{1-.95^2} \\approx .9\\)\nE.1 You’ll find the answers are 2% and 4% respectively.\nReturn to Top\nFPP Chapter 12\nA.1. A regression equation for predicting average income from education is: predicted income = 2000(Edu) + 5000. Predict the income for one of these men who has\n8 years of schooling \\(2000(8) + 5000 = 21,000\\)\n12 years of schooling \\(2000(12) + 5000 = 29,000\\)\n16 years of schooling \\(2000(16) + 5000 = 37,000\\)\nA.3. The average height of fathers is \\(\\approx 68\\) inches with a SD of 2.7. The average height of sons is \\(\\approx 69\\) inches with a SD of 2.7. \\(r \\approx 0.5\\). Find the regression line for predicting the height of a son from the height of a father. Find the reverse regression line.\nThe slope coefficient is \\(\\frac{.5(2.7)}{2.7} = .5\\). The intercept is \\(69(.5)=34.5\\). The line is \\(.5F + 34.5\\). For the reverse, the only change is the intercept \\(.5S + 33.5\\).\nA.4. An expert witness offers testimony that “regression is a substitute for controlled experiments. It provides a precise estimate of the effect of one variable on another.”\nThis is false regression only says something about how the average value of one variable relates to values of another variable in the population being observed, and says nothing without further assumptions about how y would respond if you intervene to change the value of x. It also presumes a linear relationship which may or may not be accurate. Note though that the analysis of controlled experiments has a regression interpretation, but this is outside of the mechanics of regression.\nB.1 For men age 25-34 in the HNES2 sample, the regression equation for predicting height from education is predicted height = (.25in)(education) + 66.75 inches. Predict the height of a man with 12 years of education; with 16 years of education. Does going to college increase a man’s height? Explain.\nThe first value is 69.75. The second is 70.75. No going to college does not increase a man’s height. The regression has an omitted variable (among other issues).\nB.3 A study is made of Math and Verbal SAT scores for the entering class at a certain college. The summary statistics are average M-SAT = 560, SD 120, average V-SAT = 540, SD 110, r = 0.66. The inestigator uses the SD line to predict V-SAT from the M-SAT score.\nif a student scores 680 on the M-SAT, the predidcted V-SAT score is 540+110 =650\nIf a student scores 560 on the M-SAT, the predicted V-SAT score is 540\nThe investigator’s rms error is greater than \\(\\sqrt{1-.66^2}110\\).\nReturn to Top\nFPP Chapter 16\nA.1 A machine has been designed to toss a coin automatically and keep track of the number of heads. After 1000 tosses, it has 550 heads. Express the chance error both in absolute terms and as a percentage of the number of tosses.\n50, 5%\nA.4 A coin is tossed and you win a dollar if there are more than 60% heads. Which is better 10 tosses or 100?\nThe former. There is more chance variability and so a greater likelihood of winning. Again assuming a fair coin.\nSame as above but 40% heads.\nThe latter now. The coin will have less chance variability.\nSame as above but now if between 40% and 60% heads.\nSame as b.\nSame as above but exactly 50% heads.\nThe former. There is more chance error and so more ways to come up with a specific number. We can actually show this in this case by noting that the coins follow a binomial distribution. In the first case, we have n = 10 and p = 0.5. In the second we have n = 100 and p = 0.5.\n\\(P(X = 5) = {10\\choose5}(.5)^5(.5)^5 \\approx .25\\) while \\(P(X = 50) = {100\\choose50}(.5)^50(.5)^50 \\approx .08\\)\nA.6 A box contains 20% red marbles and 80% blue marbles. A thousand marbles are drawn at random with replacement. One of the following statements is true and why?\nAbout 200 marbles are going to be red, give or take a dozen or so is the right answer. The first statement doesn’t take into account the chance error.\nA.9 It is possible for the swings to be negative in this problem because the chance error can be negative.\nB.2 One hundred draws are made at random with replacement from the box [1,2]. How small can the sum be? How large? How many times do you expect the ticket 1 to turn up? The ticket 2? About how much do you expect the sum to be?\n100 is the smallest. 200 is the largest. We’d expect both tickets to turn up about half the time. We expect the sum to be 150\nB.5 One ticket will be drawn at random from the box [1:10]. What is the chance that it will be 1? That it will be 3 or less? 4 or more?\n\\(\\frac{1}{10}\\), \\(P(X \\leq 3) = P(X=1)+P(X=2)+P(X=3) = .3\\), \\(P(X \\geq 4) = 1 - P(X \\leq 3) = .7\\)\nB.7 The first two options to this question are equivalent and correct.\nC.1 The first two options are equivalent for this problem. The third option is worse than the first.\nReturn to Top\nFPP Chapter 17\nA.1 Find the expected value for the sum of 100 draws at random with replacement from the box\n[0,1,1,6]. \\(\\sum_{i=1}^{100} E[0,1,1,6] = 100(2)=200\\)\n[-2,-1,0,2] \\(\\sum_{i=1}^{100} E[-2,-1,0,2] = 100(-.25)=-25\\)\n[-2, -1, 3] \\(\\sum_{i=1}^{100} E[-2, -1, 3] = 100(0)=0\\)\n[0,1,1] \\(\\sum_{i=1}^{100} E[0,1,1] = 100(\\frac{2}{3}) \\approx 66.66\\)\nB.1. One hundred draws are going to be made at random with replacement from the box [1,2,3,4,5,6,7].\nFind the expected value and standard error for the sum.\n\\(400\\). The standard error is \\(20\\).\nThe sum of the draws will be around 400 give or take 20 or so.\nSuppose you had to guess what the sum was going to be. What would you guess? Would you expect to be off by around 2,4, or 20?\n\\(400\\) and off by \\(20\\).\nB.3. Option 1 will be the correct answer.\nB.4. Fifty draws are made at random with replacement from the box [1,2,3,4,5]; the sum of the draws turns out to be 157. The expected value for the sum is 150. The observed value is 157. The chance error is the difference between the observed value and the expected value, which here is 7. The standard error is 10.\nC.1. One hundred draws will be made at random with replacement from the box [1,1,2,2,2,4].\nThe smallest the sum can be is \\(100(1) = 100\\) and the largest is \\(100(4) = 400\\).\nThe sum of the draws will be around \\(100(2) = 200\\) give or take 10.\nThe chance that the sum will be bigger than 250 is \\(\\approx 0\\).\nC.8. A box contains 10 tickets. Each ticket is marked with a whole number between -5 and 5. The numbers are not all the same; their average equals 0. There are two choices: A) 100 draws are made from the box and you win $1 if the sum is between -15 and 15. B) 200 draws are made from the box, and you win $1 if the sum is between -30 and 30. Which gives the best chance of winning?\nOption ii does. Multiplying the number of draws by 2 makes the standard error only goes up by \\(\\sqrt{2}\\).\nD.1 The formula short cut from this chapter is \\((B-S)\\sqrt{\\frac{1}{B}(1-\\frac{1}{B}})\\)\nWe can write two formulas to compute it. Using R:\n\n\nstd = function(x){\n  t = (x - mean(x))^2\n  return(sqrt(mean(t)))\n}\n\nshortcut = function(x){\n  if(mean(x %in% c(0,1)) == 1){\n    return((max(x) - min(x))*sqrt((sum(x)/length(x))*(1- sum(x)/length(x))))\n  }\n  return((max(x) - min(x)) * sqrt((1/max(x))*(1 - (1/max(x)))))\n}\n\nsame = function(x){\n  return(std(x) == shortcut(x))\n}\n\n## The four lists of numbers\nl = list(c(7,7,7,7,-2,-2),\n         c(0,0,0,0,5),\n         c(0,0,1),\n         c(2,2,3,4,4,4))\nlapply(X = l, FUN =same)\n\n[[1]]\n[1] FALSE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] FALSE\n\nE.1. A coin is tossed 16 times.\nThe number of heads is like the sum of 16 draws made at random with replacement from one of the following boxes. Which one and why? The answer will be [0,1]. The [head, tail] box is out because you cannot add words.\nThe number of heads will be around 8 give or take 2 or so.\nE.4. The standard error here is \\(\\sqrt{np(1-p)}\\). Plugging in 100 and \\(\\frac{1}{2}\\) gives a standard error of \\(5\\).\nE.9. A die is rolled 100 times. Someone figures the expected number of aces as \\(100\\frac{1}{6}\\) and the standard error is \\(\\sqrt{100}\\sqrt{\\frac{1}{6}\\frac{5}{6}}\\). Is this right? Answer yes or no and explain. Yes it’s right (at least approximately).\nReturn to Top\nR Sequence\nThe R sequence suggested timeline is 2-4 weeks, though learning to and practicing coding will occupy much of one’s time in quantitative research. Much of the material contained in the R sequence prep is available under the Coding tab, which offers both R and Python coding notes and exercises.\nReturn to Top\n\n\n\n",
      "last_modified": "2023-05-24T22:02:47-07:00"
    },
    {
      "path": "linalg.html",
      "title": "Linear Algebra",
      "description": "Notes on Linear Algebra.\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nMatrices\nMatrix Definition\nSquare Matrix\nSymmetric Matrix\nDiagonal Matrix\nScalar Matrix\nTriangular\nIdentity Matrix\n\nMatrix Manipulation\nTranspose\nMatrix Addition\nInner Product\nMatrix Multiplication\nScalar Multiplication\nVector Sums\nCentering Matrix\n\nMatrix Geometry\nVector Spaces\nLinear Dependence\nRank\nDeterminants\nNorms\nThe Cosine Law\n\nSolving Systems of Linear Equations\nTypes of Systems\nInverse Matrix\n\n\nThis page provides notes on linear algebra with a bias towards results that are particularly useful to doing statistics and machine learning. Most of the notation comes from Greene (2017) and Boyd and Vandenberghe (2018).\nMatrices\nMatrix Definition\nFirst we need some terminology. A matrix is a rectangular array of numbers.\n\\[\\begin{aligned}\nA &= [a_{jk}]  \\\\\nA &= [A]_{ik} \\\\\nA &= \\begin{pmatrix} a_{11} & a_{12} & ... & a_{1k} \\\\\n... & ... & ... & ... \\\\\na_{n1} & a_{n2} & ... & a_{nk}\n\\end{pmatrix}\n\\end{aligned}\\]\nWe always read any subscripted element of a matrix as “a row, column”. For example\n\\[\\begin{aligned}\nA &= \\begin{pmatrix} 1 & 2 & 3 \\\\\n4 & 5 & 6  \\\\\n7 & 8 & 9\n\\end{pmatrix}\n\\end{aligned}\\]\nThen \\(A_{11} = 1\\).\nA matrix is made up of a vectors. A vector is an ordered set of numbers arranged either in a row or column. A column vector can be thought of as a matrix with one column. A row vector can be thought of a matrix with one row.1\nSquare Matrix\nA square matrix is a matrix where the number of rows is equal to the number of columns.\nSymmetric Matrix\nThere are a few matrices that are very common in statistics. The first is a symmetric matrix which is where \\(a_{ik} = a_{ki}, \\forall i, k\\)\n\\[\\begin{aligned}\n\\begin{pmatrix}\n  1 & 2 & 5 \\\\\n  2 & 3 & 6 \\\\\n  5 & 6 & 4\n\\end{pmatrix}\n\\end{aligned}\\] is symmetric.\nDiagonal Matrix\nA diagonal matrix is a square matrix where all off-diagonal elements are 0, and the only non-zero elements are on the main diagonal.\n\\[\\begin{aligned}\n\\begin{pmatrix}\n  1 & 0 & 0 \\\\\n  0 & 3 & 0 \\\\\n  0 & 0 & 4\n\\end{pmatrix}\n\\end{aligned}\\]\nScalar Matrix\nA scalar matrix is a square matrix where every diagonal element is the same.\n\\[\\begin{aligned}\n\\begin{pmatrix}\n  2 & 0 & 0 \\\\\n  0 & 2 & 0 \\\\\n  0 & 0 & 2\n\\end{pmatrix}\n\\end{aligned}\\]\nTriangular\nA triangular matrix is a matrix that has only zeros either above or below the main diagonal. If the zeros are above the diagonal, it is lower triangular. If the zeros are below the diagonal, it is upper triangular.\n\\[\\begin{aligned}\n\\begin{pmatrix}\n  1 & 0 & 0 \\\\\n  2 & 3 & 0 \\\\\n  5 & 6 & 4\n\\end{pmatrix}\n\\end{aligned}\\]\nIdentity Matrix\nA special kind of scalar, diagonal, symmetric, and triangular matrix is the idential matrix. We always denote this as \\(\\textbf{I}\\) with a subscript to denote the number of columns (the order).\n\\[\\begin{aligned}\n\\textbf{I}_3\\begin{pmatrix}\n  1 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 1\n\\end{pmatrix}\n\\end{aligned}\\]\nReturn to Top\nMatrix Manipulation\nTwo matrices \\(\\textbf{A}\\) and \\(\\textbf{B}\\) are equal if and only if they have the same dimensions and each element of \\(\\textbf{A}\\) is the same as each element in \\(\\textbf{B}\\).\nFor example,\n\\[\\begin{aligned}\n\\textbf{A} &= \\begin{pmatrix}\n  1 & 2 & 5 \\\\\n  2 & 3 & 6 \\\\\n  5 & 6 & 4\n\\end{pmatrix} \\\\\n\\textbf{B} &= \\begin{pmatrix}\n  1 & 2 & 5 \\\\\n  2 & 3 & 6 \\\\\n  5 & 6 & 4\n\\end{pmatrix}\n\\end{aligned}\\]\nare equal to each other.\nTranspose\nThe transpose of a matrix is denoted \\(\\textbf{A}^T\\) or \\(\\textbf{A}^{'}\\) is the matrix that we get by rotating the rows and columns of \\(\\textbf{A}\\). Formally \\(\\textbf{B} = \\textbf{A}^T \\iff b_{ik} = a_{ki}\\).\nAn immediate implication of this definition is that any symmetric matrix \\(\\textbf{A}\\) is equal to \\(\\textbf{A}^T\\) and any matrix \\(\\textbf{A} = (\\textbf{A}^{T})^T\\).\n\\[\\begin{aligned}\n\\textbf{A} &= \\begin{pmatrix}\n  1 & 2 & 3 \\\\\n  4 & 5 & 6 \\\\\n  7 & 8 & 9\n\\end{pmatrix} \\\\\n\\textbf{A}^T &= \\begin{pmatrix}\n  1 & 4 & 7 \\\\\n  2 & 5 & 8 \\\\\n  3 & 6 & 9\n\\end{pmatrix}\n\\end{aligned}\\]\nMatrix Addition\nWe can add and subtract any two matrices of equal sizes (conformable). \\(\\textbf{C} = \\textbf{A} + \\textbf{B} = [a_{ik} + b_{ik}]\\). A special matrix known as the zero matrix is one where all elements are zero and can be any order. It plays the same role as zero does in regular addition.\nProperties of Matrix Addition\nMatrix addition is commutative. \\(\\textbf{A} + \\textbf{B} = \\textbf{B} + \\textbf{A}\\)\nThe proof is straight forward. Suppose \\(\\textbf{A}_{m \\times n}\\) and \\(\\textbf{B}_{m \\times n}\\). Let \\(\\textbf{C}_{m \\times n}\\) be the result of adding \\(\\textbf{A} + \\textbf{B}\\). Let \\(\\textbf{D}_{m \\times n} = \\textbf{B} + \\textbf{A}\\). Then:\n\\[\\begin{aligned}\nc_{ij} &= a_{ij} + b_{ij} \\\\\nd_{ij} &= b_{ij} + a_{ij} \\\\\nc_{ij} &= d_{ij}\n\\end{aligned}\\]\nMatrix addition is associative. \\((\\textbf{A} + \\textbf{B}) + \\textbf{B} = \\textbf{A} + (\\textbf{B} + \\textbf{C})\\)\nThe proof is slightly more involved, but only slightly. Assume all three matrices are the same order. Let \\(\\textbf{D} = \\textbf{B} + \\textbf{C}\\), \\(\\textbf{E} = \\textbf{A} + \\textbf{B}\\), \\(\\textbf{F} = \\textbf{A} + \\textbf{D}\\), and \\(\\textbf{G} = \\textbf{E} + \\textbf{C}\\). Then:\n\\[\\begin{aligned}\nd_{ij} &= b_{ij} + c_{ij} \\\\\ne_{ij} &= a_{ij} + b_{ij} \\\\\nf_{ij} &= a_{ij} + d_{ij} \\rightarrow a_{ij} + b_{ij} + c_{ij}\\\\\ng_{ij} &= e_{ij} + c_{ij} \\rightarrow a_{ij} + b_{ij} + c_{ij}\n\\end{aligned}\\].\nWe observe that both the new matrices F and G are the same order and that their elements are the same, completing the proof.\nMatrix addition with transposes follows \\((\\textbf{A} +\\textbf{B})^T = \\textbf{A}^T + \\textbf{B}^T\\).\nThe proof is straight forward. The \\((i,j)\\) entry of \\((\\textbf{A} +\\textbf{B})^T\\) is the sum of \\((i,j)\\) entries of \\(\\textbf{A}^T + \\textbf{B}^T\\). which are \\((j,i)\\) entries of A and B. That means that the \\((i,j)\\) entry of \\(\\textbf{A}^T + \\textbf{B}^T\\) is the \\((j,i)\\) entry of the sum of \\(\\textbf{A} +\\textbf{B}\\) , which is equal to the \\((i,j)\\) entry of the transpose \\((\\textbf{A} +\\textbf{B})^T\\).\nInner Product\nWe multiply matrices and vector by using the inner product (also known as the dot product). The inner product of two vectors a and b is \\(a^Tb = a_1b_1 + ... + a_nb_n\\). Of course provided they are conformable it is also the case that \\(a^Tb = b^Ta\\).\nMatrix Multiplication\nFor an \\(n \\times k\\) matrix A and a \\(k \\times m\\) matrix B, the product matrix \\(\\textbf{C} = \\textbf{A}\\textbf{B}\\) is a \\(n \\times m\\) matrix whose \\(ik^{th}\\) element is the inner product of row i in A and column k in B.\n\\[\\begin{aligned}\n\\textbf{C} = \\textbf{A}\\textbf{B} \\rightarrow c_{ik} = a_i^Tb_k\n\\end{aligned}\\]\nMatrices can only be multiplied if they are conformable. In simple terms, the number of columns in A must be equal to the number of rows in B. It is not generally the case that matrix multiplication is commutative. For example consider the two matrices:\n\\[\\begin{aligned}\nA &= \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix} \\\\\nB &= \\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{pmatrix}\n\\end{aligned}\\]\nIt is possible to multiply AB, but not possible to multiply BA because the latter has non-conformable arguments. Even when A and B have the same dimensions, it is generally the case that they will not be equal. Here’s a simple example.\n\\[\\begin{aligned}\nAB &= \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{pmatrix} = \\begin{pmatrix} 19 & 22 \\\\ 43 &50 \\end{pmatrix} \\\\\nBA &= \\begin{pmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 2 \\\\\n4 & 5\n\\end{pmatrix} = \\begin{pmatrix} 23 & 34 \\\\ 31 & 46 \\end{pmatrix}\n\\end{aligned}\\]\nBecause it is not the case that matrix multiplication is commutative, we have two different ways of multiplying. We can premultiply or postmultiply. In AB, B is premultiplied by A and A is postmultiplied by B.\nScalar Multiplication\nFor any matrix (or vector), scalar multiplication is the operation of multiplying every element by a given scalar \\(c\\textbf{A} = [ca_{ik}]\\).\nThe product of a matrix and a vector is written \\(\\textbf{c} = \\textbf{A}\\textbf{b}\\). In order to perform this operation, the number of elements in b must be equal to the number of columns in A. It’s the equivalent of multiplying a \\(n \\times k\\) matrix by a \\(k \\times 1\\) matrix.\nProperties of Matrix Multiplication\nAssociative Law: (AB)C = A(BC)\nProof:\nDistributive Law: A(B + C) = AB + AC\nProof:\nTranspose of a Product: (AB)’ = B’A’\nProof:\nTranspose of an extended product: (ABC)’ = C’B’A’\nProof:\nVector Sums\nThere are some common summations that are useful to see in matrix form.\nLet i be a column vector of ones. \\(\\textbf{ix} = \\sum_{i=1}^nx_i\\).\nFor any constant c then \\(\\textbf{x} = c\\textbf{i} = \\sum_{i=1}^ncx_i = c\\sum_{i=1}^nx_i = a\\textbf{i}^T\\textbf{x}\\).\nThe sum of squares of the elements of x is \\(\\textbf{x}^T\\textbf{x}\\).\nCentering Matrix\nA fundamental matrix that we use to transform data to deviations from their mean is the centering matrix\n\\[\\begin{aligned}\n\\textbf{i}\\bar{x} &= \\textbf{i}\\frac{1}{n}\\textbf{i}^T\\textbf{x} \\\\\n\\textbf{i}\\bar{x} &= \\begin{pmatrix} \\bar{x} \\\\ \\bar{x} \\\\\\cdot \\\\ \\cdot \\\\ \\bar{x}\\end{pmatrix} \\\\\n\\textbf{i}\\bar{x} &= \\frac{1}{n}\\textbf{i}\\textbf{i}^T\\textbf{x}\n\\end{aligned}\\]\nThe first matrix \\(\\frac{1}{n}ii^T\\) is an \\(n \\times n\\) matrix with every element equal to \\(\\frac{1}{2}\\). This means that the set of values is \\(\\textbf{x} - \\frac{1}{2}ii^Tx\\).\nSince \\(x = \\mathbb{I}x\\) this implies that we can write this as \\([\\mathbb{I} - \\frac{1}{n}ii^T]x = \\textbf{M}^0\\textbf{x}\\). \\(M^0\\) is an idempotent matrix, which means that it is equal to its square, and it is symmetric so \\((M^0)^TM^0 = M^0\\).\nThe matrix is particularly useful when computing sums of squared deviations. For example, for a single variable x the sum of squared deviations about the mean is \\(\\sum_{i=1}^n(x_i - \\bar{x})^2 = (\\sum_{i=1}^nx_i^2) - n\\bar{x}^2\\).\nThe equivalent in matrix notation is:\n\\[\\begin{aligned}\n\\sum_{i=1}^n(x_i - \\bar{x})^2 &= (\\textbf{x} - \\bar{x}\\textbf{i})(\\textbf{x} - \\bar{x}\\textbf{i}) \\\\\n\\sum_{i=1}^n(x_i - \\bar{x})^2 &= (M^0x)^T(M^0x) \\\\\n\\sum_{i=1}^n(x_i - \\bar{x})^2 &= x^TM^0M^0x \\\\\n\\sum_{i=1}^n(x_i - \\bar{x})^2 &= x^TM^0x\n\\end{aligned}\\]\nMatrix Geometry\nVector Spaces\nA vector space is any set of vectors that is closed under scalar multiplication and addition.\n- Closed under multiplication means that every scalar multiple of a vector is also in the set of vectors.\n- Closed under addition means that the sum of any two vectors in the space is also a vector in the space.\nA basis is a set of vectors in a vector space that are linearly independent and any vector in the vector space can be written as a linear combination of that set of vectors.\nFor example, consider three arbitrary vectors in \\(\\mathbb{R}^2\\), a,b, c. If a and b are a basis, then we can find some numbers \\(\\alpha_1\\) and \\(\\alpha_2\\) such that \\(c = \\alpha_1 a + \\alpha_2 b\\). The solutions to this problem turn out to be:\n\\[\\begin{aligned}\n\\alpha_1 = \\frac{b_2c_1 - b_1c_2}{a_1b_2 - b_1a_2} \\\\\n\\alpha_2 = \\frac{a_1c_2 - a_2c_1}{a_1b_2 - b_1a_2}\n\\end{aligned}\\]\nThese solution are unique unless the denominator is zero. In a world where this is true, it implies that b is a multiple of a and therefore does not point in different directions. Any basis of a vector space is not unqiue, but for any specific basis only one linear combination will produce another particularly vector in the vector space.\nLinear Dependence\nA set of \\(k \\geq 2\\) vectors is linearly dependent if at least one of the vectors in the set can be written as a linear combination of the others.\nFor example consider the vectors \\(a = [1,2]^T, b = [3,3]^T, c = [10,14]^T\\). As a whole they are linearly dependent \\(2a + b - .5c = 0\\), but any two of them are linearly independent.\nA set of vectors is linearly independent if and only if the only solution to \\(\\alpha_1 a_1 + \\cdot\\cdot\\cdot + \\alpha_k a_k = 0\\) is where all the coefficients are zero \\(\\alpha_i = 0, \\forall i \\in 1,..., k\\).\nA consequence of this is that another definition of a basis is any set of linearly independent vectors in the vector space. Because any \\((K+1)st\\) vector can be written as a linear combination of the K basis vectors, this also implies that any set of more than K vectors in \\(\\mathbb{R}^K\\) must be linearly dependent.\nThe spanning vectors are the set of all linear combinations of a set of vectors.\nRank\nA matrix is a set of column vectors. The number of columns in the matrix is equivalent to the number of vectors in the set, and the number of rows is the number of coordinates in each column vector.\nA column space of a matrix is the vector space that is spanned by its column vectors. The column rank of a matrix is the dimension of a vector space that is spanned by its column vectors.\nIt turns out to be the case (and an important theorem) that the column rank and the row rank of a matrix is the same, which also means that the row space and the column space have the same dimension. We say a matrix has full rank when all columns (or rows) are linearly independent of each other.\nA useful fact about the rank is that the rank of a matrix A \\(rank(A) = rank(A^TA) = rank(AA^T)\\)\nDeterminants\nThe determinant of a square matrix is a function of the elements of a matrix. Importantly, the determinant of a matrix is nonzero and that matrix is therefore non-singular if and only if it has full rank.\nDeterminants matter in part because a matrix is nonsingular if and only if its inverse exists, and in order for this to be true, the determinant of said matrix cannot be zero.\nNorms\nThe Euclidean length or norm of a vector e is given by the Pythagorean theorem: \\(\\lVert e \\rVert = \\sqrt{e^Te}\\).\nOrthogonal Vectors\nTwo nonzero vectors are orthogonal written as \\(a \\perp b\\) if and only if \\(a^Tb = b^Ta = 0\\)\nThe Cosine Law\nThe angle \\(\\theta\\) between two vectors satisfies \\(cos(\\theta) = \\frac{a^Tb}{\\lVert a \\rVert \\lVert b \\rVert}\\)\nSolving Systems of Linear Equations\nTypes of Systems\nThere are two types of systems of equations. A homogeneous system is of the form \\(Ax = 0\\). By definition, we know that a nonzero solution to this system exists if and only if the matrix A does not have full rank. A nonhomogeneous system of equations is of the form \\(Ax = b\\) where b is a nonzero vector.\nInverse Matrix\nTo solve nonhomogeneous systems, we need a square matrix B such that\n\\[\\begin{aligned}\nBAx &= Ix \\\\\nBAx &= x \\\\\nBAx &= Bb\n\\end{aligned}\\]\nIf such a matrix exists, then it is the inverse of A and denoted \\(A^{-1}\\). If A is nonsingular, then the unique solution to the nonhomogeneous systems equation is \\(x = A^{-1}b\\)\n\nUnless otherwise noted, a vector is always a column vector.↩︎\n",
      "last_modified": "2023-05-24T22:02:48-07:00"
    },
    {
      "path": "matrix-code.html",
      "title": "Linear Algebra Code",
      "description": "A Crash Course in Linear Algebra using R and Python\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nWhat’s here?\nVectors\nBlock and stacked vectors\nSome special vectors\nRandom Vectors\n\nVector Addition and Multiplication\nScalar Multiplication and division\nElementwise operations\nUsing what we’ve learned to confirm the distributive property\nInner Product\n\nMatrices\nAdditional Definitions\nMatrix Arithmetic\n\nNorms\nConceptual Definition\nEuclidean (\\(\\ell_2\\)) Norm\nManhattan (\\(\\ell_1\\)) Norm\nInfinity (\\(\\ell_\\infty\\)) Norm\nDistance\n\nTaylor Approximation\nRoot Mean Square\nCommon Transformations\nDemeaning a Vector\nCorrelation\n\nOrdinary Least Squares\nOLS in Matrix Form\nFrisch-Waugh-Lovell (FWL)\n\nCovariance Matrices and Standard Errors\nClassical Standard Errors\nSandwich Standard Errors\n\nPrincipal Components Analysis\nK-Means\nTensors\n\nWhat’s here?\nThis page provides code in R and Python for doing linear algebra. In Python, we make use of the NumPy library and the PyTorch library.\n\nimport numpy as np \nimport torch\n\nFor tensor processing in R, we’ll use the R port of torch\n\n\nlibrary(torch)\n\n\nFor more on the theory of Linear Algebra consult the Linear Algebra Notes.\nVectors\nThe simplest way to represent vectors in R is by using the vector data structure.\n\n\nx = c(-1.1, 0.0, 3.6, -7.2)\nlength(x) ## 4 \n\n[1] 4\n\nIn python:\n\nx = np.array([-1.1, 0.0, 3.6, -7.2])\nlen(x)\n4\n\nBlock and stacked vectors\nIn addition to creating vectors, we can concatenate vectors together to produce blocked and stacked vectors using the c() function.\n\n\nx = c(1,-2)\ny = c(1,1,0)\nz = c(x,y)\nz\n\n[1]  1 -2  1  1  0\n\nIn python:\n\nx = np.array([1,-2])\ny = np.array([1,1,0])\nz = np.concatenate((x,y))\nprint(z)\n[ 1 -2  1  1  0]\n\nSome special vectors\nThe Zeros vector is a default behavior of creating a vector with a given length.\n\n\nz = numeric(3)\nz\n\n[1] 0 0 0\n\nIn python:\n\nz = np.zeros(3)\nz\narray([0., 0., 0.])\n\nThe Ones vector can be made by way of the rep() function.\n\n\no = rep(1,3)\no\n\n[1] 1 1 1\n\nIn python:\n\no = np.ones(3)\no\narray([1., 1., 1.])\n\nRandom Vectors\nFor simulation, it is often useful to generate random vectors to check an implementation. Alternatively, we might want to test an identity with some ground truth we already know. To generate a random vector of size \\(n\\)\n\n\n## Generate a vector of three values from a uniform distribution.\nrunif(3)\n\n[1] 0.1829365 0.5387020 0.4805885\n\nIn python\n\n## By default this is on the [0,1) interval. \nnp.random.random(3)\narray([0.73646857, 0.59519615, 0.5038193 ])\n\nWe can generate additional vectors from certain distributions. The most common from a simulation standpoint are standard normal.\n\n\nrnorm(3)\n\n[1]  2.0228722  1.7164500 -0.9018789\n\nIn python\n\nnp.random.randn(3)\narray([ 0.4053525 , -0.09158996,  0.00116039])\n\nReturn to Top\nVector Addition and Multiplication\nIf x and y are vectors of the same size, then x+y and x-y give their element wise sum and difference respectively. R by default computes most vector operations element wise.\n\n\nx = c(1,2,3)\ny = c(100,200,300)\nx+y\n\n[1] 101 202 303\n\nIn python:\n\nx = np.array([1,2,3])\ny = np.array([100,200,300])\nx + y\narray([101, 202, 303])\nnp.add(x,y)\narray([101, 202, 303])\n\nScalar Multiplication and division\nIf a is a number and x is a vector, then we can express the scalar vector product as either a*x or x*a\n\n\na = 2 \nx = c(1,2,3)\na*x\n\n[1] 2 4 6\n\nx*a\n\n[1] 2 4 6\n\nIn python:\n\na = 2\nx = np.array([1,2,3])\na*x\narray([2, 4, 6])\nx*a\narray([2, 4, 6])\n\nElementwise operations\nWe can perform elementwise operations on both R vectors and NumPy arrays.\n\n\np_i = c(50,40,30)\np_f = c(52,38,33)\nout = (p_f - p_i)/p_i\nout\n\n[1]  0.04 -0.05  0.10\n\nIn python\n\np_i = np.array([50, 40, 30])\np_f = np.array([52,38,33])\nout = (p_f - p_i)/p_i\nprint(out)\n[ 0.04 -0.05  0.1 ]\n\nUsing what we’ve learned to confirm the distributive property\nThe distributive property \\(\\beta(a+b) = \\beta a + \\beta b\\) holds for any two n-vector a and b and any scalar \\(\\beta\\).\n\n\na = c(3,5,6)\nb = c(2,4,9)\nbeta = 5\nlhs = beta*(a+b)\nrhs = beta*a + beta*b\nprint(lhs)\n\n[1] 25 45 75\n\nprint(rhs)\n\n[1] 25 45 75\n\nlhs == rhs\n\n[1] TRUE TRUE TRUE\n\nIn python:\n\na = np.array([3,5,6])\nb = np.array([2,4,9])\nbeta = 5 \nlhs = beta*(a+b)\nrhs = beta*a + beta*b\nprint('lhs:', lhs)\nlhs: [25 45 75]\nprint('rhs:', rhs)\nrhs: [25 45 75]\nlhs == rhs\narray([ True,  True,  True])\n\nInner Product\nThe inner product of n-vector x and y is denoted \\(x^Ty\\)\n\n\nx = c(1,2,3,4)\ny = c(3,4,6,7)\n\n## t() is the transpose function in R\nt(x)%*% y\n\n     [,1]\n[1,]   57\n\nIn python:\n\nx = np.array([1,2,3,4])\ny = np.array([3,4,6,7])\nnp.inner(x,y) \n\n# Alternatively \n57\nx @ y\n57\n\nReturn to Top\nMatrices\nA matrix \\(\\textbf{X}\\) is an \\(m\\) x \\(n\\) data structure that is a rectangular array of scalar numbers. The numbers \\(x_{ij}\\) are components or elements of \\(\\textbf{X}\\). The transpose of a matrix is the \\(n\\) x \\(m\\) matrix \\(\\textbf{X}'\\)\n\n\n## Creating a matrix in R \nX = matrix(seq(1,16,1), \n           nrow = 4, \n           byrow = T)\nX\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\nWe can also create matrices from vectors or from data frames\n\n\n## Equivalent to above but with vectors \nX2 = rbind(1:4, 5:8,9:12,13:16)\nX2\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\n\n\n## via a data frame \ndf = data.frame(\n  x = 1:4,\n  y = 5:8,\n  z = 9:12,\n  w = 13:16\n)\nX3 = as.matrix(df)\nX3\n\n     x y  z  w\n[1,] 1 5  9 13\n[2,] 2 6 10 14\n[3,] 3 7 11 15\n[4,] 4 8 12 16\n\nIn python:\n\nX = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12], [13,14,15,16]])\nX\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12],\n       [13, 14, 15, 16]])\nX.shape\n(4, 4)\n\nSome other useful matrices\n\nnp.identity(4) \narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\nnp.zeros((4,4))\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\nnp.ones((4,4))\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\nReturn to Top\nAdditional Definitions\nTranspose\nThe transpose in R\n\n\nX_transpose = t(X)\nX_transpose\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nIn python:\n\nX_transpose = X.T\nX_transpose\narray([[ 1,  5,  9, 13],\n       [ 2,  6, 10, 14],\n       [ 3,  7, 11, 15],\n       [ 4,  8, 12, 16]])\n\nDiagonal\nA diagonal matrix with all elements not on the diagonal equal to zero is a diagonal matrix. By default, R creates an identity matrix with the diag() function.\n\n\ndM = diag(4)\ndM\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n\nTrace\nThe trace of a matrix is the sum of the diagonal elements\n\\[trace(X) = \\sum_{i=1}^n x_{ii}\\]\n\n\nmatrix_trace = function(mat){\n  return(sum(diag(mat)))\n}\n\nmatrix_trace(X)\n\n[1] 34\n\nInverses\nA matrix is invertible if it is not singular. The inverse is useful because we can use it to solve the system of equations represented by the matrix. An inverse indicates whether the system has a solution.\n\n\nA = rbind(c(1,-2,3), c(0,2,2), c(-4,-4,-4))\nsolve(A)\n\n              [,1] [,2]  [,3]\n[1,] -2.775558e-17 -0.5 -0.25\n[2,] -2.000000e-01  0.2 -0.05\n[3,]  2.000000e-01  0.3  0.05\n\nIn python\n\nA = np.array([[1,-2,3], [0,2,2], [-4,-4,-4]])\nnp.linalg.inv(A)\narray([[-2.77555756e-17, -5.00000000e-01, -2.50000000e-01],\n       [-2.00000000e-01,  2.00000000e-01, -5.00000000e-02],\n       [ 2.00000000e-01,  3.00000000e-01,  5.00000000e-02]])\n\nReturn to Top\nMatrix Arithmetic\nAddition and subtraction of matrices of the same order are performed element by element. Scalar multiplication is element by element\n\n\nA = matrix(data = seq(1,9,1), nrow = 3, byrow = T)\nB = matrix(data = seq(1,9,1), nrow = 3, byrow = T)\nA+B\n\n     [,1] [,2] [,3]\n[1,]    2    4    6\n[2,]    8   10   12\n[3,]   14   16   18\n\nProvided that the number of columns of A equals the number of rows of B, we can multiple A by B.\n\n\n## To get the appropriate multiplication, we wrap * in %*%\nA%*%B\n\n     [,1] [,2] [,3]\n[1,]   30   36   42\n[2,]   66   81   96\n[3,]  102  126  150\n\nNote that we can perform X’X in one of two ways.\n\n\nt(A)%*%A \n\n     [,1] [,2] [,3]\n[1,]   66   78   90\n[2,]   78   93  108\n[3,]   90  108  126\n\n## same but can be slightly faster \ncrossprod(A)\n\n     [,1] [,2] [,3]\n[1,]   66   78   90\n[2,]   78   93  108\n[3,]   90  108  126\n\nIn python:\n\nA = np.array([[1,2,3], [4,5,6], [7,8,9]])\nB = np.array([[1,2,3], [4,5,6], [7,8,9]])\nA+B\narray([[ 2,  4,  6],\n       [ 8, 10, 12],\n       [14, 16, 18]])\nnp.add(A,B)\n\n# Multiplication \narray([[ 2,  4,  6],\n       [ 8, 10, 12],\n       [14, 16, 18]])\nnp.matmul(A,B)\narray([[ 30,  36,  42],\n       [ 66,  81,  96],\n       [102, 126, 150]])\nnp.matmul(A.T, A)\narray([[ 66,  78,  90],\n       [ 78,  93, 108],\n       [ 90, 108, 126]])\n\nReturn to Top\nNorms\nConceptual Definition\nInformally, the norm of a vector tells us how big it is. Formally, a norm is a function \\(\\lVert \\cdot \\rVert\\) that maps a vector to a scalar which satisfies:\nGiven any vector x \\(\\lVert \\alpha x \\rVert = \\lVert \\alpha \\rVert \\lvert x \\rVert\\).\nFor any vectors x, y, norms satisfy the triangle inequality \\(\\lVert x + y \\rVert \\leq \\lVert x \\rVert + \\lVert y \\rVert\\)\nThe norm of a vector \\(\\lVert x \\rVert > 0,\\forall x \\neq 0\\)\nEuclidean (\\(\\ell_2\\)) Norm\nThe most common norm is the \\(\\ell_2\\) norm or Euclidean norm which is defined as \\(\\lVert x \\rVert_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\\)\n\n\nx = c(3,-4)\nl2 = sqrt(crossprod(x,x))\nl2\n\n     [,1]\n[1,]    5\n\n## We can also call R's built in norm() function \nnorm(x, \"2\")\n\n[1] 5\n\nIn python\n\nx = np.array([3,-4])\nnp.linalg.norm(x)\n5.0\n\nManhattan (\\(\\ell_1\\)) Norm\nWe sometimes take the Manhattan norm (\\(\\ell_1\\)) which sums the absolute values of a vectors elements \\(\\lVert x \\rVert_1 = \\sum_{i=1}^n|x_i|\\)\n\n\nx = c(3,-4)\nl1 = function(x){\n  return(sum(abs(x)))\n}\nl1(x)\n\n[1] 7\n\nIn python\n\nx = np.array([3,-4])\nnp.linalg.norm(x, 1)\n7.0\n\nInfinity (\\(\\ell_\\infty\\)) Norm\nThe infinity norm or max norm (\\(\\ell_\\infty\\)) is common in machine learning applications defined as \\(\\lVert x \\rVert_\\infty = \\max_i |x_i|\\), which simplifies to the absolute value of the element with the largest magnitude in the vector.\n\n\nx = matrix(c(3,-4), byrow = T, nrow = 2)\nnorm(x, type =\"I\")\n\n[1] 4\n\nl_inf = function(x){\n  return(max(abs(x)))\n}\nl_inf(c(3,-4))\n\n[1] 4\n\nIn python\n\nx = np.array([3,-4])\nmax(abs(x))\n4\n\nDistance\nThe distance between two vectors is \\(d(x,y) = \\lVert x-y \\rVert\\).\n\n\nl2 = function(x, y){\n  d = x - y \n  return(sqrt(t(d)%*%d))\n}\nu = c(1.8, 4, 6, 24)\nv = c(2,4,6,8)\nl2(u,v)\n\n         [,1]\n[1,] 16.00125\n\nIn python\n\nu = np.array([1.8, 4, 6, 24])\nv = np.array([2,4,6,8])\nnp.linalg.norm(u-v)\n16.00124995117569\n\nReturn to Top\nTaylor Approximation\nThe (first order) Taylor approximation of some function \\(f: \\textbf{R}^n \\rightarrow \\textbf{R}\\) at the point \\(z\\) is the affine function \\(\\hat{f}(x) = f(z) + \\nabla f(z)'(x-z)\\)\nIn python\n\n## Suppose f(x) = x1 + exp(x2 - x1)\nf = lambda x: x[0] + np.exp(x[1]-x[0])\ngrad_f = lambda z: np.array([1 - np.exp(z[1]-z[0]), np.exp(z[1] - z[0])])\nz = np.array([1,2])\n\nf_hat = lambda x: f(z) + grad_f(z) @ (x-z)\nf([1,2]), f_hat([1,2])\n(3.718281828459045, 3.718281828459045)\nf([.96, 1.98]), f_hat([.96, 1.98])\n(3.7331947639642977, 3.732647465028226)\n\nReturn to Top\nRoot Mean Square\nThe root mean square of a vector \\(x\\) is \\(\\frac{\\lVert x\\rVert}{\\sqrt{n}}\\).\n\n\nrms = function(x){\n  num = sqrt(t(x)%*%x)\n  den = sqrt(length(x))\n  return(num/den)\n}\nt = c(0,1.01, 0.01)\nx = cos(10*t) - 2*sin(11*t)\nrms(x)\n\n        [,1]\n[1,] 1.00939\n\nIn python\n\nrms = lambda x: (sum(x**2)**0.5)/(len(x)**0.5)\nt = np.array([0,1.01, 0.01])\nx = np.cos(10*t) - 2*np.sin(11*t)\nrms(x)\n1.0093902676233697\n\nReturn to Top\nCommon Transformations\nDemeaning a Vector\nA demeaned vector is the result of performing \\({x - E[x]\\textbf{1}}\\). If we want this to be normalized, we then divide by the standard deviation \\(\\sigma_x = \\frac{\\lVert x - E[x]\\textbf{1}\\rVert}{\\sqrt{n}}\\).\n\n\nscale_h = function(x){\n  demean = x - as.numeric(mean(x))\n  demean = as.vector(demean)\n  std = (sqrt(crossprod(demean)))/sqrt(length(x))\n  return(demean/std)\n}\n\nx = c(2,4,5,7)\nscale_h(x)\n\n[1] -1.3867505 -0.2773501  0.2773501  1.3867505\n\nIn python\n\ndef scale_h(x):\n  demean = x - sum(x)/len(x)\n  std = np.linalg.norm(x - sum(x)/len(x))/(len(x)**0.5)\n  return demean / std\n\nx = np.array([2,4,5,7])\nscale_h(x)\narray([-1.38675049, -0.2773501 ,  0.2773501 ,  1.38675049])\n\nCorrelation\nThe correlation coefficient between two vectors \\(a\\) and \\(b\\) is defined \\(\\rho = \\frac{\\tilde{x}^T\\tilde{y}}{\\lVert \\tilde{x} \\rVert \\lVert \\tilde{y} \\rVert}\\) presuming that both standard deviations are non-zero. The \\(\\tilde{x}\\) means that we have taken a demeaned version of \\(a\\).\n\n\ncorr_c = function(x,y){\n  x_tilde = x - as.numeric(mean(x))\n  y_tilde = y - as.numeric(mean(y))\n  x_tn = sqrt(crossprod(x_tilde))\n  y_tn = sqrt(crossprod(y_tilde))\n  return((t(x_tilde)%*%y_tilde)/ (x_tn*y_tn))\n}\n\nx= c(4.4, 9.4, 15.4, 12.4, 10.4, 1.4, -4.6, -5.6, -0.6, 7.4)\ny = c(6.2, 11.2, 14.2, 14.2, 8.2, 2.2, -3.8, -4.8, -1.8, 4.2)\ncorr_c(x,y)\n\n          [,1]\n[1,] 0.9678196\n\nIn python\n\ndef corr_c(x,y):\n  x_tilde = x - sum(x)/len(x)\n  y_tilde = y - sum(x)/len(x)\n  denom = np.linalg.norm(x_tilde) * np.linalg.norm(y_tilde)\n  return (x_tilde @ y_tilde)/denom \n\nx = np.array([4.4, 9.4, 15.4, 12.4, 10.4, 1.4, -4.6, -5.6, -0.6, 7.4])\ny = np.array([6.2, 11.2, 14.2, 14.2, 8.2, 2.2, -3.8, -4.8, -1.8, 4.2])\ncorr_c(x,y)\n0.9678196342570434\n\nReturn to Top\nOrdinary Least Squares\nOLS in Matrix Form\nThe simplest linear model expresses the dependence of a dependent or response variable y on independent variables \\(x_1,.., x_p\\) and is usually written \\(y = X\\beta + \\epsilon\\). See the Lecture Notes for more details on the properties of this model.\nDefine the design matrix as the \\(n \\times p\\) matrix of independent variables \\(x_1,..,x_p\\) and assume that the first columns is a column of ones and that the design matrix has full rank. Then the usual OLS estimator is defined as \\((X'X)^{-1}X'Y\\)\n\n\nbeta_estimator = function(X,y){\n  X = cbind(rep(1,nrow(X)), X)\n  betas = solve(t(X)%*%X)%*%t(X)%*%y\n  return(betas)\n}\n\n## example data \nset.seed(123)\nx1 = rnorm(10000)\nx2 = rnorm(10000)\ny = 2*x1 + 4*x2 + runif(10000)\n\nX = cbind(x1, x2)\nbeta_estimator(X,y)\n\n        [,1]\n   0.4997833\nx1 1.9971656\nx2 4.0020879\n\nIn python:\n\nx1 = np.random.default_rng(seed=123).normal(0, 1, size =1000)\nx2 = np.random.default_rng().normal(0, 1, size =1000)\nones = np.ones(1000)\ny = 2*x1 + 4*x2 + np.random.default_rng().uniform(size = 1000)\n\nX = np.concatenate((ones, x1, x2)).reshape((-1,3), order = 'F')\n\n# Alternatively we can make a matrix with column_stack()\nX = np.column_stack((ones, x1, x2))\n\n# (X'X)^-1X'y\nnp.linalg.inv(X.T @ X) @ X.T @ y\narray([0.48629052, 2.01128435, 4.00035884])\n\nFrisch-Waugh-Lovell (FWL)\nFWL states that any coefficient in an OLS fit is equivalent to the coefficient estimated from a bivariate model in which the residualized outcome is regressed onto the residualized component of the predictors, and the residuals are taken from models regressing the outcome and the coefficient on all other coefficients in the OLS fit separately.\n\n\nfwl = function(){\n  ### Regress X1 on X2 (Partialling out/orthogonalization)\n  \n  ### Compute the residuals x1_tilde (variation not explained by X2)\n  \n  ### regress Y on X2 \n  \n  ### Compute residuals y_tilde (variation not explained by X2)\n  \n  ### regress y_tilde on x1_tilde\n}\n\n\nIn python\n\ndef FWL():\n  pass\n\nReturn to Top\nCovariance Matrices and Standard Errors\nClassical Standard Errors\nThe least squares solution gives us point estimates for coefficients, but if we want to do inference, we need to get standard errors. See the Lecture Notes for more details.\nTo get standard errors, we must first calculate the covariance matrix of our estimates and then take the square root of the diagonal.\n\n\nbeta_estimator = function(X,y){\n  X = cbind(rep(1,nrow(X)), X)\n  betas = solve(t(X)%*%X)%*%t(X)%*%y\n  return(betas)\n}\n\nbetas_and_std_errors = function(X,y){\n  betas = beta_estimator(X,y)\n  ## get the design matrix again\n  X = cbind(rep(1,nrow(X)), X)\n  residuals = y - X %*% betas \n  \n  ## Degree of freedom calculation \n  p = ncol(X) - 1 \n  df = nrow(X) - p - 1 \n  \n  ## Residual variance \n  res_var = sum(residuals^2) / df \n  \n  ## Covariance matrix of estimate \n  ## cov(\\hat{\\beta}|X) = (X'X)^-1X'cov(\\epsilon|X)X(X'X)^-1\n  beta_cov = res_var * solve(t(X)%*%X)\n  \n  ## Standard errors are square root of diagonal \n  return(list(beta = betas, se = sqrt(diag(beta_cov))))\n}\n\n## example data \n## To keep consistent with python examples I use pre-generated \n## random variables created in python with:\n# x1 = np.random.default_rng(seed=123).normal(0, 1, size =1000)\n# x2 = np.random.default_rng().normal(0, 1, size =1000)\n# ones = np.ones(1000)\n# y = 2*x1 + 4*x2 + np.random.default_rng().uniform(size = 1000)\n\nx1 = read.csv(\"x1.csv\", header = F) |>\n  unlist()\nx2 = read.csv(\"x2.csv\", header = F) |>\n  unlist()\ny = read.csv(\"y.csv\", header = F) |>\n  unlist()\nX = cbind(x1, x2)\n\nbetas_and_std_errors(X,y)\n\n$beta\n       [,1]\n   0.484434\nx1 2.005286\nx2 3.994715\n\n$se\n                     x1          x2 \n0.009244982 0.009198621 0.009201766 \n\nIn python\n\ndef betas_and_se(X,y):\n  betas = np.linalg.inv(X.T @ X) @ X.T @ y\n  residuals = y - X @ betas\n  df = X.shape[0] - X.shape[1]\n  res_var = np.sum(residuals**2) / df\n  cov_mat = res_var * np.linalg.inv(X.T @ X)\n  se = np.sqrt(np.diag(cov_mat))\n  return betas, se\n\n\nx1 = np.loadtxt(\"x1.csv\", delimiter = \",\", dtype = float)\nx2 = np.loadtxt(\"x2.csv\", delimiter = \",\", dtype = float)\ny = np.loadtxt(\"y.csv\", delimiter = \",\", dtype = float)\n\n# One way to make a matrix from vectors \n# X = np.concatenate((ones, x1, x2)).reshape((-1,3), order = 'F')\n\n# Alternatively we can make a matrix with column_stack()\nX = np.column_stack((ones, x1, x2))\n\nbetas, se = betas_and_se(X,y)\nprint(\"betas:\", betas)\nbetas: [0.48443399 2.00528572 3.99471469]\nprint(\"SE:\", se)\nSE: [0.00924498 0.00919862 0.00920177]\n\nSandwich Standard Errors\n\n\nbetas_and_std_errors_sandwich = function(X,y){\n  betas = beta_estimator(X,y)\n  ## get the design matrix again\n  X = cbind(rep(1,nrow(X)), X)\n  residuals = y - X %*% betas \n\n  ## Degree of freedom calculation \n  p = ncol(X) - 1 \n  df = nrow(X) - p - 1 \n  \n  \n  ## HC1 or Eicker-Huber_White Variance Estimator \n  ## This is a way of creating a diagonal matrix from a matrix\n  ## with one column in R. \n  u2 = matrix(diag(as.vector(residuals^2)), ncol = nrow(X))\n  beta_cov = (nrow(X)/df) * solve(t(X)%*%X) %*% t(X) %*% u2 %*% X  %*% solve(t(X)%*%X)\n  \n  ## Standard errors are square root of diagonal \n  return(list(beta = betas, se = sqrt(diag(beta_cov))))\n}\n\n## example data \nx1 = read.csv(\"x1.csv\", header = F) |>\n  unlist()\nx2 = read.csv(\"x2.csv\", header = F) |>\n  unlist()\ny = read.csv(\"y.csv\", header = F) |>\n  unlist()\nX = cbind(x1, x2)\n\nbetas_and_std_errors_sandwich(X,y)\n\n$beta\n       [,1]\n   0.484434\nx1 2.005286\nx2 3.994715\n\n$se\n                     x1          x2 \n0.009248444 0.009296868 0.009105095 \n\nIn python\n\ndef betas_and_std_errors_sandwich(X,y):\n  ## Calculate Beta coefficients\n  betas = np.linalg.inv(X.T @ X) @ X.T @ y\n  \n  ## Get residuals, degree of freedom, and squared residuals\n  residuals = y - X @ betas\n  df = X.shape[0] - X.shape[1]\n  u2 = residuals**2\n\n  ## apply the HC1 formula with appropriate correction \n  beta_cov = (X.shape[0]/df) * np.linalg.inv(X.T @ X) @ X.T @ np.diag(u2) @ X @ np.linalg.inv(X.T @ X)\n  se = np.sqrt(np.diag(beta_cov))\n  return betas, se\n\nx1 = np.loadtxt(\"x1.csv\", delimiter = \",\", dtype = float)\nx2 = np.loadtxt(\"x2.csv\", delimiter = \",\", dtype = float)\ny = np.loadtxt(\"y.csv\", delimiter = \",\", dtype = float)\n\nX = np.column_stack((ones, x1, x2))\n\nbetas, se = betas_and_std_errors_sandwich(X,y)\nprint(\"betas:\", betas)\nbetas: [0.48443399 2.00528572 3.99471469]\nprint(\"SE:\", se)\nSE: [0.00924844 0.00929687 0.00910509]\n\nReturn to Top\nPrincipal Components Analysis\nSuppose we have a collection of points in \\(\\mathbb{R}^n\\) and we want to encode these points to represent a lower-dimensional version of them. If \\(X'\\) is a \\(n \\times p\\) matrix, then the first principal component of \\(X'\\) is the linear combination of the \\(p\\) variables \\(y'_1 = (X-\\bar{X})'a_1\\) s.t \\(V(y_1)'\\) is maximized subject to the constraint that \\(a_1'a_1 =1\\). Subsequent principal components are defined successively in a similar way.\n\n\noptions(scipen=999)\n\nscale_and_center = function(x){\n  ## center columns \n  x_s = x - mean(x)\n  \n  ## return scaled columns \n  return(x_s/sd(x))\n}\n\nprcomp_by_hand = function(A) {\n  ## Calculate mean of each column\n  C = apply(A, 2, scale_and_center)\n  \n  ## Calculate covariance matrix of centered matrix\n  V = cov(C)\n  ## Eigendecomposition of covariance matrix\n  eig = eigen(V, symmetric = F)\n  ## Transpose eigenvectors\n  eig.t = t(eig$vectors)\n  ## calculate new dataset\n  A.new = eig.t %*% t(C)\n  df.new = t(A.new)\n  return(list(points = df.new, vectors = eig$vectors))\n}\n\nresults = prcomp_by_hand(USArrests)\nresults$vectors \n\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\nhead(results$points)\n\n                 [,1]       [,2]        [,3]         [,4]\nAlabama     0.9756604  1.1220012 -0.43980366  0.154696581\nAlaska      1.9305379  1.0624269  2.01950027 -0.434175454\nArizona     1.7454429 -0.7384595  0.05423025 -0.826264240\nArkansas   -0.1399989  1.1085423  0.11342217 -0.180973554\nCalifornia  2.4986128 -1.5274267  0.59254100 -0.338559240\nColorado    1.4993407 -0.9776297  1.08400162  0.001450164\n\nUsing built-in function in R.\n\n\n## As Brian Ripley pointed out on R-help back in 2003\n## using different compilers on the same machine and \n## the same version of R may give different signs for the eigenvectors. \n## The moral is, don't rely on the signs of eigenvectors! \n## (This is on the help page.)\nt = prcomp(USArrests, center = T,scale = T)\nhead(-1*t$rotation)\n\n               PC1        PC2        PC3         PC4\nMurder   0.5358995  0.4181809 -0.3412327 -0.64922780\nAssault  0.5831836  0.1879856 -0.2681484  0.74340748\nUrbanPop 0.2781909 -0.8728062 -0.3780158 -0.13387773\nRape     0.5434321 -0.1673186  0.8177779 -0.08902432\n\nhead(-1*t$x) \n\n                  PC1        PC2         PC3          PC4\nAlabama     0.9756604  1.1220012 -0.43980366 -0.154696581\nAlaska      1.9305379  1.0624269  2.01950027  0.434175454\nArizona     1.7454429 -0.7384595  0.05423025  0.826264240\nArkansas   -0.1399989  1.1085423  0.11342217  0.180973554\nCalifornia  2.4986128 -1.5274267  0.59254100  0.338559240\nColorado    1.4993407 -0.9776297  1.08400162 -0.001450164\n\nIn python\n\nnp.set_printoptions(suppress=True)\n\ndef scale(mat):\n  center = mat - np.mean(mat, axis = 0)\n  scale = center / np.std(mat, axis = 0, ddof = 1)\n  return scale\n\ndef pca(mat):\n  center = mat - np.mean(mat, axis = 0)\n  scale = center / np.std(mat, axis = 0, ddof = 1)\n  cov_mat = np.cov(scale, rowvar = False)\n  vals, vec = np.linalg.eig(cov_mat)\n  \n  ## Sort eigen vectors and eigen values in order \n  idx = (-vals).argsort()\n  vals = vals[idx]\n  vec = vec[:, idx]\n  ## Calculate new dataset \n  A_new = (vec.T @ scale.T).T\n  return vec, A_new\n\n## Test with the same UArrests dataset \n\n## We need to do a bit of cleaning of the raw dataset to \n## turn it into an appropriate matrix\nusarrests = np.loadtxt(\"USArrests.csv\", delimiter = \",\", dtype = str, skiprows = 1)\nusarrests = np.delete(usarrests, (0), axis = 1)\nusarrests = usarrests.astype(dtype = \"float\")\nvec, transformed = pca(usarrests)\nprint('Principal Components:', vec)\nPrincipal Components: [[ 0.53589947  0.41818087 -0.34123273  0.6492278 ]\n [ 0.58318363  0.1879856  -0.26814843 -0.74340748]\n [ 0.27819087 -0.87280619 -0.37801579  0.13387773]\n [ 0.54343209 -0.16731864  0.81777791  0.08902432]]\ntransformed[:6]\narray([[ 0.97566045,  1.12200121, -0.43980366,  0.15469658],\n       [ 1.93053788,  1.06242692,  2.01950027, -0.43417545],\n       [ 1.74544285, -0.73845954,  0.05423025, -0.82626424],\n       [-0.13999894,  1.10854226,  0.11342217, -0.18097355],\n       [ 2.49861285, -1.52742672,  0.592541  , -0.33855924],\n       [ 1.49934074, -0.97762966,  1.08400162,  0.00145016]])\n\nReturn to Top\nK-Means\nReturn to Top\nTensors\nTensors are generic \\(n^{th}\\) order arrays. Vectors are a 1st order tensor. Matrices are a second order tensor.\n\n\nt1 = torch_tensor(1)\n\n\nIn python using PyTorch\n\nt1 = torch.tensor(1.0)\nt1\ntensor(1.)\n\nReturn to Top\n\n\n\n",
      "last_modified": "2023-05-24T22:03:11-07:00"
    },
    {
      "path": "ml.html",
      "title": "Machine Learning",
      "description": "Notes on Machine Learning \n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2023-05-24T22:03:11-07:00"
    },
    {
      "path": "optim.html",
      "title": "Optimization",
      "description": "Notes on Optimization.\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2023-05-24T22:03:12-07:00"
    },
    {
      "path": "prob.html",
      "title": "Probability",
      "description": "Notes on Probability.\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nFundamentals\nSample Spaces, Event Spaces, Probability Axioms\nKolmogorov’s Axioms\nBasic Properties of Probability\nJoint and Continuous Probability\nThe Law of Total Probability\nIndependence\n\nRandom Variables\n\nThe notes and notation come predominantly from Aronow and Miller (2019), Pittman (1993), and Casella and Berger (2002).\nFundamentals\nSample Spaces, Event Spaces, Probability Axioms\nA set S of subsets of a sample space \\(\\Omega\\) is an event space if it satisfies:\n- Non-empty: \\(S \\neq \\emptyset\\)\n- Closed under complements: \\(A \\in S \\implies A^C \\in S\\)\n- Closed under countable unions: \\(A_1,A_2,\\cdot\\cdot\\cdot \\in S \\implies A_1 \\cup A_2 \\cup \\cdot \\cdot \\cdot \\in S\\)\nA set S of subsets of some other set \\(\\Omega\\) that satisfies these properties is known as a \\(\\sigma\\)-algebra on \\(\\Omega\\).\nA probability measure is a function \\(P: S \\rightarrow \\mathbb{R}\\) that assigns a probability to every event in the event space.\nKolmogorov’s Axioms\nLet \\(\\Omega, S, P\\) be defined as above. Then \\((\\Omega, S, P)\\) is a probability space if it satisfies:\n- Non-negativity: \\(\\forall A \\in S, P(A) \\geq 0\\) and \\(P(A)\\) is finite and real.\n- Unitarity: \\(P(\\Omega) = 1\\)\n- Countable additivity: If \\(A_1, A_2, ... \\in S\\) are pairwise disjoint \\(\\forall i \\neq j A_i \\cap A_j = \\emptyset\\) implies \\(P(A_1 \\cup A_2 \\cup ...) = P(A_1) + P(A_2) + \\cdot \\cdot \\cdot = \\sum_iP(A_i)\\)\nBasic Properties of Probability\nLet \\((\\Omega, S, P)\\) be a probability space. The following properties hold:\n- Monotonicity: \\(\\forall A,B \\in S\\) if \\(A \\subseteq B \\implies P(A) \\leq P(B)\\)\n- Subtraction Rule: \\(\\forall A,B \\in S\\) if \\(A \\subseteq B \\implies P(B\\A) = P(B) - P(A)\\)\n- Zero Probability of the empty set: \\(P(\\emptyset) = 0\\)\n- Probability Bounds: \\(\\forall A \\in S, 0 \\leq P(A) \\leq 1\\)\n- Complement Rule: \\(\\forall A \\in S, P(A^C) = 1 - P(A)\\)\nJoint and Continuous Probability\nThe joint probability for \\(A,B \\in S\\) of A and B is \\(P(A \\cap B)\\)\nThe Addition Rule\nFor \\(A,B \\in S, P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\). In words the probability of at least one of two events occurring is equal to the sum of the probabilities of each occurring minus the probability of both occurring.\nConditional Probability\nFor \\(A,B \\in S\\) the conditional probability of A given B is \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\) where \\(P(B) > 0\\).\nThe Multiplicative Law of Probability is a rearrangement \\(P(A|B)P(B) = P(A \\cap B)\\)\nBayes Rule\nFor \\(A,B \\in S\\) with \\(P(A) > 0\\) and \\(P(B) > 0\\)\n\\[\\begin{aligned}\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\\end{aligned}\\]\nThe Law of Total Probability\nTo understand the definition (and the utility) of the Law of Total Probability, first define a partition as the case if \\(A_1, A_2,... \\in S\\) are nonempty and pairwise disjoint and \\(\\Omega = A_1 \\cup A_2 \\cup ...\\) then \\(\\{A_1, A_2, ...\\}\\) is a partition of \\(\\Omega\\).\nPartitions divide the sample space into mutually exclusive and exhaustive categories. Every outcome in the sample space is contained in exactly one event, so exactly one event in the partition occurs for any draw from the probability space.\nThe Law of Total Probability states:\nIf \\(\\{A_1, A_2,... \\}\\) is a partition of \\(\\Omega\\) and \\(B \\in S\\) then \\(P(B) = \\sum_i P(B \\cap A_i)\\). This can be equivalently stated as \\(P(B) = \\sum_i P(B|A_i)P(A_i)\\) in the event that \\(P(A_i) > 0, i = 1,2,3,...\\).\nThe probability of an event B is thus a weighted average of the conditional probabilities of that event. The weights are the probabilities of the events that are being conditioned on.\nIndependence\nPresuming we have a random generative process, then events \\(A,B \\in S\\) are independent if \\(P(A \\cap B) = P(A)P(B)\\). This also means that for \\(A,B \\in S\\) with \\(P(B) > 0\\) A and B are independent if and only if \\(P(A|B) = P(A)\\).\nRandom Variables\nA random variable is a variable that takes on a real value that is determined by a random generative process. Formally, it is a function \\(X: \\Omega \\rightarrow \\mathbb{R}\\) such that \\(\\forall r \\in \\mathbb{R}, \\{\\omega \\in \\Omega : X(\\omega) \\leq r \\} \\in S\\)\n\n\n\n",
      "last_modified": "2023-05-24T22:03:13-07:00"
    },
    {
      "path": "py-coding.html",
      "title": "Python",
      "description": "Notes on Python Coding.\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nBasic Python Operations\nData Types\nNumeric Types\nStrings\nString Methods\nBoolean\n\nChanging between types\nLists and Tuples\nDictionaries\nMatrices\nData Frames\nSubsetting\nConditionals\nFunctions\n\nGraphing in Python\nThe Basics\nUseful Plots for Political Science\nCoefficient Plot\nAdded Variable Plots\nMarginal Effects Plots\n\n\nSimulations in Python\nThe Birthday Problem\n\n\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf \nimport torch \n\nBasic Python Operations\nData Types\nNumeric Types\nPython has three numeric data types: integers, floating points and complex numbers (not used here).\n\n# Integer \nx = 42 \ntype(x)\n\n# Floating Point \n<class 'int'>\ny = 42.0\ntype(y)\n<class 'float'>\n\nAll of the common arithmetic operations works on numeric types.\n\nx = 10 \n\n# Addition \nx + 2 \n\n# Subtraction \n12\nx - 2 \n\n# Multiplication \n8\nx * 2 \n\n# Division \n20\nx / 2 \n\n# Exponentiation \n5.0\nx ** 2\n\n# Integer Division always rounds down\n100\nx // 2\n\n# Modulo \n5\nx % 2\n0\n\nStrings\nText is stored as a data type called a string. Strings are sequences of characters. We enclose strings in either ’’ or ““.\n\ns = \"Hello World!\" \n\n# To enclose a sequence that includes an apostrophe always\n# use double quotes\ns2 = \"Hello World. Nice to meet'cha\" \n\nString Methods\nThere are a variety of string methods in python. Some of the most useful are .split(), .join(), .lower()\n\ns = \"Here is a string\" \ns.lower()\n'here is a string'\ns.split()\n['Here', 'is', 'a', 'string']\nk = s.split()\n\n\" \".join(k)\n'Here is a string'\n\nBoolean\nBoolean types have two values. Either True or False. These are special and reserved words in Python.\n\ntruth = True \n\nlies = False \n\nNote that we can perform arithmetic operations on boolean types.\n\nTrue ** 2\n1\nFalse + 2\n2\n\nBoolean types often come about as a result of comparisons.\n\nx = True \ny = False \n\n## Equality \nx == y\n\n## Not equal \nFalse\nx != y\n\n## Greater than \nTrue\nx > y \n\n## Greater than or equal \nTrue\nx >= y \n\n## Less than \nTrue\nx < y \n\n## Less than or equal \nFalse\nx <= y \n\n## The same object \nFalse\nx is y \n\n## are both true \nFalse\nx and y \n\n## is at least one true \nFalse\nx or y \n\n## is it false \nTrue\nnot x \nFalse\n\nChanging between types\nSometimes we need to cast a value from one type to another. For example, we have read in a set of numbers as strings and we need to make them numeric types to do arithmetic.\n\nx = \"1\" \ny = 1\nz = \"will not convert to numeric\" \n\n# Convert to float \nfloat(x)\n\n# Convert to integer \n1.0\nint(x)\n\n# Convert to string \n1\nstr(y)\n'1'\n\n\n# A failure to cast because there is no clear numeric \nfloat(z)\n\nLists and Tuples\nLists and tuples allow us to store multiple elements in a single object. The elements are ordered and generally when we reference them we use zero-based indexing, which means that the first element is in the 0th position. List use [] as brackets.\n\na_list = [1, 2, \"skip\", \"to\", \"my\", \"lou\"]\n\n# Get the length of the list\nlen(a_list)\n\n# Get the first element \n6\na_list[0]\n\n# Get the third element \n1\na_list[2]\n\n# Get the last element\n'skip'\na_list[-1]\n\n# Alternatively \n'lou'\na_list[5]\n\n# Get the third through fifth elements \n'lou'\na_list[2:5]\n['skip', 'to', 'my']\n\nNote that our example list is holding different data types! Lists can hold any data type. Lists are objects, which means they have built in methods for interacting with their data. We call a method by using a period. For example:\n\nb_list = [1,2,3,4]\n\n# Calling a method that adds 5 to the end of our list\nb_list.append(5)\nb_list\n\n## insert a number in between 3 and 4 \n[1, 2, 3, 4, 5]\nb_list.insert(4, 37)\nb_list\n[1, 2, 3, 4, 37, 5]\n\nThe fact that we can use a method like insert means that we can change the list. In other words, a list is mutable.\nTuples are like lists but cannot be changed once created. They are “immutable.” We use () to define them.\n\na_tuple = (1,2,3)\nlen(a_tuple)\n3\n\nDictionaries\nA dictionary is a mapping between key-value pairs and we use {} to define them. They become very useful once we introduce the pandas library.\n\nmascots = {\"Minnesota\": \"Goldy Gopher\",\"Cal\": \"Oski Bear\", \"Michigan State\": \"Sparty\",\"Stanford\": 0,\n}\nmascots[\"Minnesota\"]\n'Goldy Gopher'\n\nMatrices\nData Frames\nSubsetting\nConditionals\nConditionals allow us to program code where only certain blocks of code are executing depending on the state of our program.\n\nbest_mascot = \"Goldy Gopher\"\n\nif best_mascot == \"Goldy Gopher\":\n  print(\"Correct\")\nelif best_mascot == \"Bucky Badger\":\n  print(\"Extremely Wrong\")\nelse:\n  print(\"There is only one correct answer to this question.\")\nCorrect\n\nPython cares about white space, so notice that we use the keywords if, elif, and else, a : ends each conditional line, and we indent after appropriately. However, if we just have a single if statement we can write them on one line for simplicity.\n\nif best_mascot == \"Goldy Gopher\": print(\"Correct\")\n\n## We can also do one liner if else \nCorrect\nprint(\"Correct\") if best_mascot == \"Goldy Gopher\" else print(\"incorrect\")\nCorrect\n\nFunctions\nGraphing in Python\nThe Basics\nUseful Plots for Political Science\nCoefficient Plot\nAdded Variable Plots\nMarginal Effects Plots\nSimulations in Python\nThe Birthday Problem\n\n\n\n",
      "last_modified": "2023-05-24T22:03:38-07:00"
    },
    {
      "path": "r-coding.html",
      "title": "Coding Notes",
      "description": "Coding Notes and Resources\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nBasic R Operations\nData Types\nSubsetting\nControl Operations\nFunctions\n\nGraphing in R\nThe Basics\nUseful Plots for Political Science\nAdded Variable Plots\nMarginal Effects Plots\n\nSimulations in R\nThe Birthday Problem\n\n\n\n\nlibrary(broom)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(fixest)\nlibrary(ggplot2)\nlibrary(marginaleffects)\nlibrary(purrr)\n\n\nBasic R Operations\nThere are a voluminous amount of resources on R online. Here is a very minimal crash course.\nThe most important underlying concept in R is that (almost) everything is an object.\nData Types\nR has six basic data types. The four most common types we work with are vectors, lists, matrices, and data frames.\nVectors\n\n\n## Three ways to create a vector\nv = c(1,2,3,4,5)\nv2 = seq(from=1,to=5, by = 1)\n\n## Since the sequence is counting with no breaks we can also do this\nv3 = 1:5\n\n\nVectors can also be characters (strings in other languages)\n\n\nw = c(\"A\", \"B\", \"C\")\nw2 = c(\"1\",\"2\",\"3\")\n\n## We can convert to other types by the as.* series \nw3 = as.character(v)\nw4 = as.numeric(w2)\n\n\nLists\n\n\nl = list(v, w)\nl\n\n[[1]]\n[1] 1 2 3 4 5\n\n[[2]]\n[1] \"A\" \"B\" \"C\"\n\nMatrices\n\n\nm = matrix(seq(1,16,1), nrow = 4, byrow = T)\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\nFor much more about linear algebra in R, consult the Matrix Algebra in R section.\nData Frames\n\n\ndf = data.frame(\n  var1 = letters[1:5],\n  var2 = c(1,2,3,4,5)\n)\ndf\n\n  var1 var2\n1    a    1\n2    b    2\n3    c    3\n4    d    4\n5    e    5\n\nSubsetting\nControl Operations\nFunctions\nGraphing in R\nThe Basics\nR has default plotting, but for the purpose of these notes I exclusively use ggplot2. A main distinction between base plotting and ggplot2 is that ggplot2 requires a data frame and presumes that your data is tidy.\n\n\n## Using the built-in dataset diamonds\ndf = diamonds\n\n## Data\ndf |>\n  ## Mapping\n  ggplot(aes(x = carat, y = price)) +\n  ## geom\n  geom_point() +\n  ## we can have multiple geoms \n  geom_smooth(method = \"lm\", se = F)+\n  ## Labels\n  labs(x = \"Carat\",\n       y = \"Price\",\n       title = \"Example Chart\")+\n  ## theme \n  theme_minimal()\n\n\n\nFor more extensive discussion of graphing capabilities in R, check out Data Visualization: A Practical Introduction by Healy.\nUseful Plots for Political Science\nCoefficient Plot\nFirst let’s make a model using the gapminder dataset.\n\n\n## Make a model with robust standard errors using the gapminder\n## dataset\nm1 = fixest::feols(lifeExp ~ gdpPercap + pop + continent, data = gapminder::gapminder, vcov = \"HC1\") |>\n  tidy(conf.int = TRUE)\n\nhead(m1)\n\n# A tibble: 6 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  4.78e+1   3.79e-1    126.   0         4.71e+1   4.86e+1\n2 gdpPercap    4.50e-4   7.27e-5      6.19 7.67e-10  3.07e-4   5.92e-4\n3 pop          6.57e-9   1.52e-9      4.32 1.66e- 5  3.59e-9   9.55e-9\n4 continentA…  1.35e+1   6.98e-1     19.3  3.06e-75  1.21e+1   1.48e+1\n5 continentA…  8.19e+0   6.87e-1     11.9  1.44e-31  6.85e+0   9.54e+0\n6 continentE…  1.75e+1   9.88e-1     17.7  2.47e-64  1.55e+1   1.94e+1\n\n\n\nm1 |>\n  ggplot(aes(x = term, y = estimate, \n             ymin = conf.low, \n             ymax = conf.high))+\n  geom_point() +\n  geom_pointrange() + \n  coord_flip() +\n  theme_minimal() +\n  labs(x = \"\",\n       y = \"Coefficient Estimates\",\n       title = \"Coefficient Plot\")\n\n\n\nAdded Variable Plots\nMarginal Effects Plots\n\n\nmarginModel = fixest::feglm(lifeExp ~ gdpPercap + pop + continent, data = gapminder::gapminder, vcov = \"HC1\") |>\n  marginaleffects::slopes(by = TRUE)\n\n\nSimulations in R\nThe Birthday Problem\n\n\n\n",
      "last_modified": "2023-05-24T22:03:48-07:00"
    },
    {
      "path": "stats.html",
      "title": "Lecture Notes",
      "description": "Lecture Notes and Resources\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nSimple Linear Regression\nMultiple Linear Regression\nOLS Asympotics\n\nSimple Linear Regression\nWe are interested in studying models that take the following form:\n\\(y = \\beta_0 + \\beta_1x + u\\)\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope parameter and u is the error term. In the next set of notes, we will extend this model to situations where we have more than one covariate.\nWe can think of \\(\\beta_0 + \\beta_1x\\) as the systematic part of y whereas u is the unsystematic part of y. That is, u represents y not explained by x.\nError Term Assumptions\nIn order to make progress, we make the following assumptions about the error term.\n\\(E[u] = 0\\) as long as an intercept term is included in the equation. Note that this essentially defines the intercept.\n\\(E[u|x] = E[u] = 0\\). This is the Zero Conditional Mean Assumption for the error term.\nThe average value of the unobservables is the same across all slices of the population determined by the value of x and is equal to the average of u over the entire population\nBy EA.1 that means the average is 0\nDeriving OLS Estimates\nTo estimate the Population Regression Function (PRF), we need a sample.\nLet \\((x_i, y_i): i = 1,...,n\\) be a random sample of size n from the population. We can estimate the PRF by a model:\n\\(y = \\beta_0 + \\beta_1x_i + u_i\\) (E.2)\nError Assumption 2 implies that in the popuation x and u are uncorrelated, and the zero conditional mean assumption for the error implies that \\(E[u] = 0\\). This implies that the covariance between x and u is 0 or formally:\n\\(Cov(x,u) = E(xu) = 0\\)\nWe can rewrite previous equations as follows\n\\(E[u] = E[ y - \\beta_0 + \\beta_1x]\\) (E.3)\n\\(Cov(x,u) = E[x(y - \\beta_0 + \\beta_1x)]\\) (E.4)\nOur goal is to choose sample \\(\\hat{\\beta_0}\\),\\(\\hat{\\beta_1}\\) to solve the sample equations:\n\\(\\frac{1}{n}\\sum_{i=1}^n y - \\hat{\\beta_0} + \\hat{\\beta_1}x = 0\\) (E.5)\n\\(\\frac{1}{n}\\sum_{i=1}^n x_i(y - \\hat{\\beta_0} + \\hat{\\beta_1}x) = 0\\) (E.6)\nRewrite E.4\n\\(\\bar{y} = \\hat{\\beta_0} + \\hat{\\beta_1}\\bar{x}\\) which implies\n\\(\\beta_0 = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\nEstimating The Slope Parameter\nDrop the \\(\\frac{1}{n}\\) in E.5 because it does not affect the solution. Plug in \\(\\bar{y} - \\hat{\\beta_1}\\bar{x}\\) for \\(\\beta_0\\) which yields the equation\n\\(\\sum_{i=1}^n x_i(\\bar{y} - \\hat{\\beta_1}\\bar{x}) - \\hat{\\beta_1}x) = 0\\)\nRearrange terms to get the y’s and the x’s on opposite sides of the equation.\n\\(\\sum_{i=1}^n x_i(y_i - \\bar{y})\\)\n\\(\\hat{\\beta_1}\\sum x_i(x_i - \\bar{x})\\)\nSetting these equal to each other and using properties of the sum operator, we can rewrite the the top sum to be \\(Cov(x,y)\\) and the bottom sum to \\(V(x)\\). As long as \\(V(x) > 0\\),\n\\(\\hat{\\beta_1} = \\frac{\\hat{Cov(x,y)}}{\\hat{V(x)}}\\)\nIn words, the slope parameter estimate is the sample covariance of x and y divided by the sample variance of x. We refer to this as the OLS procedure and the OLS regression line as\n\\(\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x\\)\nAlgebraic Properties of OLS on Any Sample of Data\nThe following hold by construction for any sample of data estimated by OLS\nThe sum and therefore sample average of the residuals is 0. This is because the OLS estimates are chosen to make the residuals sum to 0.\nSample covariance between regressors and OLS residuals is 0\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the OLS regression line\nVariation in Y\nWe can view OLS as decomposing each \\(y_i\\) into two parts, a fitted value and a residual. There are three parts of this decomposition: the total sum of squares (SST), the explained sum of squares (SSE), and the residual sum of squares (SSR).\n\\(SST = \\sum_{i=1}^n (y_i -\\bar{y})^2\\)\n\\(SSE = \\sum_{i=1}^n (\\hat{y_i} -\\bar{y})^2\\)\n\\(SSR = \\sum_{i=1}^n \\hat{u}^2\\)\nSST is a measure of total sample variation in the \\(y_i\\)’s. Dividing SST by n-1 gets us the sample variance of y.\nThe Total Variation in y is SST = SSE + SSR.\nTo derive\n\\(\\sum_{i=1}^n (y_i -\\bar{y})^2\\)\n\\(\\sum_{i=1}^n [(y_i - \\hat{y_i}) + (\\hat{y_i}-\\bar{y})]^2\\)\n\\(\\sum_{i=1}^n \\hat{u_i} + (\\hat{y_i}-\\bar{y})]^2\\)\nExpand out the sum and replace with definitions to get\n\\(SSR + 2Cov(\\hat{u}, \\hat{y}) + SSE\\)\nSince the covariance between u and y is 0, that term drops out.\nGoodness of Fit\nThe ratio of the explained sample variation in y by x is known as \\(R^2\\) and defined:\n\\(R^2 = 1 - \\frac{SSR}{SST}\\)\nExpected Values and Unbiasedness of OLS Estimators\nOLS is an unbiased estimator of the population model provided the following assumptions hold. These assumptions are also known as Gauss-Markov assumptions.\nA1. Linear in paramters\nIn the population model, y is related to x and u\n$y = _0 + _1x + u $\nA2. Random Sample\nWe have a random sample of size n from the population model\nA3. Sample variation in x\nThe sample outcomes \\(x_i: i = 1,2,..., n\\) are not all the same value. If they are, there is no variance of X and so \\(\\beta_1\\) cannot be estimated.\nA4. Zero Conditional Mean of the Error\nFor a random sample, this assumption implies\n\\(E(u_i|x_i) = 0: \\forall i \\in [0,1,...n]\\)\nA4 is violated whenever we think that u and x are correlated. In the simple bivariate case, an example might be using the variable education to predict salary. education is correlated with many variables, including income and family history. These may affect salary and therefore will give us biased results.\nNote: We can write the slope estimator \\(\\beta_1\\) in a slightly different way\n\\(\\hat{\\beta_1} = \\frac{\\sum\\_{i=1}^n (x_i - \\bar{x})*(\\beta_0 - \\beta_1x + u_i)}{SST_x}\\)\n\\(\\hat{\\beta_1} = \\beta\\_0 \\sum\\_{i=1}^n\\) + _1_{i=1}^n x_i(x_i -{x}) + _{i=1}^n u_i(x_i - {x})$\nThe first term sums to 0 and drops out. Thus:\n\\(\\hat{\\beta_1} = \\beta_1 + \\frac{\\sum\\_{i=1}^n u_i(x_i - \\bar{x})}{SST_x}\\)\nWe now have all the information we need to prove that OLS is unbiased. Unbiasedness is a feature of the sampling distributions of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). Unbiasedness says nothing about the estimates for any given sample we may draw.\nTheorem 1: Using A1-A4 OLS produces unbiased estimates\n\\(E(\\hat{\\beta_0}) = \\beta_0\\) and \\(E(\\hat{\\beta_1}) = \\beta_1\\)for any values of \\(\\beta_0\\) and \\(\\beta_1\\).\nProof:\nIn this proof the expected values are conditional on sample values of the independent variable x. Because \\(SST_x\\) and \\((x_i - \\bar(x))\\) are functions on of \\(x_i\\) they are non-random once we condition on x.\n\\(E\\hat{\\beta_1}] = E\\beta_1 + \\frac{\\sum\\_{i=1}^n u_i(x_i - \\bar{x})}{SST_x}]\\)\n\\(E\\hat{\\beta_1}] = \\beta_1 + \\frac{\\sum\\_{i=1}^n E[u_i(x_i - \\bar{x})]}{SST_x}\\)\n\\(E\\hat{\\beta_1}] = \\beta_1 + \\frac{\\sum\\_{i=1}^n 0 (x_i - \\bar{x})}{SST_x}\\)\n\\(E\\hat{\\beta_1}] = \\beta_1\\)\nWe can also prove the same for \\(\\beta_0\\).\n\\(E\\hat{\\beta_0}] = \\beta_0 + E[(\\beta_1 - \\hat{\\beta_1}\\bar{x} + E\\bar{u}]\\)\n\\(E\\hat{\\beta_0}] = \\beta_0 + E[(\\beta_1 - \\hat{\\beta_1}\\bar{x} + 0\\)\n\\(E\\hat{\\beta_0}] = \\beta_0\\)\nIn the last equation, because \\(\\hat{\\beta_1} = \\beta_1\\) the second term drops out.\nVariances of OLS Estimators\nAn additional assumption we can make about the variance of the OLS estimators is that the error u has the same variances conditional on any value of the explanatory variable.\n\\(V(u|x) = \\sigma^2\\)\nBy adding this assumption, which to be clear will break down horribly if it is violated, we can prove the following theorem.\nTheorem 2: Using assumptions 1-4 and homoskedastic error assumption\n\\(V(\\hat{\\beta_1}) = \\frac{\\sigma^2}{SST_x}\\) and \\(V(\\hat{\\beta_0}) = \\frac{\\sigma^2\\frac{1}{n}\\sum\\_{i=1}^n x_i^2}{\\sum\\_{i=1}^n(x_i - \\bar{x})^2}\\)\nwhere these are conditioned on the sample values.\nProof for \\(V(\\hat{\\beta_1})\\)\n\\(V(\\hat{\\beta_1}) = \\frac{1^2}{SST_x^2}V(\\sum\\_{i=1}^n u_i(x_i - \\bar{x}))\\)\nSubstitute \\(d_i = (x_i - \\bar{x})\\)\n\\(V(\\hat{\\beta_1}) = \\frac{1^2}{SST_x^2}\\sum\\_{i=1}^n u_i d_i^2\\)\nSince \\(V(u_i) = \\sigma^2 : \\forall i\\) we can substitute that constant into the equation.\n\\(V(\\hat{\\beta_1}) = \\frac{1}{SST_x^2}\\sigma^2 \\sum\\_{i=1}^n d_i^2\\)\nObserve that the second RHS term is just \\(SST_x\\) after pulling out the constant, we can rewrite as\n\\(V(\\hat{\\beta_1}) = \\frac{\\sigma^2 SST_x}{SST_x^2}\\)\nwhich reduces to our stated result.\nNow that we know the way to estimate the variance, we can ask the following question. How does \\(V(\\hat{\\beta_1})\\) depend on error variance?\nThe larger the error variance, the larger \\(V(\\hat{\\beta_1})\\).\nThe larger the \\(V(x)\\), the smaller \\(V(\\hat{\\beta_1})\\)\nAs sample size increases, the total variation in x increases which leads to a decrease in \\(V(\\hat{\\beta_1})\\)\nEstimating the Error Variance\nErrors are never observed. Instead, we observe residuals that we can compute from our sample data. We can write the errors as a function of the residuals.\n\\(\\hat{u_i} = u_i - (\\hat{\\beta_0} - \\beta_0) - (\\hat{\\beta_1} - \\beta_1)x\\)\nOne problem that we run into is that using the residuals as an estimator is biased without correction because it does not take into account two restrictions for OLS residuals. OLS residuals have to sum to 0 and have a 0 covariance between x and u. Formally,\n\\(\\sum\\_{i=1}^n \\hat{u_i} = 0\\)\nand\n\\(\\sum\\_{i=1}^n \\hat{u_i}x_i = 0\\).\nThus we need to correct by n-2 degrees of freedom for an unbiased estimator. When we do so, we get the following.\n\\(\\hat{\\sigma}^2 = \\hat{s}^2 = \\frac{1}{n-2}\\sum\\_{i=1}^n\\hat{u_i}^2\\)\n\\(\\hat{\\sigma}^2 = \\frac{SSR}{n-2}\\)\nMultiple Linear Regression\nWe are now going to extend our previous discussion of Simple Linear Regression into the multiple variate case. Here we consider models that include more than one independent variable.\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x\\_2 + ... + \\beta_kx\\_k + u\\)\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) measures change in y with respect to \\(x_1\\) holding all other covariates fixed and so on.\nAs shorthand we refer to parameters other than the intercept as slope parameters.\nWe can generalize the zero conditional mean assumption for the errors to be\n\\(E(U|x\\_1, x\\_2, ..., x\\_k) = 0\\)\nMechanics and Interpretation of OLS\nOur goal is to get estimates \\(\\beta_0, \\beta_1,...,\\beta_k\\). We choose these to minimize the sum of squared residuals\n\\(\\sum_{i=1}^n\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x\\_{i1}+...+ \\hat{\\beta_k}x\\_{ik}\\)\nThis minimization leads to k + 1 linear equations in k + 1 unknowns. We call these the OLS first order equations.\nOLS Fitted Values and Residuals\nFor observation i the fitted values are:\n\\(\\sum_{i=1}^n\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x\\_{i1}+...+ \\hat{\\beta_k}x\\_{ik}\\)\nRecall that OLS minimizes the average square prediction error which says nothing about any given prediction. The residual is generalized from the simple linear regression case to be:\n\\(\\hat{u_i} = y_i - \\hat{y_i}\\)\nProperties of fitted values and residuals\nThe sample average of the residuals is 0. \\(\\bar{y} = \\hat{\\bar{y}}\\)\nSample covariance between each independent variable and the residuals is 0 by construction\nThe average point is always on the regression line by construction.\nGoodness of Fit\nLike before SST = SSE + SSR and \\(R^2 = 1 - \\frac{SSR}{SST}\\)\nExpected Value of OLS Estimators\nWe restate the assumptions needed for OLS regressions\nLinear in Parameters\nThe model can be written as \\(y = \\beta_0 + \\beta_1x\\_1 + \\.\\.\\. + \\beta_kx\\_k\\)\nRandom sampling\nWe have a random sample of n observations \\({(x\\_{i1}, x\\_{i2}, ..., x\\_{ik}): i = 1,2,..., n}\\)\nNo perfect collinearity\nIn the sample (and therefore population) none of the independent variables is constant and there are no exact linear relationships among the independent variables\nZero Conditional Mean of Error\n\\(E(u|x_1, x_2, ..., x_k) = 0\\)\nThis implies that none of the independent variables are correlated with the error term\nIf these four assumptions hold\nTheorem 3.1: OLS is unbiased\n\\(E(\\hat{\\beta_j} = \\beta_j : j = 1,2,\\.\\.\\.,k)\\)\nfor any values of the population parameter \\(\\beta_j\\)\nNote: Adding irrelevant independent variables does not effect unbiasedness. These terms will on average be 0 across many samples. Adding irrelevant independent variables will hurt the estimator’s variances.\nOmitted Variable Bias\nA major problem that will lead to bias in our estimates is omitting a relevant variable from our model. To see why suppose we have the following population model\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + u\\)\nbut we only run the regression\n\\(y = \\hat{\\beta_0} + \\beta_1x\\)\nFor example, we want to estimate the effect of education on wages but do not include some measure of innate ability.\nSince the model is misspecified, we can define bias as:\n\\(\\tilde{\\beta_1} = \\hat{\\beta_1} + \\hat{\\beta_2}\\tilde{\\delta_1}\\)\nwhere \\(\\hat{\\beta_1}, \\hat{\\beta_2}\\) are slope estimator from the regression $y_i $ on \\(x_1, x_2\\) and \\(\\tilde{\\delta_1}\\) is the slope from the regression \\(x\\_{i2}\\) on \\(x\\_{i1}\\).\n\\(\\tilde{\\delta_1}\\) depends only on the independent variables in the sample so we can treat it as a fixed quanitty when computing \\(E[\\tilde{\\beta_1}]\\)\n\\(E[\\tilde{\\beta_1}] = E[\\hat{\\beta_1} + \\hat{\\beta_2}\\tilde{\\delta_1}]\\)\n\\(E[\\tilde{\\beta_1}] = E[\\hat{\\beta_1} + \\tilde\\delta_1\\hat{\\beta_2}]\\)\n\\(E[\\tilde{\\beta_1}] = \\beta_1 + \\tilde\\delta_1\\beta_2\\)\nwhich implies that the omitted variable bias is \\(\\beta_2\\tilde{\\delta_1}\\)\nThe bias in the model is 0 if:\n\\(\\beta_2 = 0\\)\n\\(\\tilde{\\delta_1}=0\\)\nwhich occurs if and only if \\(x_1\\) and \\(x_2\\) are uncorrelated in the sample.\nVariance of OLS Estimators\nKeeping our previous Multiple Linear Regression assumptions we add\nAssumption 5: Homoskedasticity of errors\nThe error u has the same variance given any values of the explanatory variables.\n\\(V(u| x_1, x_2, ..., x_k)=\\sigma^2\\)\nTheorem 2\nUnder assumptions 1-5 conditional on the sample values of the independent variables\n\\(v(\\hat{\\beta_j}) = \\frac{\\sigma^2}{{SST}_j(1-R_j^2)}\\)\nand\n\\(E[\\hat{\\sigma%^2}= \\sigma^2]\\)\nThe unbiased estimator of \\(\\sigma^2\\) is \\(\\hat{\\sigma^2}= \\frac{1}{n-k-1}\\sum_{i=1}^n \\hat{u_i}^2\\) where n is the number of observations and k + 1 is the number of parameters.\nThe standard error of \\(\\hat{\\beta_j}\\) is\n\\(se(\\hat{\\beta_j})= \\frac{\\hat{\\sigma}}{\\sqrt{{SST}_j(1-\\hat{R_j}^2)}}\\)\nEfficiency Properties of OLS\nThe OLS estimator \\(\\hat{\\beta_j}\\) for \\(\\beta_j\\) is BLUE: The Best Linear Unbiased Estimator.\nestimator: rule that can be applied to data to generate an estimate\nunbiased \\(E[\\hat{\\beta_j}]=\\beta_j\\) for any estimator\nlinear: An estimator is linear if it can be expressed as a linear function of the data on the dependent variable\nbest: Gauss-Markov holds that for any estimator \\(\\tilde{\\beta_j}\\) that is linear and unbiased \\(V(\\hat{\\beta_j}) \\leq V(\\tilde{\\beta_j})\\). That is the OLS estimator has at least as small if not smaller variance than any other linear unbiased estimator.\nFrom our previous assumptions we add an additional assumption.\nAssumption 6: Normality\nThe population error u is independent of the explanatory variables \\(x_1, x_2, ..., x_k\\) and is normally distributed as \\(u \\sim N(0, \\sigma^2)\\)\nAssumption 6 is strong and amounts to also assuming MLR Assumption 4 and MLR Assumption 5. Taken together, the six assumptions we’ve made so far collectively are the classical linear model (CLM) assumptions.\nWe can summarise the CLM as:\n\\(y|\\textbf{x} \\sim N(\\beta_0 + \\beta_1x\\_{ik} + ... + \\beta_kx\\_{ik}, \\sigma^2)\\)\nwhere \\(\\textbf{x}\\) is shorthand for \\(x_1, x_2, ..., x_k\\)\nNote: Problems with Normal Error Assumption\nFactors in u can have very different distributions in the population. While central limit theorems can still hold, the normal approximation can be poor.\nCentral limit theorem arguments assume all error affects y in separate additive fashion which has no guarantee of truth. Any breakdown in this assumption will break this assumption\nIn any given application, normal error assumptions are an empirical matter. In general the assumption will be false if y takes on just a few possible values.\nTheorem 4.1: Normality of error terms lead to normality of sampling distributions of OLS estimators\nUnder the CLM assumptions (MLR Assumptions 1-6) conditional on the sample values of the independent variables\n\\(\\hat{\\beta_j} \\sim N(\\beta_j, V(\\beta_j))\\) which implies\n\\(\\frac{\\hat{\\beta_j} - \\beta_j}{V(\\hat{\\beta_j})} \\sim N(0,1)\\)\nTesting hypotheses about a single population parameter: The t-test\nTheorem 4.2: t distribution for standard errors\nUnder the CLM assumptions (MLR Assumptions 1-6)\n\\(\\frac{\\hat{\\beta_j} - \\beta_j}{se(\\hat{\\beta_j})} \\sim t\\_{n-k-1} = t\\_{df}\\)\nwhere k+1 is the number of paramters in the population model and n-k-1 is the degrees of freedom.\nTheorem 2 is important because it allows us to test hypotheses involving \\(\\beta_j\\). The test statistic we use is the t-statistic.\n\\(t_{\\hat{\\beta_j}} = \\frac{\\hat{\\beta_j}}{se(\\hat{\\beta_j})}\\)\nNull Hypothesis Tests\nThe most common hypothesis test is a two-sided alternative\n\\(H_0: \\beta_j = 0\\)\n\\(H_a: \\beta_j \\neq 0\\)\nWe can also test against one sided alternatives where we expect\n\\(H_0: \\beta_j \\leq 0\\)\n\\(H_a: \\beta_j > 0\\)\nor the reverse.\nA p-value is the probability of observing a test statistic, in this case a t-statistic, as extreme as the one we observed given that the null hypothesis is true.\nTesting Hypotheses about a single linear combination of Parameters\nOur t-statistic keeps the same general format but changes slightly.\n\\(t = \\frac{\\hat{\\beta_j} - \\hat{\\beta_k}}{se(\\hat{\\beta_j} - \\hat{\\beta_k})}\\)\nwhere\n\\(se(\\hat{\\beta_j} - \\hat{\\beta_k}) = \\sqrt{se(\\hat{\\beta_j})^2 + se(\\hat{\\beta_k})^2 + 2Cov(\\hat{\\beta_j},\\hat{\\beta_k})}\\)\nSome guidelines for discussing signficance of a variable in a MLR model\nCheck for statistical significance. If yes, discuss the magnitude of the coefficient to get an idea of its practical importance.\nIf variable isn’t significant, look at whether the variable has the expected effect and if it is practically large.\nConfidence Intervals\nA confidence interval is if random samples were obtained infinitely many times with \\(\\tilde{\\beta_j}, \\hat{\\beta_j}\\) computed each time then the (unknown) \\(\\beta_j\\) population value would lie in the interval \\((\\tilde{\\beta_j}, \\hat{\\beta_j})\\) for 95% of the samples.\nUnder the CLM assumptions, a confidence interval is\n\\(\\hat{\\beta_j} \\pm \\alpha SE(\\hat{\\beta_j})\\)\nwhere \\(\\alpha\\) is a critical value\nTesting Multiple Linear Restrictions: The F-test\nOften we want to test whether a group of variables have no effect on the dependenet variables.\nThe null hypothesis is that a set of variables has no effect on y, once another set of variables have been controlled.\nIn contest to hypothesis testing:\nThe restricted model is the model without hte groups of variables we are testing\nThe unrestricted model is the model of all the parameters\nFor the general case:\nThe unrestricted model with k independent variables \\(y = \\beta_0 + \\beta_1x_1 +...+ \\beta_k x_k + u\\)\nThe null hypothesis is \\(H_0: \\beta\\_{k-q-1} = 0,..., \\beta_k = 0\\) which puts q exclusion restrictions on the model\nThe F-statistic is:\n\\(F = \\frac{\\frac{SSR_r - SSR\\_{ur}}{q}}{\\frac{SSR\\_{ur}}{n-k-1}}\\)\nwhere \\(SSR_r\\) is the sum of squared residuals from the restricted model and \\(SSR\\_{ur}\\) is the sum of squared residuals from the unrestricted model and q is the numerator degrees of freedom which is the degrees of freedom in the restricted model minus the degrees of freedom in the unrestricted model.\nThe F-statistic is always non-negative. If \\(H_0\\) is rejected then we say that \\(x\\_{k-q+1},...x_k\\) are jointly statistically significant. If we fail to reject then the variables are jointly statistically insignificant.\nThe R-Squared Form of the F-statistic\nUsing the fact that \\(SSR_r = SST(1-R_r^2)\\) and \\(SSR\\_{ur} = SST(1 - R\\_{ur}^2)\\) we can substitute in to the F-statistic to get\n\\(F = \\frac{\\frac{R\\_{ur}^2 - R_r^2}{q}}{\\frac{1-R\\_{ur}^2}{n-k-1}}\\)\nThis is often more convenient for testing exclusion restrictions in models but cannot test all linear restrictions.\nOLS Asympotics\nWe know under certain assumptions that OLS estimators are unbiased, but unbiasedness cannot always be achieved for an estimator. Another property that we are interested in is whether an estimator is consistent.\nTheorem 5.1: OLS is a consistent estimator\nUnder MLR Assumptions 1-4, the OLS estimator \\(\\hat{\\beta_j}\\) is consistent for \\(\\beta_j \\forall \\\\ j \\in 1,2,...,k\\).\nInformally, as n tends to infinitythe distribution of \\(\\hat{\\beta_j}\\) collapses to the single point \\(\\beta_j\\)\nWe can add an assumption MLR 4’:\n\\(E(u) = 0,Cov(x_j, u) = 0 \\forall j \\in 1,2,...,k\\)\nThis is a weaker assumption than MLR 4. MLR 4’ requires only that \\(x_j\\) is uncorrelated with u and that u has zero mean in the population. Indeed MLR 4 implies MLR 4’.\nWe use MLR4 as an assumption because OLS is biased but consistent under MLR 4’ if \\(E[u| x_1, ..., x_k]\\) depends on any of the \\(x_j\\). Second, if MLR 4 holds, then we have properly modeled the population regression function.\nDeriving Inconsistency of OLS\nCorrelation between u and any of the \\(x_1, ..., x_k]\\) generally causes all of the OLS estimators to be inconsistent. If the error is correlated with any of the independent variables then OLS is biased and inconsistent.\nThere is an asymptotic analogue to Omitted Variable Bias. Suppose the model \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + v\\) satisfies MLR assumptions 1-4. If we omit \\(x_2\\) then:\n\\(plim\\tilde{\\beta_1} = \\beta_1 + \\beta_2\\delta_1\\)\n\\(plim\\tilde{\\beta_1} = \\beta_1 + \\beta_2\\frac{Cov(x_1,x_2)}{V(x_1)}\\)\nIf the covariance term is zero then the estimator is still consistent. Otherwise, the inconsistency takes on the same sign as the covariance term.\nAsymptotic Normality and Large Sample Inference\nIn cases where the \\(y_i\\) do not follow normal distributions we can still get asymptotic normality.\nTheorem 5.2: Asymptotic Normality\nUnder MLR Assumptions 1-5\n\\(\\sqrt{n}(\\hat{\\beta_j} - \\beta_j) \\xrightarrow{a} N(0, \\frac{\\sigma^2}{a_j^2}\\) where \\(a_j^2\\) is the asymptotic variance of \\(\\sqrt{n}(\\hat{\\beta_j} - \\beta_j)\\). For the slope coefficients \\(a\\_j^2 = plim(\\frac{1}{n} \\sum\\_{i=1}^n \\hat{r\\_{ij}^2})\\) where the \\(r\\_{ij}\\) are the residuals from regressing \\(x_j\\) on the other independent variables.\n\\(\\hat{\\sigma^2}\\) is a consistent estimator of \\(\\sigma^2\\)\nFor each j\n\\(\\frac{\\hat{\\beta_j} - \\beta_j}{sd(\\hat{\\beta_j})} \\xrightarrow{a} N(0,1)\\) which we cannot compute from data and\n\\(\\frac{\\hat{\\beta_j} - \\beta_j}{se(\\hat{\\beta_j})} \\xrightarrow{a} N(0,1)\\) which we can compute from data.\nThis theorem does not require MLR 6 from the list of required assupmtions. What this theorem says is that regardless of the population distribution of u, the OLS estimators when properly standardized have approximate standard normal distributions.\nFurther,\n\\(\\frac{\\hat{\\beta_j} - \\beta_j}{se(\\hat{\\beta_j})} \\xrightarrow{a} t\\_{df}\\)\nbecause \\(t\\_{df}\\) approaches $ N(0,1)$ as the degrees of freedom gets large so we can carry out t-tests and confidence intervals in the same way as the CLM assumptions.\nRecall that \\(\\widehat{V(\\hat{\\beta_j})} = \\frac{\\hat{\\sigma^2}}{SST_j(1-R_j^2)}\\) where \\(SST_j\\) is the total sum of squares of \\(x_j\\) in the sample and \\(R_j^2\\) is the R-squared from regressing \\(x_j\\) on all other independent variables. As the sample size increases:\n\\(\\hat{\\sigma^2} \\xrightarrow{d} \\sigma^2\\)\n\\(R_j^2 \\xrightarrow{d} c\\) which is some number between 0 and 1\nThe sample variance \\(\\frac{SST_j}{n} \\xrightarrow{d} V(x_j)\\)\nThese imply that \\(\\widehat{V(\\hat{\\beta_j})}\\) shrinks to 0 at the rate of \\(\\frac{1}{n}\\) and \\(se(\\hat{\\beta_j}) = \\frac{c_j}{\\sqrt{n}}\\) where \\(c_j = \\frac{\\sigma}{\\sigma\\sqrt{1 - \\rho_j^2}}\\).\nThis last equation is an approximation. A good rule of thumb is that standard eerrors can be expected to shrink at a rate that is the inverse of the square root of the sample size.\nAsymptotic Efficiency of OLS\nTheorem 5.3\nUnder Gauss-Markov assumptions, let \\(\\tilde{\\beta_j}\\) denote estimators that solve the equation\n\\(\\sum\\_{i=1}^n g_j(\\textbf{x}_i)(y_i - \\tilde{\\beta_0}-\\tilde{\\beta_1}x\\_{i1} - ... - \\tilde{\\beta_k}x\\_{ik}) = 0\\)\nLet \\(\\hat{\\beta_j}\\) denote the OLS estimators. The OLS estimators have the smallest asymptotic variance.\n\\(AVar(\\sqrt{n}(\\hat{\\beta_j} - \\beta_j)) \\leq AVar(\\sqrt{n}(\\tilde{\\beta_j} - \\beta_j))\\)\n\n\n\n",
      "last_modified": "2023-05-24T22:03:49-07:00"
    },
    {
      "path": "survey.html",
      "title": "Survey Design and Sampling",
      "description": "Notes on Survey Design and Sampling.\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nSampling\nSampling Types\nSimple Random Sampling\nUnderlying ideas for SRS\nDerivations for SRS\nExamples and Exercises\nRandom Sampling with Replacement\n\nConfidence Intervals\nExercises\n\nSample Size\nExercises\n\nEstimating Proportions, Ratios, and Subpopulation Means\nEstimating a population proportion\nSample size for estimating a proportion\nEstimating a Ratio\nEstimating a Mean, total, or Population of a subpopulation\nExercises\n\nUnequal Probability Sampling\nSampling with Replacement\nThe Horvitz-Thompsom Estimator\nHajek Estimator\nExamples\nExercises\n\nChapter 7\nChapter 8\nChapter 9\nChapter 10\nChapter 11\nCluster and Systemic Sampling\nPrimary units selected via SRS\nRatio Estimators\nThe Basic principle\n\nMultistage Designs\nSRS without replacement at each stage\nRatio Estimator\nAny Multistage Design with replacement\n\nDouble Sampling\nRatio Estimation\nAllocation for ratio estimation\nDouble Sampling for stratification\nNonresponse and double sampling\nExercises\n\nNetwork Sampling and Link-Tracing Designs\nEstimation of the Population Total or Mean\nHorvitz-Thompson Estimator\n\nDetectability and Sampling\n\nSampling\nSampling: Selecting some part of a population to observe so that we\ncan estimate a population parameter\nThe basic setup of sampling:\nPopulation is known and of finite size N units \\(i \\in \\{1,...,N\\}\\)\nEach unit has an associated value of interest (y-value) which is\nfixed and unknown\nSampling Design: The procedure through which the sample of units is\nselected from the population\nSampling Types\nSimple Random Sampling\nSimple Random Sampling: A sampling design in which n distinct\nunits are selected from N units in the population in such a way that\nevery possible combination of n units is equally likely to be in the\nselected sample.\nSRS is also known as random sampling without replacement.\nThe inclusion probability for SRS is the same for all units. The\nprobability that the \\(i^{th}\\) unit of the population is included is\n\\(\\pi_i = \\frac{n}{N}\\). A SRS guarantees that each possible sample of\nn units for all units.\nWe estimate the population mean with SRS as follows:\n\\(\\bar{y}\\) is unbiased for the population mean \\(\\mu\\). We define\n\\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i\\)\n\\(s^2\\) is unbiased for the population variance \\(\\sigma^2\\). We define\n\\(s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(y_i - \\bar{y})^2\\)\nThe variance of \\(\\bar{y}\\) with SRS is\n\\(V[\\widehat{\\bar{y}}] = \\frac{N-n}{N}\\frac{s^2}{n}\\). The standard\nerror is defined by taking the square root of this quantity.\nTo estimate the population total an unbiased estimator\n\\(\\tau = N\\mu\\). An unbiased estimator of \\(V[\\tau]\\) is\n\\(N(N-n)\\frac{s^2}{n}\\)\nUnderlying ideas for SRS\n\\(\\bar{y}\\) is a random variable whose expectation is the population\nparameter where the expectation is taken over all possible samples. This\nmakes \\(\\bar{y}\\) design-unbiased and this does not depend on any\nassumptions about the population. The variance estimates are design\nunbiased as well.\nDerivations for SRS\n\\[\\begin{aligned}\nE[\\bar{y}] &= \\sum\\bar{y_s}P(s) \\\\\nE[\\bar{y}] &= \\frac{1}{n} \\sum y_i\\frac{\\binom{N-1}{n-1}}{\\binom{N}{n}} \\\\\nE[\\bar{y}] &= \\frac{1}{N}\\sum y_i\n\\end{aligned}\\]\nAn alternative derivation\n\\[\\begin{aligned}\nE[\\bar{y}] &= \\frac{1}{n}\\sum_{i=1}^{N}y_iE[z_i] \\\\\nE[\\bar{y}] &= \\frac{1}{n}\\sum_{i = 1}^{N}y_i\\frac{n}{N}\\\\\nE[\\bar{y}] &= \\frac{1}{n}\\frac{n}{N}\\sum_{i=1}^{N}y_i \\\\\nE[\\bar{y}] &= \\frac{1}{N}\\sum_{i=1}^{N} y_i\n\\end{aligned}\\] To derive the variance of the sample mean, see pages\n21-22.\nExamples and Exercises\nExample 1 p. 16\n\n\n\n\n\n\nExample 2 p.18\n\n\n\nExample 3 p. 20\n\n\n\nExample 4 p. 32\n\n\n\nExercise 2 p.36\n\n\n\nExercise 3 p. 36\n\n\n\nExercise 4 p.36\nShow that \\(E[s^2] = \\sigma^2\\)\n\\[\\begin{aligned}\nE[s^2] &= E[\\frac{n}{n-1}(\\bar{X^2}-\\bar{X}^2)] \\\\\nE[s^2] &= \\frac{n}{n-1}E[\\bar{X^2}-\\bar{X}^2)]\\\\\nE[s^2] &= \\frac{n}{n-1}\\left(\\frac{n-1}{n}\\sigma^2 \\right) \\\\\nE[s^2] &= \\sigma^2\n\\end{aligned}\\] Exercise 6 p.37\n\n\n\nRandom Sampling with Replacement\nRandom Sampling with Replacement: A sample of n units selected\nfrom a population of size N, returning each unit to the population\nafter it has been drawn.\nEvery possible sequence of n units has equal probability under the\ndesign.\nThe plug in estimator \\(\\bar{y_n} = \\frac{1}{n}\\sum_{i=1}^{n} y_i\\).\nThe variance of the plug-in sample mean estimator is\n\\(V[\\hat{\\bar{y_n}}] = \\frac{s^2}{n}\\) which is unbiased for the\nparameter \\(V[\\bar{y_n}] = \\frac{N-1}{nN}\\sigma^2\\).\nThe variance of the SRS sample mean is in general lower than the\nvariance of the sample mean taken via random sampling with replacement.\nIn random sampling with replacement \\(\\bar{y_n}\\) depends on the number of\ntimes each unit is selected. This is why the notation is a bit\ndifferent. Two surveys that observe the same distinct set of units but\nwith different repeats in the sample will generally yield different\nestimates.\nEffective sample size: In random sampling with replacement, this is\nthe number of distinct units in the sample (denoted v).\n\\(\\bar{y_v} = \\frac{1}{v}\\sum_{i=1}^{n}y_i\\). This quantity is an\nunbiased estimator of the population mean.\nConfidence Intervals\nLet I represent a confidence interval for the population mean \\(\\mu\\).\nChoose a small number \\(\\alpha\\) as the allowable probability of error\nsuch that the procedure has a probability \\(P(\\mu \\in I) = 1-\\alpha\\).\nI is a random variable and the endpoints of I will vary from sample\nto sample. Conversely, \\(\\mu\\) is fixed and unknown parameter.\nUnder SRS a \\(1-\\alpha\\) CI means that for \\(1-\\alpha\\) of the possible\nsamples, the interval contains the true value of the population mean.\nAn approximate \\(100(1-\\alpha)\\%\\) CI for \\(\\mu\\) is\n\\[\\bar{y} \\pm t\\sqrt{\\frac{N-n}{N}\\frac{s^2}{n}}\\] where t is the upper\n\\(\\frac{\\alpha}{2}\\) point of a t-distribution with n-1 degrees of\nfreedom.\nWe rely here for this on the finite population central limit theorem\nwhich shows that the distribution will converge to a standard normal\ndistribution as n and N grow large.\nExercises\nExercise 3 p.51\n\n\n\nSample Size\nThe first question when planning a survey is what sample size should be\nused. We estimate a statistic \\(\\hat{\\theta}\\) that is an unbiased\nestimator of \\(\\theta\\) which is the population parameter of interest.\nWe want to choose a maximum allowable difference d between the true\nparameter and the estimate and allow for some small probability \\(\\alpha\\)\nthat the error could exceed our chosen specification.\nThis means that we choose our sample size n such that\n\\(P(|\\hat{\\theta} - \\theta) > d < \\alpha\\)\nSample size for estimating a population mean\nUnder SRS\n\\[V[\\bar{y}] = (N - n)\\frac{\\sigma^2}{Nn}\\]\nThus \\(d = z \\sqrt{V[\\bar{y}]}\\) assuming that that our estimator is\nnormally distributed, which is the case for a population mean.\nSolving for n\n\\[\\begin{aligned}\nn &= \\frac{1}{\\frac{d^2\\sigma^2}{t^2} + \\frac{1}{N}} \\\\\nn &= \\frac{1}{\\frac{1}{n_0} + \\frac{1}{N}}\n\\end{aligned}\\]\nwith an appropriate substitution. The only difference to get a\npopulation total is to multiply \\(n_0\\) by \\(N^2\\) in the above formula.\nBoth of these formulas presume knowledge of the population variance,\nwhich is generally unknown. To get an estimate, we tend to either guess\ncompletely or use the results of a previous survey.\nIf we ignore the finite population correct, then we only have to\ncalculate \\(n_0\\). This sample size will always be larger than the sample\nsize with a finite population correction.\nSample Size for Relative Precision\nThe formula is now\n\\[n = \\frac{1}{\\frac{r^2\\mu^2}{t^2\\sigma^2} + \\frac{1}{N}}\\] where r\nrepresents the relative error of interest. In this formula the\npopulation quantity on which sample size depends when the desire is to\ncontrol relative precision is the coefficient of variation in the\npopulation.\nExercises\nExercise 1 p.56\n\n\n\nEstimating Proportions, Ratios, and Subpopulation Means\nWe can use the the same formulas as before, but some special features of\nnote: - Formulas simplify considerably with attribute data - Exact\nConfidence Intervals are possible - A sample size sufficient for a\ndesired absolute precision may be chosen without any information on\npopulation parameters\nEstimating a population proportion\n\\[p = \\frac{1}{N}\\sum_{i=1}^{N}y_i = \\mu\\]\nFinite population variance\n\\[s^2 = \\frac{n}{n-1}\\hat{p}(1-\\hat{p})\\]\nVariance of the estimator\n\\[V[\\hat{p}] = \\frac{N-n}{N-1}\\frac{\\hat{p}(1-\\hat{p})}{n}\\] The first\nterm is the FPC (I think) so we can ignore it to get an estimator that\nis slightly too large.\nConfidence intervals work as expected\n\\[\\hat{p} \\pm \\sqrt{V[\\hat{p}]}\\]\nSample size for estimating a proportion\nSample size requirements when we ignore the finite population correction\n\\[n_0 = \\frac{z^2p(1-p)}{d^2}\\]\nWhen including the FPC\n\\[n = \\frac{1}{\\frac{1}{n_0} + \\frac{1}{N}}\\] where \\(n_0\\) is defined as\nabove.\nLike before these formulas depend on an estimate of \\(p\\). If \\(p\\) is\nunknown use \\(p = 0.5\\) because this is the value for which the function\nabove takes on its maximum value as a function of p. To see this, simply\nlook at the part of the function that includes \\(p\\).\n\\[\\begin{aligned}\n\\frac{d}{dp} &= [p(1-p)]' \\\\\n0 &= 1-2p \\\\\np &= \\frac{1}{2}\n\\end{aligned}\\]\nEstimating a Ratio\nThe population ratio is estimated by dividing the total y-values by the\ntotal x-values in the samples\n\\[r = \\frac{\\sum_{i=1}^{n}y_i}{\\sum_{i=1}^{n}x_i} = \\frac{\\bar{y}}{\\bar{x}}\\]\nRatio estimators are not design unbiased. More on this in Chapter\n7 and Chapter 12\nEstimating a Mean, total, or Population of a subpopulation\nAn estimator for sub-population with characteristic \\(a_i\\) in a sample of\n\\(n\\) with attribute \\(n_1\\) is:\n\\[\\hat{p_i} = \\frac{a_i}{n_i}\\]\nThis proportion has two random variables: \\(a_i\\) and \\(n_i\\). To estimate\nthe sub-population mean, we have N units in a population. Let \\(N_k\\) be\nthe number belonging to the \\(k^{th}\\) sub-population. The variable of\ninterest \\(y_{ik}\\).\nThe total is:\n\\[\\tau_k = \\sum_{i=1}^{N}y_{ki}\\] The estimator of the mean is design\nunbiased\n\\[\\bar{y}_k =\\frac{1}{n_k} \\sum_{i=1}^{n_k}y_{ki}\\]\nProof sketch.\nCondition on the domain sample size \\(n_k\\). Given \\(n_k\\) every\npossible combination of \\(n_k\\) of the \\(N_k\\) sub-population units has\nequal probability of being selected via SRS.\nGiven (1), \\(\\bar{y}_k\\) behaves as the sample mean of a SRS of \\(n_k\\)\nfrom \\(N_k\\): \\(E[\\bar{y}_k|n_k] = \\mu_k\\)\n\\(E[E[\\bar{y}_k|n_k]] = E[\\bar{y}_k] = \\mu_k\\)\nThe variance of the subpopulation mean\n\\[\\begin{aligned}\nV[\\bar{y}_k] &= E[V[\\bar{y}_k|n_k]] + V[E[\\bar{y}|n_k]] \\\\\nV[\\bar{y}_k] &= E[V[\\bar{y}_k|n_k]] + V[\\mu_k] \\\\\nV[\\bar{y}_k] &= E[V[\\bar{y}_k|n_k]]\n\\end{aligned}\\]\nGiven SRS\n\\[V[\\bar{y}_k | n_k] = \\sigma^2_k\\left(\\frac{1}{n_k} - \\frac{1}{N_k}\\right)\\]\nwhere \\(\\sigma^2_k\\) is the population variance for units in the \\(k^{th}\\)\npopulation.\n\\(\\sigma^2_k = \\frac{1}{N_k -1}\\sum_{i=1}^{N_k}(y_{ki} - \\mu_k)^2\\)\nThe unconditional variance for the estimator is thus:\n\\[V[\\bar{y}_k] = \\frac{N_k - n_k}{N_k n_k}s^2_k\\]\nwhere \\(s^2_k\\) is the sample variance of the sub-population of interest.\nConfidence intervals are found in the usual way. If the sub-population\nis unknown, we replace the FPC with its expected value \\(\\frac{N-n}{N}\\)\nExercises\nExercise 1 p. 66\n\n\n\nExercise 3 p. 66\n\n\n\nUnequal Probability Sampling\nIn some sampling procedures different units have different probabilities\nof being included in the sample. The unequal probabilities must be taken\ninto account for unbiased estimation of effects.\nSampling with Replacement\nThese estimators are Hansen-Hurwitz Estimators\nThe Horvitz-Thompsom Estimator\nWith any design with or without replacement given probability \\(\\pi_i\\)\nthat unit i is included in the sample for \\(i \\in \\{1,...,n\\}\\) the\nHorvitz-Thompson Estimator is:\n\\[\\hat{\\tau}_{HT} = \\sum_{i=1}^v \\frac{y_i}{\\pi_i}\\]\nwhere v is the effective sample size (AKA the number of distinct units\nin the sample).\nAn unbiased estimator of the variance is:\n\\[\\hat{V}[\\hat{\\tau}_{HT}] = \\sum_{i=1}^v\\left( \\frac{1}{\\pi_i^2} - \\frac{1}{\\pi_i}\\right)y_i^2 + 2\\sum_{i=1}^v\\sum_{j>1}\\left(\\frac{1}{\\pi_i\\pi_j} - \\frac{1}{\\pi_{ij}} \\right)y_iy_j\\]\nwhere \\(\\pi_j\\) is the probability that both units i and j are included in\nthe sample. All the joint inclusion probabilities \\(\\pi_{ij} > 0\\) are\nrequired for this to hold.\nAn alternative conservative estimator of variance that is guaranteed to\nbe non-negative: - For the \\(i^{th}\\) of the v distinct units in the\nsample compute \\(t_i = \\frac{vy_i}{\\pi_i}\\) - The expectation of \\(t_i\\) is\nthe HT estimator - The sample variance of \\(t_i\\) is\n\\[\\hat{V}[t_i] = \\frac{1}{v-1} \\sum_{i=1}^v(t_i - \\hat{t}_\\pi)^2\\]\nThe alternative variance estimator for the population is then:\n\\[\\hat{V}[\\hat{\\tau}_{HT}] = \\sum_{i=1}^v\\sum_{j<i}\\left(\\frac{\\pi_i\\pi_j}{\\pi_{ij}} \\right)\\left( \\frac{y_i}{\\pi} - \\frac{y_j}{\\pi_j}\\right)^2\\]\nprovided that for all \\(i,j\\) \\(\\pi_{ij}> 0\\).\nNOTE: While unbiased the HT estimator can have a large variance in\npopulations in which variables of interest and inclusion probabilities\nare not well related.\nHajek Estimator\nA generalized unequal probability estimator (Hajek) of the population\nmean is:\n\\[\\mu_g = \\frac{\\sum_{i \\in S}\\frac{y_i}{\\pi_i}}{\\sum_{i \\in S}\\frac{1}{\\pi_i}}\\]\nThe numerator is the HT estimator. The denominator is an unbiased\nestimate of the population size. Since this is a ratio estimator it is\nnot precisely unbiased, but the bias is small in large samples.\nA variance estimator for the this is:\n\\[\\hat{V}[\\hat{\\mu}_g] = \\frac{1}{N^2}\\left[\\sum_{i=1}^v(\\frac{1-\\pi_i}{\\pi_i^2})(y_i - \\hat{\\mu}_g)^2 + \\sum_{i=1}^v\\sum_{i \\neq 1} \\left(\\frac{\\pi_{ij}- \\pi_i\\pi_j}{\\pi_i\\pi_j}\\right)\\frac{(y_i - \\hat{\\mu}_g)(y_j - \\hat{\\mu}_g)}{\\pi_{ij}} \\right]\\]\nprovided that \\(\\pi_{ij} > 0\\). We can replace N with its estimator\n\\(\\sum_{i \\in S}\\frac{1}{\\pi_i}\\)\nExamples\nExercises\nChapter 7\nChapter 8\nChapter 9\nChapter 10\nChapter 11\nCluster and Systemic Sampling\nBoth concepts share the structure populations are partitioned into\nprimary units which are composed of secondary units.\nSystematic sampling: a single primary unit consists of secondary units\nspaced in some systematic fashion throughout the population.\nCluster sampling: Primary units consists of a cluster of secondary\nunits usually in close proximity with each other.\nPrimary units selected via SRS\nAn unbiased estimator the population total\n\\[\\tau = \\frac{N}{n}\\sum_{i=1}^n y_i = N\\bar{y}\\] where \\(\\bar{y}\\) is the\nsampling mean of the primary unit total.\nThe variance estimator\n\\[\\hat{V}[\\hat{\\tau}] = \\frac{N(N-n)}{n}\\hat{s}^2_u\\] where\n\\(\\hat{s}^2_u = \\frac{1}{n-1}\\sum_{i=1}^n(y_i - \\bar{y})^2\\)\nRatio Estimators\nIf \\(y_i\\) is highly correlated with primary unit size \\(M_i\\) a ratio\nestimator based on size may be efficient.\n\\[\\hat{\\tau}_r = rM\\]\nwhere \\(r = \\frac{\\sum_{i=1}^ny_i}{\\sum_{i=1}^n M_i}\\)\nBecause this is a ratio is is not an unbiased estimator but bias is\nsmall in large samples.\n\\[\\hat{V}[\\tau_r] = \\frac{N(N-n)}{n(n-1)}\\sum_{i=1}^n(y_i - rM_i)^2\\]\nThe Basic principle\nWithin primary unit variance does not enter into the variance of the\nestimators. Thus the basic systematic and cluster sampling principle is\nto obtain estimators of low variance or MSE. This implies that\npopulation should be partitioned into clusters such that clusters are\nsimilar to each other.\nMultistage Designs\nLet N denote the number of primary units in the population\n\\(M_i\\) is the number of secondary units of the \\(i^{th}\\) primary unit.\n\\(y_{ij}\\) is the value of the variable of interest for \\(j^{th}\\)\nsecondaryin the \\(i^{th}\\) primary unit.\n\\(\\mu_i\\) is the mean per secondary unit in the \\(i^{th}\\) primary unit.\n\\[\\tau = \\sum_{i=1}^n\\sum_{j=1}^{m_i}y_{ij}\\]\nSRS without replacement at each stage\nIn the first stage we take an SRS of n primary units. In the second\nstage from the \\(i^{th}\\) selected primary unit a SRS without replacement\nof \\(m_i\\) secondary units is selected.\nThe unbiased estimator:\n\\[\\hat{y_i} = \\frac{M_i}{m_i}\\sum_{j=1}^{m_i}y_{ij}\\] where\n\\(\\bar{y}_i = \\frac{1}{m_i}\\sum_{j=1}^{m_i}y_{ij}\\).\nSince SRS is used at the first stage, the overall estimate of the\npopulation parameter.\n\\[\\hat{\\tau} = \\frac{N}{n}\\sum_{i=1}^n\\hat{y}_i\\] with variance\n\\(V[\\hat{\\tau}] = N(N-n)\\frac{\\sigma^2_u}{n} + \\frac{N}{n}\\sum_{i=1}^N M_i(M_i - m_i)\\frac{\\sigma^2_i}{m_i}\\)\nwhere \\(\\sigma^2_u = \\frac{1}{N-1}\\sum_{i=1}^N(y_i - \\mu_1)^2\\) and\n\\(\\sigma^2_i = \\frac{1}{M_i -1}\\sum_{j=1}^{M_i}(y_{ij} - \\mu_i)^2\\)\nRatio Estimator\nA ratio estimator of the population total based on the sizes of the\nprimary unit is:\n\\[\\hat{\\tau}_r = \\hat{r}M\\]\nwhere \\(\\hat{r} = \\frac{\\sum_{i=1}^n\\hat{y}_i}{\\sum_{i=1}^nM_i}\\)\nThe variance is:\n\\[\\hat{V}[\\\\hat{\\tau}_r] = \\frac{N(N-n)}{n(n-1)}\\sum_{i=1}^n(\\hat{y}_i - M_i\\hat{r})^2 + \\frac{N}{n}\\sum_{i=1}^nM_i(M_i - m_i)\\frac{s^2_i}{m_i}\\]\nAny Multistage Design with replacement\nThe estimator for the population\n\\[\\hat{\\tau}_p = \\frac{1}{n}\\sum_{i=1}^n\\frac{y_i}{p_i}\\]\nwith variance\n\\(\\hat{V}[\\hat{\\tau}_p] = \\frac{1}{n(n-1)}\\sum_{i=1}^n(\\frac{\\hat{y}_i}{p_i}- \\hat{\\tau})^2\\)\nDouble Sampling\nDouble Sampling: Designs in which initially a sample of units is\nselected for obtaining auxiliary information only and then a second\nsample is selected for which the variable of interest is observed in\naddition to auxiliary information. - The second sample is often selected\nas a subsample of the first - Purpose is to use relationship between\nauxiliary variables and variable of interst to obtain better estimators.\nRatio Estimation\nLet \\(n'\\) be the number of units in the first sample and \\(n\\) be the\nnumber of units in the second sample. The ratio estimator\n\\[r = \\frac{\\sum_{i=1}^ny_i}{\\sum_{i=1}^nx_i}\\] is taken from the small\nsample containing both y’s and x’s. From the full sample for which the x\nvalues are obtained.\n\\[\\tau_x = \\frac{N}{n'}\\sum_{i=1}^{n'}x_i\\]\nThen the ratio estimator is \\(\\hat{\\tau}_r = r\\hat{\\tau}_x\\)\nThe variance of the estimator is estimated by\n\\[\\hat{V}[\\hat{\\tau}_r] = N(N-n)\\frac{s^2}{n'} + N^2\\left(\\frac{n'-n}{n'n(n-1)}\\right)\\sum_{i=1}^n(y_i - rx_i)^2\\]\nAllocation for ratio estimation\nRatio estimation is effective when the y variable is linearly related to\nan auxilliary variable x with y tending to be 0 when x is zero and it\nis cheaper or easier to measure x than y.\nSuppose the cost observing an x variable per unit is \\(c'\\) and the cost\nfor measuring y is \\(c\\). Then an equation for the total cost is:\n\\[C = c'n' + cn\\]\nFor a fixed cost C the lowest variance of \\(\\hat{\\tau}_r\\) is observed\nwith the following subsampling fraction.\n\\[\\frac{n}{n'} = \\sqrt{\\frac{c'}{c}\\frac{\\sigma^2_r}{\\sigma^2-\\sigma^2_r}}\\]\nDouble Sampling for stratification\nSuppose units can only be assigned to strata after the sample is\nselected. In Chapter 11, the methods therein were useful\nonly if the relative proportion \\(W_h = \\frac{N_h}{N}\\) of population\nunits in stratum h is known for each strata. When this is not the case,\ndouble sampling can be used with an initial (large) sample to classify\nunits into strata and then stratify the sample selected.\nThe estimators for this are:\nPop mean: \\(\\bar{y}_d = \\sum_{h=1}^L w_h\\bar{y}_h\\)\nVariance:\n\\(\\frac{N-1}{N}\\sum_{h=1}^L(\\frac{n'_h-1}{n\"-1}-\\frac{n_h-1}{N-1})\\frac{w_hs^2_h}{n_h} + \\frac{N-n'}{N(n'-1)}\\sum_{h=1}^L w_h(\\bar{y}_h - \\bar{y}_d)^2\\)\nNonresponse and double sampling\nIn situations of non-response survey often use callbacks to adjust for\nthis problem. In these cases nonresponding units may be considered a\nseparate stratum. A subsample can then be taken. Divide the population\ninto two strata: responders and nonresponders. Suppose \\(n'\\) is the\ninitial simple sample, \\(n'_1\\) is the number of responders and \\(n'_2\\) is\nthe number of nonresponders.\nIn callback stages responses are obtained for a SRS of \\(n_2\\) of the\ninitial respondents. An unbiased estimator of the population mean is:\n\\[\\bar{y}_d = \\frac{n'_1}{n'}\\bar{y}_1 + \\frac{n'_2}{n'}\\bar{y}_2\\]\nwith variance:\n\\[\\hat{V}[\\bar{y}_d] = \\frac{N-1}{N}\\sum_{h=1}^L(\\frac{n'_h-1}{n\"-1}-\\frac{n_h-1}{N-1})\\frac{w_hs^2_h}{n_h} + \\frac{N-n'}{N(n'-1)}\\sum_{h=1}^L w_h(\\bar{y}_h - \\bar{y}_d)^2\\]\nExercises\nExercise 1 (p. 198)\nIn an aerial survey of four plots selected by simple random sampling,\nthe numbers of ducks detected were 44, 55, 4, and 16. Careful\nexamination of photoimagery of the first and third of these plots\n(selected as a simple random subsample) revealed the actual presence of\n56 and 6 ducks, respectively. The study area consists of 10 plots.\nEstimate the total number of ducks in the study area by using a\nratio estimator. Estimate the variance of the estimator.\n\n\n\nSuppose that the photo analysis doubles the cost of observing a\nplot. Estimate the optimal subsampling fraction.\n\n\n\nNetwork Sampling and Link-Tracing Designs\nNetwork Sampling: Also referred to as multiplicty sampling. A SRS or\nstratified random sample of units is selected and all observation units\nlinked to any of the units selected are included or observed. - The\nmultiplicity of a unit is the number of selection units to which a\nperson is linked.\nSince there are unequal selection probabilities the sample mean is no\nlonger an unbiased estimator of the population mean in such a design.\nTherefore, different estimators must be used.\nEstimation of the Population Total or Mean\n\\(y_i\\) is the variable of interest for the \\(i^{th}\\) unit in a population.\nN is the number of observation units in the population.\n\\(\\tau\\) is the population total defined \\(\\tau = \\sum_{i=1}^Ny_i\\). \\(m_i\\)\nis the multiplicity of unit i. The number of selection units in the\npopulation is M. The population mean per selection unit is\n\\(\\mu = \\frac{\\tau}{M}\\). Consider a SRS of \\(n_0\\) selection units with\nevery observational unit linked to any selection unit included in the\nsample.\nThe draw by draw selection probability \\(p_i\\) is the probability that any\none of the \\(m_i\\) selection units to which a unit is linked is selected.\n\\[p_i = \\frac{m_i}{M}\\]\nAn unbiased estimator of \\(\\tau\\) can be formed by dividing each observed\ny-value by the associated selection probability\n\\[\\hat{\\tau}_m = \\frac{M}{n}\\sum_{i \\in s}\\frac{y_i}{m_i}\\] where s is\nthe sequence of observational units in the sample and includes repeat\nselections. An alternative way to write this is to define for the\n\\(j^{th}\\) selection unit in the population with \\(A_j\\) units linked the\nvariable \\(w_j\\) which is the sum of \\(\\frac{y_i}{m_i}\\) for all\nobservational units linked with selection j. In this case the\nestimator for the population is:\n\\[\\hat{\\tau}_m = \\frac{M}{m}\\sum_{j=1}^nw_j\\]\nAn unbiased estimator for the variance of the population is:\n\\[\\hat{V}[\\hat{\\tau}_m] = \\frac{M(M-n)}{n}\\frac{1}{n-1}\\sum_{j=1}^n(y_j - \\bar{w})^2\\]\nHorvitz-Thompson Estimator\nThe probability that the \\(i^{th}\\) unit is included in the sample is the probability that one or more of the \\(m_i\\) selection units to which it is linked is selected. We can simplify the problem by changing notation to\nbe in terms of networks instead of individual observational units\nbecause the inclusion probabilities are identical for all units within a\nnetwork.\nDefine K as the number of networks and \\(y_k^*\\) to be the total of the y-values overall all observational units in the \\(k^{th}\\) network while \\(m_k*\\) is the common multiplicity.\nThe inclusion probability is:\n\\[\\pi_k = 1 - \\frac{M-m_k^*\\choose{n}}{M \\choose n}\\]\nwhich in words is 1 minus the probability that the entire SRS of n selection units is selected from the selection units not linked with network k.\nThe HT estimator of the population total is:\n\\[\\hat{\\tau}_\\pi = \\sum_{k=1}^K \\frac{y^*_k}{\\pi_k}\\] This estimator does not depend on the number of times any unit is selected, which is in contrast to the multiplicity estimator above.\nAn unbiased estimator of the HT variance is\n\\[\\hat{V}[\\hat{\\tau}_\\pi] = \\sum_{k=1}^k\\left(\\frac{1}{\\pi^2_k}-\\frac{1}{\\pi_k} \\right) {y^*_k}^2 + \\sum_{k=1}^K\\sum_{l \\neq k}\\left(\\frac{1}{\\pi_k\\pi_l - \\frac{1}{\\pi_{kl}}} \\right)y^*_ky^*_l\\]\n\n\n\nDetectability and Sampling\nIn the basic sampling framework, we assume that our variable of interest is recorded without error for each unit in the sample. In actuality this is not the case. Some individuals may not be observed (detected).\nDetectability: is the probabily that an object in a selected unit is observed.\n\n\n\n",
      "last_modified": "2023-05-24T22:03:50-07:00"
    }
  ],
  "collections": []
}
