{
  "articles": [
    {
      "path": "about.html",
      "title": "Alex Stephenson",
      "author": [],
      "contents": "\n\n          \n          \n          Methods Tutoring Notes\n          \n          \n          Home\n          Creator\n          Lecture Notes\n          Coding Notes\n          Linear Algebra\n          ☰\n          \n          \n      \n        \n          \n            Alex Stephenson\n          \n          \n            \n              I am a PhD Candidate at the University of California,\n              Berkeley.\n            \n            \n              I am a PhD Candidate at the University of California,\n              Berkeley.\n            \n          \n\n          \n            \n              \n                  \n                    \n                      Github\n                    \n                  \n                \n                                \n                  \n                    \n                      Twitter\n                    \n                  \n                \n                                \n                  \n                    \n                      My Website\n                    \n                  \n                \n                              \n          \n\n          \n            \n              \n                                \n                  \n                    Github\n                  \n                \n                                \n                  \n                    Twitter\n                  \n                \n                                \n                  \n                    My Website\n                  \n                \n                              \n            \n          \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2023-05-06T13:13:24-07:00"
    },
    {
      "path": "coding.html",
      "title": "Coding Notes",
      "description": "Coding Notes and Resources\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nBasic R Operations\nGraphing in R\nSimulations in R\n\n\n\nlibrary(broom)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(fixest)\nlibrary(ggplot2)\nlibrary(marginaleffects)\n\n\nBasic R Operations\nThere are a voluminous amount of resources on R online. Here is a very minimal crash course.\nThe most important underlying concept in R is that (almost) everything is an object.\nData Types\nR has six basic data types. The four most common types we work with are vectors, lists, matrices, and data frames.\nVectors\n\n\n## Three ways to create a vector\nv = c(1,2,3,4,5)\nv2 = seq(from=1,to=5, by = 1)\n\n## Since the sequence is counting with no breaks we can also do this\nv3 = 1:5\n\n\nVectors can also be characters (strings in other languages)\n\n\nw = c(\"A\", \"B\", \"C\")\nw2 = c(\"1\",\"2\",\"3\")\n\n## We can convert to other types by the as.* series \nw3 = as.character(v)\nw4 = as.numeric(w2)\n\n\nLists\n\n\nl = list(v, w)\nl\n\n[[1]]\n[1] 1 2 3 4 5\n\n[[2]]\n[1] \"A\" \"B\" \"C\"\n\nMatrices\n\n\nm = matrix(seq(1,16,1), nrow = 4, byrow = T)\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\nFor much more about linear algebra in R, consult the Matrix Algebra in R section.\nData Frames\n\n\ndf = data.frame(\n  var1 = letters[1:5],\n  var2 = c(1,2,3,4,5)\n)\ndf\n\n  var1 var2\n1    a    1\n2    b    2\n3    c    3\n4    d    4\n5    e    5\n\nSubsetting\nControl Operations\nFunctions\nGraphing in R\nThe Basics\nR has default plotting, but for the purpose of these notes I exclusively use ggplot2. A main distinction between base plotting and ggplot2 is that ggplot2 requires a data frame and presumes that your data is tidy.\n\n\n## Using the built-in dataset diamonds\ndf = diamonds\n\n## Data\ndf |>\n  ## Mapping\n  ggplot(aes(x = carat, y = price)) +\n  ## geom\n  geom_point() +\n  ## we can have multiple geoms \n  geom_smooth(method = \"lm\", se = F)+\n  ## Labels\n  labs(x = \"Carat\",\n       y = \"Price\",\n       title = \"Example Chart\")+\n  ## theme \n  theme_minimal()\n\n\n\nFor more extensive discussion of graphing capabilities in R, check out Data Visualization: A Practical Introduction by Healy.\nUseful Plots for Political Science\nCoefficient Plot\nFirst let’s make a model using the gapminder dataset.\n\n\n## Make a model with robust standard errors using the gapminder\n## dataset\nm1 = fixest::feols(lifeExp ~ gdpPercap + pop + continent, data = gapminder::gapminder, vcov = \"HC1\") |>\n  tidy(conf.int = TRUE)\n\nhead(m1)\n\n# A tibble: 6 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  4.78e+1   3.79e-1    126.   0         4.71e+1   4.86e+1\n2 gdpPercap    4.50e-4   7.27e-5      6.19 7.67e-10  3.07e-4   5.92e-4\n3 pop          6.57e-9   1.52e-9      4.32 1.66e- 5  3.59e-9   9.55e-9\n4 continentA…  1.35e+1   6.98e-1     19.3  3.06e-75  1.21e+1   1.48e+1\n5 continentA…  8.19e+0   6.87e-1     11.9  1.44e-31  6.85e+0   9.54e+0\n6 continentE…  1.75e+1   9.88e-1     17.7  2.47e-64  1.55e+1   1.94e+1\n\n\n\nm1 |>\n  ggplot(aes(x = term, y = estimate, \n             ymin = conf.low, \n             ymax = conf.high))+\n  geom_point() +\n  geom_pointrange() + \n  coord_flip() +\n  theme_minimal() +\n  labs(x = \"\",\n       y = \"Coefficient Estimates\",\n       title = \"Coefficient Plot\")\n\n\n\nAdded Variable Plots\nMarginal Effects Plots\n\n\nmarginModel = fixest::feglm(lifeExp ~ gdpPercap + pop + continent, data = gapminder::gapminder, vcov = \"HC1\") |>\n  marginaleffects::slopes(by = TRUE)\n\n\nSimulations in R\nThe Birthday Problem\n\n\n\n",
      "last_modified": "2023-05-06T13:13:43-07:00"
    },
    {
      "path": "index.html",
      "title": "Methods Tutoring Notes",
      "description": "Welcome to the main course site\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\nWelcome to the Methods Tutoring Notes site. I put this together based on my methods training and years being a graduate quantitative methods\ntutor at UC Berkeley.\n\n\n\n",
      "last_modified": "2023-05-06T13:13:44-07:00"
    },
    {
      "path": "lectures.html",
      "title": "Lecture Notes",
      "description": "Lecture Notes and Resources\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\n\n\n",
      "last_modified": "2023-05-06T13:13:44-07:00"
    },
    {
      "path": "matrix.html",
      "title": "Linear Algebra",
      "description": "A Crash Course in Linear Algebra using R and Python\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nVectors\nVector Addition and Multiplication\nMatrices\nNorms\nOrdinary Least Squares\nCovariance Matrices and Standard Errors\nPrincipal Components Analysis\n\nThis page provides code in R and Python for doing linear algebra. In Python, we make use of the NumPy library.\n\nimport numpy as np \n\nVectors\nThe simplest way to represent vectors in R is by using the vector data structure.\n\n\nx = c(-1.1, 0.0, 3.6, -7.2)\nlength(x) ## 4 \n\n[1] 4\n\nIn python:\n\nx = np.array([-1.1, 0.0, 3.6, -7.2])\nlen(x)\n4\n\nBlock and stacked vectors\nIn addition to creating vectors, we can concatenate vectors together to produce blocked and stacked vectors using the c() function.\n\n\nx = c(1,-2)\ny = c(1,1,0)\nz = c(x,y)\nz\n\n[1]  1 -2  1  1  0\n\nIn python:\n\nx = np.array([1,-2])\ny = np.array([1,1,0])\nz = np.concatenate((x,y))\nprint(z)\n[ 1 -2  1  1  0]\n\nSome special vectors\nThe Zeros vector is a default behavior of creating a vector with a given length.\n\n\nz = numeric(3)\nz\n\n[1] 0 0 0\n\nIn python:\n\nz = np.zeros(3)\nz\narray([0., 0., 0.])\n\nThe Ones vector can be made by way of the rep() function.\n\n\no = rep(1,3)\no\n\n[1] 1 1 1\n\nIn python:\n\no = np.ones(3)\no\narray([1., 1., 1.])\n\nVector Addition and Multiplication\nIf x and y are vectors of the same size, then x+y and x-y give their element wise sum and difference respectively. R by default computes most vector operations element wise.\n\n\nx = c(1,2,3)\ny = c(100,200,300)\nx+y\n\n[1] 101 202 303\n\nIn python:\n\nx = np.array([1,2,3])\ny = np.array([100,200,300])\nx + y\narray([101, 202, 303])\nnp.add(x,y)\narray([101, 202, 303])\n\nScalar Multiplication and division\nIf a is a number and x is a vector, then we can express the scalar vector product as either a*x or x*a\n\n\na = 2 \nx = c(1,2,3)\na*x\n\n[1] 2 4 6\n\nx*a\n\n[1] 2 4 6\n\nIn python:\n\na = 2\nx = np.array([1,2,3])\na*x\narray([2, 4, 6])\nx*a\narray([2, 4, 6])\n\nUsing what we’ve learned to confirm the distributive property\nThe distributive property \\(\\beta(a+b) = \\beta a + \\beta b\\) holds for any two n-vector a and b and any scalar \\(\\beta\\).\n\n\na = c(3,5,6)\nb = c(2,4,9)\nbeta = 5\nlhs = beta*(a+b)\nrhs = beta*a + beta*b\nprint(lhs)\n\n[1] 25 45 75\n\nprint(rhs)\n\n[1] 25 45 75\n\nlhs == rhs\n\n[1] TRUE TRUE TRUE\n\nIn python:\n\na = np.array([3,5,6])\nb = np.array([2,4,9])\nbeta = 5 \nlhs = beta*(a+b)\nrhs = beta*a + beta*b\nprint('lhs:', lhs)\nlhs: [25 45 75]\nprint('rhs:', rhs)\nrhs: [25 45 75]\nlhs == rhs\narray([ True,  True,  True])\n\nInner Product\nThe inner product of n-vector x and y is denoted \\(x^Ty\\)\n\n\nx = c(1,2,3,4)\ny = c(3,4,6,7)\n\n## t() is the transpose function in R\nt(x)%*% y\n\n     [,1]\n[1,]   57\n\nIn python:\n\nx = np.array([1,2,3,4])\ny = np.array([3,4,6,7])\nnp.inner(x,y) \n\n# Alternatively \n57\nx @ y\n57\n\nMatrices\nA matrix \\(\\textbf{X}\\) is an \\(m\\) x \\(n\\) data structure that is a rectangular array of scalar numbers. The numbers \\(x_{ij}\\) are components or elements of \\(\\textbf{X}\\). The transpose of a matrix is the \\(n\\) x \\(m\\) matrix \\(\\textbf{X}'\\)\n\n\n## Creating a matrix in R \nX = matrix(seq(1,16,1), \n           nrow = 4, \n           byrow = T)\nX\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\nWe can also create matrices from vectors or from data frames\n\n\n## Equivalent to above but with vectors \nX2 = rbind(1:4, 5:8,9:12,13:16)\nX2\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\n\n\n## via a data frame \ndf = data.frame(\n  x = 1:4,\n  y = 5:8,\n  z = 9:12,\n  w = 13:16\n)\nX3 = as.matrix(df)\nX3\n\n     x y  z  w\n[1,] 1 5  9 13\n[2,] 2 6 10 14\n[3,] 3 7 11 15\n[4,] 4 8 12 16\n\nIn python:\n\nX = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12], [13,14,15,16]])\nX\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12],\n       [13, 14, 15, 16]])\nX.shape\n(4, 4)\n\nSome other useful matrices\n\nnp.identity(4) \narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\nnp.zeros((4,4))\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\nnp.ones((4,4))\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\nAdditional Definitions\nThe transpose in R\n\n\nX_transpose = t(X)\nX_transpose\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nIn python:\n\nX_transpose = X.T\nX_transpose\narray([[ 1,  5,  9, 13],\n       [ 2,  6, 10, 14],\n       [ 3,  7, 11, 15],\n       [ 4,  8, 12, 16]])\n\nA diagonal matrix with all elements not on the diagonal equal to zero is a diagonal matrix. By default, R creates an identity matrix with the diag() function.\n\n\ndM = diag(4)\ndM\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n\nThe trace of a matrix is the sum of the diagonal elements\n\\[trace(X) = \\sum_{i=1}^n x_{ii}\\]\n\n\nmatrix_trace = function(mat){\n  return(sum(diag(mat)))\n}\n\nmatrix_trace(X)\n\n[1] 34\n\nMatrix Arithmetic\nAddition and subtraction of matrices of the same order are performed element by element. Scalar multiplication is element by element\n\n\nA = matrix(data = seq(1,9,1), nrow = 3, byrow = T)\nB = matrix(data = seq(1,9,1), nrow = 3, byrow = T)\nA+B\n\n     [,1] [,2] [,3]\n[1,]    2    4    6\n[2,]    8   10   12\n[3,]   14   16   18\n\nProvided that the number of columns of A equals the number of rows of B, we can multiple A by B.\n\n\n## To get the appropriate multiplication, we wrap * in %*%\nA%*%B\n\n     [,1] [,2] [,3]\n[1,]   30   36   42\n[2,]   66   81   96\n[3,]  102  126  150\n\nNote that we can perform X’X in one of two ways.\n\n\nt(A)%*%A \n\n     [,1] [,2] [,3]\n[1,]   66   78   90\n[2,]   78   93  108\n[3,]   90  108  126\n\n## same but can be slightly faster \ncrossprod(A)\n\n     [,1] [,2] [,3]\n[1,]   66   78   90\n[2,]   78   93  108\n[3,]   90  108  126\n\nIn python:\n\nA = np.array([[1,2,3], [4,5,6], [7,8,9]])\nB = np.array([[1,2,3], [4,5,6], [7,8,9]])\nA+B\narray([[ 2,  4,  6],\n       [ 8, 10, 12],\n       [14, 16, 18]])\nnp.add(A,B)\n\n# Multiplication \narray([[ 2,  4,  6],\n       [ 8, 10, 12],\n       [14, 16, 18]])\nnp.matmul(A,B)\narray([[ 30,  36,  42],\n       [ 66,  81,  96],\n       [102, 126, 150]])\nnp.matmul(A.T, A)\narray([[ 66,  78,  90],\n       [ 78,  93, 108],\n       [ 90, 108, 126]])\n\nNorms\nInformally, the norm of a vector tells us how big it is. Formally, a norm is a function \\(||\\cdot||\\) that maps a vector to a scalar which satisfies:\nGiven any vector x \\(||\\alpha x|| = |\\alpha|||x||\\).\nFor any vectors x, y, norms satisfy the triangle inequality \\(||x + y|| \\leq ||x|| + ||y||\\)\nThe norm of a vector \\(||x|| > 0\\hspace \\forall x \\neq 0\\)\nThe most common norm is the \\(\\ell_2\\) norm or Euclidean norm which is defined as \\(||x||_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\\)\n\n\nx = c(3,-4)\nl2 = sqrt(crossprod(x,x))\nl2\n\n     [,1]\n[1,]    5\n\n## We can also call R's built in norm() function \nnorm(x, \"2\")\n\n[1] 5\n\nIn python\n\nx = np.array([3,-4])\nnp.linalg.norm(x)\n5.0\n\nWe sometimes take the Manhattan norm (\\(\\ell_1\\)) which sums the absolute values of a vectors elements \\(||x||_1 = \\sum_{i=1}^n|x_i|\\)\n\n\nx = c(3,-4)\nl1 = function(x){\n  return(sum(abs(x)))\n}\nl1(x)\n\n[1] 7\n\nIn python\n\nx = np.array([3,-4])\nnp.linalg.norm(x, 1)\n7.0\n\nThe infinity norm or max norm (\\(\\ell_\\infty\\)) is common in machine learning applications defined as \\(||x||_\\infty = \\max_i |x_i|\\), which simplifies to the absolute value of the element with the largest magnitude in the vector.\n\n\nx = matrix(c(3,-4), byrow = T, nrow = 2)\nnorm(x, type =\"I\")\n\n[1] 4\n\nl_inf = function(x){\n  return(max(abs(x)))\n}\nl_inf(c(3,-4))\n\n[1] 4\n\nIn python\n\nx = np.array([3,-4])\nmax(abs(x))\n4\n\nOrdinary Least Squares\nThe simplest linear model expresses the dependence of a dependent or response variable y on independent variables \\(x_1,.., x_p\\) and is usually written \\(y = X\\beta + \\epsilon\\). See the Lecture Notes for more details on the properties of this model.\nDefine the design matrix as the \\(n \\times p\\) matrix of independent variables \\(x_1,..,x_p\\) and assume that the first columns is a column of ones and that the design matrix has full rank. Then the usual OLS estimator is defined as \\((X'X)^{-1}X'Y\\)\n\n\nbeta_estimator = function(X,y){\n  X = cbind(rep(1,nrow(X)), X)\n  betas = solve(t(X)%*%X)%*%t(X)%*%y\n  return(betas)\n}\n\n## example data \nset.seed(123)\nx1 = rnorm(10000)\nx2 = rnorm(10000)\ny = 2*x1 + 4*x2 + runif(10000)\n\nX = cbind(x1, x2)\nbeta_estimator(X,y)\n\n        [,1]\n   0.4997833\nx1 1.9971656\nx2 4.0020879\n\nIn python:\n\nx1 = np.random.default_rng(seed=123).normal(0, 1, size =1000)\nx2 = np.random.default_rng().normal(0, 1, size =1000)\nones = np.ones(1000)\ny = 2*x1 + 4*x2 + np.random.default_rng().uniform(size = 1000)\n\nX = np.concatenate((ones, x1, x2)).reshape((-1,3), order = 'F')\n\n# Alternatively we can make a matrix with column_stack()\nX = np.column_stack((ones, x1, x2))\n\n# (X'X)^-1X'y\nnp.linalg.inv(X.T @ X) @ X.T @ y\narray([0.49364612, 2.00709351, 3.98956255])\n\nCovariance Matrices and Standard Errors\nClassical Standard Errors\nThe least squares solution gives us point estimates for coefficients, but if we want to do inference, we need to get standard errors. See the Lecture Notes for more details.\nTo get standard errors, we must first calculate the covariance matrix of our estimates and then take the square root of the diagonal.\n\n\nbeta_estimator = function(X,y){\n  X = cbind(rep(1,nrow(X)), X)\n  betas = solve(t(X)%*%X)%*%t(X)%*%y\n  return(betas)\n}\n\nbetas_and_std_errors = function(X,y){\n  betas = beta_estimator(X,y)\n  ## get the design matrix again\n  X = cbind(rep(1,nrow(X)), X)\n  residuals = y - X %*% betas \n  \n  ## Degree of freedom calculation \n  p = ncol(X) - 1 \n  df = nrow(X) - p - 1 \n  \n  ## Residual variance \n  res_var = sum(residuals^2) / df \n  \n  ## Covariance matrix of estimate \n  ## cov(\\hat{\\beta}|X) = (X'X)^-1X'cov(\\epsilon|X)X(X'X)^-1\n  beta_cov = res_var * solve(t(X)%*%X)\n  \n  ## Standard errors are square root of diagonal \n  return(list(beta = betas, se = sqrt(diag(beta_cov))))\n}\n\n## example data \n## To keep consistent with python examples I use pre-generated \n## random variables created in python with:\n# x1 = np.random.default_rng(seed=123).normal(0, 1, size =1000)\n# x2 = np.random.default_rng().normal(0, 1, size =1000)\n# ones = np.ones(1000)\n# y = 2*x1 + 4*x2 + np.random.default_rng().uniform(size = 1000)\n\nx1 = read.csv(\"x1.csv\", header = F) |>\n  unlist()\nx2 = read.csv(\"x2.csv\", header = F) |>\n  unlist()\ny = read.csv(\"y.csv\", header = F) |>\n  unlist()\nX = cbind(x1, x2)\n\nbetas_and_std_errors(X,y)\n\n$beta\n       [,1]\n   0.484434\nx1 2.005286\nx2 3.994715\n\n$se\n                     x1          x2 \n0.009244982 0.009198621 0.009201766 \n\nIn python\n\ndef betas_and_se(X,y):\n  betas = np.linalg.inv(X.T @ X) @ X.T @ y\n  residuals = y - X @ betas\n  df = X.shape[0] - X.shape[1]\n  res_var = np.sum(residuals**2) / df\n  cov_mat = res_var * np.linalg.inv(X.T @ X)\n  se = np.sqrt(np.diag(cov_mat))\n  return betas, se\n\n\nx1 = np.loadtxt(\"x1.csv\", delimiter = \",\", dtype = float)\nx2 = np.loadtxt(\"x2.csv\", delimiter = \",\", dtype = float)\ny = np.loadtxt(\"y.csv\", delimiter = \",\", dtype = float)\n\n# One way to make a matrix from vectors \n# X = np.concatenate((ones, x1, x2)).reshape((-1,3), order = 'F')\n\n# Alternatively we can make a matrix with column_stack()\nX = np.column_stack((ones, x1, x2))\n\nbetas_and_se(X,y)\n(array([0.48443399, 2.00528572, 3.99471469]), array([0.00924498, 0.00919862, 0.00920177]))\n\nSandwich Standard Errors\n\n\nbetas_and_std_errors_sandwich = function(X,y){\n  betas = beta_estimator(X,y)\n  ## get the design matrix again\n  X = cbind(rep(1,nrow(X)), X)\n  residuals = y - X %*% betas \n\n  ## Degree of freedom calculation \n  p = ncol(X) - 1 \n  df = nrow(X) - p - 1 \n  \n  \n  ## HC1 or Eicker-Huber_White Variance Estimator \n  ## This is a way of creating a diagonal matrix from a matrix\n  ## with one column in R. \n  u2 = matrix(diag(as.vector(residuals^2)), ncol = nrow(X))\n  beta_cov = (nrow(X)/df) * solve(t(X)%*%X) %*% t(X) %*% u2 %*% X  %*% solve(t(X)%*%X)\n  \n  ## Standard errors are square root of diagonal \n  return(list(beta = betas, se = sqrt(diag(beta_cov))))\n}\n\n## example data \nx1 = read.csv(\"x1.csv\", header = F) |>\n  unlist()\nx2 = read.csv(\"x2.csv\", header = F) |>\n  unlist()\ny = read.csv(\"y.csv\", header = F) |>\n  unlist()\nX = cbind(x1, x2)\n\nbetas_and_std_errors_sandwich(X,y)\n\n$beta\n       [,1]\n   0.484434\nx1 2.005286\nx2 3.994715\n\n$se\n                     x1          x2 \n0.009248444 0.009296868 0.009105095 \n\nIn python\n\ndef betas_and_std_errors_sandwich(X,y):\n  ## Calculate Beta coefficients\n  betas = np.linalg.inv(X.T @ X) @ X.T @ y\n  \n  ## Get residuals, degree of freedom, and squared residuals\n  residuals = y - X @ betas\n  df = X.shape[0] - X.shape[1]\n  u2 = residuals**2\n\n  ## apply the HC1 formula with appropriate correction \n  beta_cov = (X.shape[0]/df) * np.linalg.inv(X.T @ X) @ X.T @ np.diag(u2) @ X @ np.linalg.inv(X.T @ X)\n  se = np.sqrt(np.diag(beta_cov))\n  return betas, se\n\nx1 = np.loadtxt(\"x1.csv\", delimiter = \",\", dtype = float)\nx2 = np.loadtxt(\"x2.csv\", delimiter = \",\", dtype = float)\ny = np.loadtxt(\"y.csv\", delimiter = \",\", dtype = float)\n\nX = np.column_stack((ones, x1, x2))\n\nbetas_and_std_errors_sandwich(X,y)\n(array([0.48443399, 2.00528572, 3.99471469]), array([0.00924844, 0.00929687, 0.00910509]))\n\nPrincipal Components Analysis\nSuppose we have a collection of points in \\(\\mathbb{R}^n\\) and we want to encode these points to represent a lower-dimensional version of them. If \\(X'\\) is a \\(n \\times p\\) matrix, then the first principal component of \\(X'\\) is the linear combination of the \\(p\\) variables \\(y'_1 = (X-\\bar{X})'a_1\\) s.t \\(V(y_1)'\\) is maximized subject to the constraint that \\(a_1'a_1 =1\\). Subsequent principal components are defined successively in a similar way.\n\n\noptions(scipen=999)\n\nscale_and_center = function(x){\n  ## center columns \n  x_s = x - mean(x)\n  \n  ## return scaled columns \n  return(x_s/sd(x))\n}\n\nprcomp_by_hand = function(A) {\n  ## Calculate mean of each column\n  C = apply(A, 2, scale_and_center)\n  \n  ## Calculate covariance matrix of centered matrix\n  V = cov(C)\n  ## Eigendecomposition of covariance matrix\n  eig = eigen(V, symmetric = F)\n  ## Transpose eigenvectors\n  eig.t = t(eig$vectors)\n  ## calculate new dataset\n  A.new = eig.t %*% t(C)\n  df.new = t(A.new)\n  return(list(points = df.new, vectors = eig$vectors))\n}\n\nresults = prcomp_by_hand(USArrests)\nresults$vectors \n\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\nhead(results$points)\n\n                 [,1]       [,2]        [,3]         [,4]\nAlabama     0.9756604  1.1220012 -0.43980366  0.154696581\nAlaska      1.9305379  1.0624269  2.01950027 -0.434175454\nArizona     1.7454429 -0.7384595  0.05423025 -0.826264240\nArkansas   -0.1399989  1.1085423  0.11342217 -0.180973554\nCalifornia  2.4986128 -1.5274267  0.59254100 -0.338559240\nColorado    1.4993407 -0.9776297  1.08400162  0.001450164\n\nUsing built-in function in R.\n\n\n## As Brian Ripley pointed out on R-help back in 2003\n## using different compilers on the same machine and the same version of R may give different signs for the eigenvectors. The moral is, don't rely on the signs of eigenvectors! (This is on the help page.)\nt = prcomp(USArrests, center = T,scale = T)\nhead(-1*t$rotation)\n\n               PC1        PC2        PC3         PC4\nMurder   0.5358995  0.4181809 -0.3412327 -0.64922780\nAssault  0.5831836  0.1879856 -0.2681484  0.74340748\nUrbanPop 0.2781909 -0.8728062 -0.3780158 -0.13387773\nRape     0.5434321 -0.1673186  0.8177779 -0.08902432\n\nhead(-1*t$x) \n\n                  PC1        PC2         PC3          PC4\nAlabama     0.9756604  1.1220012 -0.43980366 -0.154696581\nAlaska      1.9305379  1.0624269  2.01950027  0.434175454\nArizona     1.7454429 -0.7384595  0.05423025  0.826264240\nArkansas   -0.1399989  1.1085423  0.11342217  0.180973554\nCalifornia  2.4986128 -1.5274267  0.59254100  0.338559240\nColorado    1.4993407 -0.9776297  1.08400162 -0.001450164\n\nIn python\n\nnp.set_printoptions(suppress=True)\n\ndef scale(mat):\n  center = mat - np.mean(mat, axis = 0)\n  scale = center / np.std(mat, axis = 0, ddof = 1)\n  return scale\n\ndef pca(mat):\n  center = mat - np.mean(mat, axis = 0)\n  scale = center / np.std(mat, axis = 0, ddof = 1)\n  cov_mat = np.cov(scale, rowvar = False)\n  vals, vec = np.linalg.eig(cov_mat)\n  \n  ## Sort eigen vectors and eigen values in order \n  idx = (-vals).argsort()\n  vals = vals[idx]\n  vec = vec[:, idx]\n  ## Calculate new dataset \n  A_new = (vec.T @ scale.T).T\n  return vec, A_new\n\n## Test with the same UArrests dataset \n\n## We need to do a bit of cleaning of the raw dataset to \n## turn it into an appropriate matrix\nusarrests = np.loadtxt(\"https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/USArrests.csv\", delimiter = \",\", dtype = str, skiprows = 1)\nusarrests = np.delete(usarrests, (0), axis = 1)\nusarrests = usarrests.astype(dtype = \"float\")\nvec, transformed = pca(usarrests)\nprint('Principal Components:', vec)\nPrincipal Components: [[ 0.53589947  0.41818087 -0.34123273  0.6492278 ]\n [ 0.58318363  0.1879856  -0.26814843 -0.74340748]\n [ 0.27819087 -0.87280619 -0.37801579  0.13387773]\n [ 0.54343209 -0.16731864  0.81777791  0.08902432]]\ntransformed[:6]\narray([[ 0.97566045,  1.12200121, -0.43980366,  0.15469658],\n       [ 1.93053788,  1.06242692,  2.01950027, -0.43417545],\n       [ 1.74544285, -0.73845954,  0.05423025, -0.82626424],\n       [-0.13999894,  1.10854226,  0.11342217, -0.18097355],\n       [ 2.49861285, -1.52742672,  0.592541  , -0.33855924],\n       [ 1.49934074, -0.97762966,  1.08400162,  0.00145016]])\n\n\n\n\n",
      "last_modified": "2023-05-06T13:13:53-07:00"
    }
  ],
  "collections": []
}
