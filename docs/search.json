{
  "articles": [
    {
      "path": "about.html",
      "title": "Alex Stephenson",
      "author": [],
      "contents": "\n\n          \n          \n          Methods Tutoring\n          \n          \n          Home\n          Creator\n          \n          \n          Lecture Notes\n           \n          ▾\n          \n          \n          Causal Inference\n          Game Theory\n          Linear Algebra\n          Machine Learning\n          Optimization\n          Probability\n          Statistics\n          Survey Design and Sampling\n          \n          \n          Math Camp Solutions\n          \n          \n          Coding\n           \n          ▾\n          \n          \n          Python\n          R\n          Applied Linear Algebra\n          \n          \n          ☰\n          \n          \n      \n        \n          \n            Alex Stephenson\n          \n          \n            \n              I am a PhD Candidate at the University of California,\n              Berkeley.\n            \n            \n              I am a PhD Candidate at the University of California,\n              Berkeley.\n            \n          \n\n          \n            \n              \n                  \n                    \n                      Github\n                    \n                  \n                \n                                \n                  \n                    \n                      Twitter\n                    \n                  \n                \n                                \n                  \n                    \n                      My Website\n                    \n                  \n                \n                              \n          \n\n          \n            \n              \n                                \n                  \n                    Github\n                  \n                \n                                \n                  \n                    Twitter\n                  \n                \n                                \n                  \n                    My Website\n                  \n                \n                              \n            \n          \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2023-05-07T18:15:43-07:00"
    },
    {
      "path": "coding.html",
      "title": "Coding Notes",
      "description": "Coding Notes and Resources\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nBasic R Operations\nData Types\nSubsetting\nControl Operations\nFunctions\n\nGraphing in R\nThe Basics\nUseful Plots for Political Science\nAdded Variable Plots\nMarginal Effects Plots\n\nSimulations in R\nThe Birthday Problem\n\n\n\n\nlibrary(broom)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(fixest)\nlibrary(ggplot2)\nlibrary(marginaleffects)\n\n\nBasic R Operations\nThere are a voluminous amount of resources on R online. Here is a very minimal crash course.\nThe most important underlying concept in R is that (almost) everything is an object.\nData Types\nR has six basic data types. The four most common types we work with are vectors, lists, matrices, and data frames.\nVectors\n\n\n## Three ways to create a vector\nv = c(1,2,3,4,5)\nv2 = seq(from=1,to=5, by = 1)\n\n## Since the sequence is counting with no breaks we can also do this\nv3 = 1:5\n\n\nVectors can also be characters (strings in other languages)\n\n\nw = c(\"A\", \"B\", \"C\")\nw2 = c(\"1\",\"2\",\"3\")\n\n## We can convert to other types by the as.* series \nw3 = as.character(v)\nw4 = as.numeric(w2)\n\n\nLists\n\n\nl = list(v, w)\nl\n\n[[1]]\n[1] 1 2 3 4 5\n\n[[2]]\n[1] \"A\" \"B\" \"C\"\n\nMatrices\n\n\nm = matrix(seq(1,16,1), nrow = 4, byrow = T)\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\nFor much more about linear algebra in R, consult the Matrix Algebra in R section.\nData Frames\n\n\ndf = data.frame(\n  var1 = letters[1:5],\n  var2 = c(1,2,3,4,5)\n)\ndf\n\n  var1 var2\n1    a    1\n2    b    2\n3    c    3\n4    d    4\n5    e    5\n\nSubsetting\nControl Operations\nFunctions\nGraphing in R\nThe Basics\nR has default plotting, but for the purpose of these notes I exclusively use ggplot2. A main distinction between base plotting and ggplot2 is that ggplot2 requires a data frame and presumes that your data is tidy.\n\n\n## Using the built-in dataset diamonds\ndf = diamonds\n\n## Data\ndf |>\n  ## Mapping\n  ggplot(aes(x = carat, y = price)) +\n  ## geom\n  geom_point() +\n  ## we can have multiple geoms \n  geom_smooth(method = \"lm\", se = F)+\n  ## Labels\n  labs(x = \"Carat\",\n       y = \"Price\",\n       title = \"Example Chart\")+\n  ## theme \n  theme_minimal()\n\n\n\nFor more extensive discussion of graphing capabilities in R, check out Data Visualization: A Practical Introduction by Healy.\nUseful Plots for Political Science\nCoefficient Plot\nFirst let’s make a model using the gapminder dataset.\n\n\n## Make a model with robust standard errors using the gapminder\n## dataset\nm1 = fixest::feols(lifeExp ~ gdpPercap + pop + continent, data = gapminder::gapminder, vcov = \"HC1\") |>\n  tidy(conf.int = TRUE)\n\nhead(m1)\n\n# A tibble: 6 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  4.78e+1   3.79e-1    126.   0         4.71e+1   4.86e+1\n2 gdpPercap    4.50e-4   7.27e-5      6.19 7.67e-10  3.07e-4   5.92e-4\n3 pop          6.57e-9   1.52e-9      4.32 1.66e- 5  3.59e-9   9.55e-9\n4 continentA…  1.35e+1   6.98e-1     19.3  3.06e-75  1.21e+1   1.48e+1\n5 continentA…  8.19e+0   6.87e-1     11.9  1.44e-31  6.85e+0   9.54e+0\n6 continentE…  1.75e+1   9.88e-1     17.7  2.47e-64  1.55e+1   1.94e+1\n\n\n\nm1 |>\n  ggplot(aes(x = term, y = estimate, \n             ymin = conf.low, \n             ymax = conf.high))+\n  geom_point() +\n  geom_pointrange() + \n  coord_flip() +\n  theme_minimal() +\n  labs(x = \"\",\n       y = \"Coefficient Estimates\",\n       title = \"Coefficient Plot\")\n\n\n\nAdded Variable Plots\nMarginal Effects Plots\n\n\nmarginModel = fixest::feglm(lifeExp ~ gdpPercap + pop + continent, data = gapminder::gapminder, vcov = \"HC1\") |>\n  marginaleffects::slopes(by = TRUE)\n\n\nSimulations in R\nThe Birthday Problem\n\n\n\n",
      "last_modified": "2023-05-07T18:15:55-07:00"
    },
    {
      "path": "index.html",
      "title": "Methods Tutoring",
      "description": "A depository of quantitative methods notes\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\nWelcome to the Methods Tutoring Notes site. I put this together based on my methods training and years being a graduate quantitative methods tutor at UC Berkeley.\nThe site contains several different resources. First and foremost, it includes notes on the primary quantitative methods topics common to a graduate course sequence in political science. Second, there are coding notes for both R and Python.1. I have endeavored to provide an introduction to both languages, focusing on topics that have proved more difficult for graduate students at UC Berkeley. Third, I provide code for standard research techniques and methods currently in use in top political science journals. For pedagogical reasons, every single example and application on this website has both an R and a Python implementation.\nTexts\nI have benefited greatly in my own methods education from the teaching of professors at Berkeley. All errors on this website are entirely my own and should in no way reflect their excellent guidance.\nIn addition to classroom teaching and tutoring, the underlying materials on this website are drawn from many resources. Below is a non-exhaustive list of books from which these materials are drawn by subject. I encourage individuals who reach this page for self-study to consult them for more detail. I have put in bold the text or sets of texts that strike me as the most reasonable entry point in each category.\nCausal Inference\nAngrist, J, & Pischke J.S. 2009. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton: Princeton University Press.\nAngrist, J., & Pischke J.S. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton: Princeton University Press.\nCunningham, S. 2021. Causal Inference: The Mixtape. New Haven: Yale University Press. https://mixtape.scunning.com/\nDunning, T. 2012. Natural Experiments in the Social Sciences: A Design-Based Approach. Cambridge: Cambridge University Press.\nGerber, Alan S, and Donald P Green. 2012. Field Experiments: Design, Analysis and Interpretation. New York: W.W. Norton & Co.\nImbens, G., & Rubin, D. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge: Cambridge University Press.\nHernán, M.A, & Robins,J.A.. 2021. Causal Inference: What If. Boca Raton: Chapman & Hall/CRC Press\nMorgan, S.L., & Winship, C. 2014. Counterfactuals and Causal Inference: Methods and Principles for Social Research. 2nd ed. Cambridge: Cambridge University Press.\nRosenbaum, P.R., 2020. Design of Observational Studies. Cham: Springer International Publishing.\nGame Theory\nFudenberg, Drew, and Jean Tirole. 1991. Game Theory. Cambridge: MIT Press.\nGehlbach, S. 2013. Formal Models of Domestic Politics. Cambridge: Cambridge University Press.\nMcCarty, N., & Meirowitz, A. 2007. Political Game Theory: An Introduction. Cambridge: Cambridge University Press.\nTadelis, Steve. 2013. Game Theory: An Introduction. Princeton; Oxford: Princeton University Press.\nLinear Algebra\nBoyd, S., & Vandenberghe, L. 2018. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares. 1st ed. Cambridge University Press. https://web.stanford.edu/~boyd/vmls/\nHarville, D.A., 1997. Matrix Algebra from a Statistician’s Perspective. New York: Springer.\nMachine Learning\nGoodfellow, I., Bengio, Y. & Courville, A. 2016. Deep Learning. Cambridge: MIT Press.\nHastie, T., Tibshirani, R., & Friedman, J. 2013. Elements of Statistical Learning. 2nd ed. Springer.\nJames, G., Witten, D., Hastie, T, & Tibshirani, R. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. New York: Springer. https://www.statlearning.com/\nKneusel, R. T., 2021. Practical Deep Learning. No Starch Press.\nMurphy, K. 2020. Probablistic Machine Learning. MIT Press.\nZhang, Aston, Zachary C Lipton, Mu Li, and Alexander J Smola. 2022. Dive into Deep Learning.\nMath Prefreshers\nGill, J. 2006. Essential Mathematics for Political and Social Research. Cambridge: Cambridge University Press.\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Pres.\nOptimization\nSimon, Carl P., and Lawrence E. Blume. 1994. Mathematics for Economists. 1. ed. New York: Norton.\nStewart, J. 2016. Calculus: Early Transcendentals. 8th ed. Boston, MA, USA: Cengage Learning.\nProbability\nPitman, J. 1993. Probability. New York, NY: Springer New York.\nStatistics\nAronow, P., & Miller, B. 2019. Foundations of Agnostic Statistics. 1st ed. Cambridge University Press.\nCasella, G., & Berger, R. 2002. Statistical Inference. 2nd ed. Pacific Grove: Duxbury.\nDavidson, R., & MacKinnon, J.G. 2004. Econometric Theory and Methods. New York: Oxford University Press.\nFreedman, D. (2009). Statistical Models: Theory and Practice (2nd ed.). Cambridge: Cambridge University Press. doi:10.1017/CBO9780511815867\nFreedman, David, Robert Pisani, and Roger Purves. 2007. Statistics. 4th ed. New York: W.W. Norton & Co.\nGailmard, S. 2014. Statistical Modeling and Inference for Social Science: 1st ed. Cambridge University Press.\nGreene, W. 2017. Econometric Analysis. 8th ed. Upper Saddle River: Prentice Hall.\nWooldridge, J. M. 2010. Econometric Analysis of Cross Section and Panel Data. 2nd ed. Cambridge: MIT Press.\nWooldridge, J. M. 2016. Introductory Econometrics: A Modern Approach. 7th ed. Adrian MI: South-Western Cengage Learning.\nSurvey Design and Sampling\nLohr, S. 2021. Sampling: Design and Analysis. 3rd ed. Boca Raton: Chapman and Hall/CRC.\nThompson, Steven K. 2012. Sampling. 3rd ed. Hoboken, N.J: Wiley.\n\nThere are no resources for Stata by choice. Stata is not free software, rarely used in industry, and not needed to do research.↩︎\n",
      "last_modified": "2023-05-07T18:15:55-07:00"
    },
    {
      "path": "intro-seq.html",
      "title": "Math Camp Pre-track Solutions",
      "description": "A set of solutions for UC Berkeley's Math Camp Pre-track\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nSelf-Guided Summer Methods Program\nLinear Algebra Sequence\nM+S Chapter 1: Preliminaries\nM + S Chapter 2: Arithmetic Review\nM+S Chapter 12: Vectors and Matrices\nM+S Chapter 13: Vector Spaces and Systems of Equations\n\nCalculus Sequence\nM+S Chapter 3: Functions\nM+S Chapter 4: Limits and Continuity, Sequences and Series, and More on Sets\nM+S Chapter 5: Introduction to Derivatives\nM+S Chapter 6: Rules of Derivatives\nM+S Chapter 7: Integrals\nM+S Chapter 8: Extrema in One Dimension\nM+S Chapter 15: Multivariate Calculus\n\nRandom Variables and Probability Sequence\nM+S Chapter 1: Preliminaries\nFPP Chapter 4,6\nFPP Chapter 13, 14\nM+S Chapter 9: Introduction to Probability\nM+S Chapter 10: Introduction to Discrete Distributions\nFPP Chapter 8, 9\nM+S Chapter 11: Continuous Distributions\nFPP Chapter 10,11,12,16,17\n\nR Sequence\n\nSelf-Guided Summer Methods Program\nThe self-guided summer methods program is based on a suggested syllabus for preparing for the Political Science program at UC Berkeley’s formal and quantitative methods course. The purpose of the self-guided course is to help students prepare for the summer math camp and the introductory courses. The primary books for the self-guided course are A Mathematics Course for Political and Social Research herein dubbed M+S and Statistics: Fourth Edition herein dubbed as FPP.\nLinear Algebra Sequence\nThe Linear Algebra sequence suggested timeline is 2-4 weeks. It is entirely composed of chapters from M+S and comprises mathematical preliminaries, vectors and matrices, and vector spaces and systems of equation.\nM+S Chapter 1: Preliminaries\nExercises\nExercise 1\nParty identification of delegates at a political convention is a constant\nWar participation of the Great Powers is a variable.\nVoting record of members of Congress relative to the stated position of the president is a variable.\nRevolutions in France, Russia, China, Iran, and Nicaragua is a variable.\nAn individual voter’s vote choice in the 1992 presidential election is a constant.\nAn individual voter’s vote choice in the 1960-1992 presidential elections is a variable.\nVote choice in the 1992 presidential election is a variable.\nExercise 4\n\n\n\nExercise 5\nLet \\(A = \\{1,5,10\\}\\) and \\(B = \\{1,2,...,10\\}\\)\n\\(A \\subset B\\) is true. All elements of A are contained in B. The reverse is not true. For example \\(3 \\not\\subset A\\) but is in \\(B\\).\n\\(A \\cup B = \\{1,2,3,4,5,6,7,8,9,10\\}\\) because A is entirely contained within B, so the union of the two sets is just the set B itself.\n\\(A \\cap B = A\\) because A is entirely contained within B, so the intersection between the two sets is just the set A itself.\nPartition \\(B\\) into two sets \\(A = \\{1,5,10\\}\\) and \\(C = \\{2,3,4,6,7,8,9\\}\\). \\(C = A^C\\) or the set of elements in B that are not in A. \\(A^C\\) is known as the complement of A.\nPerhaps unsurprisingly \\(A \\cup C = A \\cup A^C = B\\).\n\\(A \\cap C = \\emptyset\\) or the empty set. There are no elements simultaneously within both A and C.\nExercise 8\nProve that the sum of any two even numbers is even, the sum of any two odd numbers is even, and the sum of any odd number with any even number is odd.\nA\nDefine the set of even numbers as the set of integers multiplied by 2 so the set of even numbers can be described as \\(\\{2n: \\forall n \\in \\mathbb{I}\\}\\). Any two even numbers can be written as \\(2n + 2m\\). For example, we can write \\(4+2\\) as \\(2(1) + 2(2)\\) where \\(n=1, m = 2\\). Since both numbers have the common factor 2, we can pull it out and rearrange \\(2(n+m) = 2n + 2m\\). Now we can simply rewrite \\(n+m = k\\) and the expression simplifies to \\(2k\\) which is the equivalent definition of an even number since we have proved that this is also even.\nB\nThe same set up as above for evens also reveals that any odd number is an even number + 1. For example, 3 is odd and \\(3 = 2+1\\), or more generally any odd number \\(\\{k : 2n +1, \\forall i \\in \\mathbb{I}\\). The sum of any two odd numbers can be written \\((2n + 1) + (2m +1) = 2(n+m) + 2\\). To get a little intuition that this is correct, consider \\(1 + 1 = 2(0+0) + 2\\), while \\(1 + 3 = 2(0+1)+2\\). Since any even number added to another even number yields an even number, and the first part of the expression \\(2(n+m)\\) we have already proven to be even, we are done.\nC\nWe now have \\(2n + 2m + 1\\), for example \\(2 + 3 = 2(1) + 2(1)+1\\). We know from part A, that \\(2n + 2m\\) is even, and we can note that the definition of an odd number is an even number plus 1. Note that we can equivalently define the odd numbers as \\(2k - 1\\) (think 3 as either being \\(2(1) + 1 \\mid 2(2) - 1\\))\nAssume that any even number plus an odd number is equal to some even number \\(2p\\). Then:\n\\[\\begin{aligned}\n2n + 2m + 1 &= 2p \\\\\n2(n + m) + 1 &= 2p \\\\\n2(n + m) &= 2p - 1 \\\\\n2k &= 2p - 1\n\\end{aligned}\\]\nwhich yields a contradiction because this states that an even number is equal to an odd number.\nWhile not explicitly required, it’s worth pointing out why the above is a contradiction.\nSuppose for the sake of contradiction that an integer \\(x\\) was both odd and even. Then \\(x = 2y\\) for some integer \\(y\\) by the definition of evenness and \\(x = 2y + 1\\) by the definition of oddness. Thus, \\(2y = 2y +1\\) or \\(1 = 2(y - z) \\implies \\frac{1}{2} = y-z\\) but we know that \\(\\frac{1}{2}\\) is not an integer and any integer added to another integer must always yield an integer.\nM + S Chapter 2: Arithmetic Review\nExercises\nExercise 1-4\nThe expression on the left hand side is the problem. The goal is to complete the following equations.\n\\(x^1 = x\\)\n\\(-a \\times (-b)^2 = -ab^2\\)\n\\(\\sum_{i=1}^4x_i = x_1 + x_2 + x_3 + x_4\\)\n\\(\\prod_{m=6}^9x_m = x_5x_6x_7x_8x_9\\)\nExercise 10\nRepresent the following as a ratio, a proporition, and a percentage:\nLatinos relative to all others: African American 98,642, Asian 62,346, Caucasian 436,756, Latino 105,342, Other 32,654\nThe ratio is 105342:630398. The proportion is \\(\\approx.14\\). The percentage is \\(\\approx14.\\%\\)\nIndependent registered voters relative to Republicans: Democrats 432, Independent 221; Republicans 312.\nThe ratio is 221:312. The proportion is \\(\\approx.41\\). The percentage is \\(41\\%\\).\nFollowing B but now Republican relative to Democrats\nThe ratio is 312:432. The proportion is \\(\\approx.42\\). The percentage is \\(42\\%\\).\nExercise 13\nIf voter turnout in the United States in 1996 was \\(56\\%\\) and in 2000 it was \\(62\\%\\), what was the percentage change in turnout from 1996 to 2000?\nThe formula for percentage change is \\(\\frac{x_2 - x_1}{x_1}\\). Plugging in values we get \\(\\approx 10.7\\%\\)\nExercise 15\nSimplify the following expressions into one term.\n\\(xz + yz = z(x+y)\\).\n\\(mn + ln - pn = n(m + l - p)\\)\n\\(zyx - 2yx = yx(z-2)\\)\n\\((z+x)y\\frac{1}{x} = \\frac{y(x+z)}{x}\\)\nExercise 17\nSimplify \\(\\frac{5 + 17x + 4x + 7}{42x}\\)\n\\(\\frac{1}{2} + \\frac{2}{7x}\\). Group and add like terms. Split the fraction. Cancel out the common terms and reduce both fractions.\nExercise 22\nFactor and reduce \\(\\frac{\\beta - \\alpha}{\\alpha^2- \\beta^2}\\)\n\\(\\frac{-1}{\\alpha+\\beta}\\). The denominator can be reduced to \\((\\alpha+\\beta)(\\alpha-\\beta)\\). Multiply both the numerator and the denominator by \\(-1\\) makes the numerator \\(\\alpha - \\beta\\) and the denominator \\(-(\\alpha+\\beta)(\\alpha-\\beta)\\). We cancel the common factor to complete the reduction.\nExercise 23\nSolve\n\\[\\begin{aligned}\n15\\delta + 45 - 6\\delta &= 36 \\\\\n15\\delta + 45 - 6\\delta - 36 &= 0 \\\\\n15\\delta + 6\\delta - 9 &= 0 \\\\\n9\\delta - 9 &= 0 \\\\\n\\delta &= -1\n\\end{aligned}\\]\nExercise 29\nSolve using the quadratic formula \\(2x^2 + 5x - 7\\)\nThe quadratic formula is \\(\\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\).\nPlugging in values we get \\(x = 1 \\mid x = \\frac{-7}{2}\\)\nExercise 30\nDerive the quadratic formula by completing the sequare for the equation \\(ax^2 + bx + c = 0\\)\nStart with a quadratic of interest and divide through by the coefficient on on \\(x^2\\) yielding \\(x^2 - \\frac{bx}{a} = -\\frac{c}{a}\\) after rearranging.\nDivide the coefficient on x by 2 and then square it. Add that value to both sides of the equation. \\(x^2 - \\frac{bx}{a} + (\\frac{b}{2a})^2 = \\frac{c}{a} + (\\frac{b}{2a})^2\\).\nFactor the left hand side into a \\((x \\pm z)^2\\) form and simplify the right hand side. \\((x + \\frac{b}{2a})^2 = \\frac{c}{a} + (\\frac{b}{2a})^2\\).\n\\((x + \\frac{b}{2a})^2 = \\frac{b^2 - 4ac}{4a^2}\\) where we’ve multiplied \\(\\frac{-c}{a}\\) by \\(4a^2\\) and eliminated an a.\n\\(x+\\frac{b}{2a} = \\pm \\frac{\\sqrt{b^2 - 4ac}}{2a}\\). Taking the square root of both sides.\nRearrange and combine terms \\(x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\)\nExercise 31\nSolve\n\\[\\begin{aligned}\n-\\delta &> \\frac{\\delta + 4}{7} \\\\\n-7\\delta &> \\delta + 4 \\\\\n-8\\delta &> 4 \\\\\n\\delta &< -\\frac{1}{2}\n\\end{aligned}\\]\nM+S Chapter 12: Vectors and Matrices\nThere are discussions of the essentials of Linear Algebra in the Lecture Notes and in the Linear Algebra computation section.\nExercises\nExercise 1\nLet \\(\\textbf{a} = \\begin{pmatrix} 10\\\\ 2\\\\ 5 \\\\ 2\\end{pmatrix}\\), \\(\\textbf{b} = \\begin{pmatrix} 4\\\\ 15\\\\ 6 \\\\ 8\\end{pmatrix}\\), \\(\\textbf{c} = (2,6,8), \\textbf{d} = (1,15,12), \\textbf{e} = (14, 17, 11, 10)^T, \\textbf{f} = (20, 4, 10, 4)^T\\). Calculate each of the following, indicating that it’s not possible if there is a calculation you cannot perform.\n\\(\\textbf{a} + \\textbf{b} = (14, 17, 11, 10)^T\\)\nThis is not possible because the vectors are not of equal length.\n\\(\\textbf{b} - \\textbf{e} = (-10, -2, -5, -2)^T\\)\n\\(15\\textbf{c} = (30, 90, 120)\\)\n\\(-3\\textbf{f} = (-60, -12, -30, -12)^T\\)\n\\(\\lVert b \\rVert = \\sqrt{\\sum_{i=1}^n b_i^2} = \\approx 18.46\\).\n\\(\\lVert c + d \\rVert= \\sqrt{\\sum_{i=1}^n (c_i + d_i)^2} = \\sqrt{3^2 + 21^2 + 20^2} = \\approx 29.2\\). The triangle inequality states \\(\\lVert c + d \\rVert \\leq \\lVert c \\rVert + \\lVert d \\rVert\\). Focusing on the right hand side, the first term is \\(\\sqrt{4+36 + 64}= \\sqrt{104} \\approx 10.2\\) and the second term is \\(\\sqrt{1 + 15^2 + 12^2} = \\sqrt{370} \\approx 19.2\\) or \\(29.4\\) showing the triangle inequality holds.\n\\(\\lVert c - d\\rVert = \\sqrt{\\sum_{i=1}^n (c_i - d_i)^2} = \\sqrt{1 + 9^2 + 4^2} \\approx 9.9\\)\n\\(a \\cdot b = a^Tb = 10(4) + 2(15) + 5(6) + 2(8) = 116\\)\n\\(c \\cdot d = c^Td = 2(1) + 6(15) + 8(12) = 188\\)\nExercise 2\nIdentify the following matrices as diagonal, identity, square, symmetric, triangular, or none of the above. Note all that apply.\n\n[1] \"The matrix A is\"\n     [,1] [,2] [,3]\n[1,]    0    1    5\n[2,]    1   -2   -1\n[3,]    5   -1    2\n[1] \"The matrix B is\"\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n[1] \"The matrix C is\"\n     [,1] [,2]\n[1,]    1    1\n[2,]    3   -2\n[1] \"The matrix D is\"\n     [,1] [,2] [,3]\n[1,]    0    1    2\n[2,]    5    1   -1\n[3,]    2    4    0\n[4,]    1    1    0\n\nSquare and symmetric matrix\nThe identity matrix is square, symmetric, triangular, and of course the identity.\nThis is just a square matrix.\nThe matrix identified is none of the above.\nExercise 3\n\n[1] \"The transpose of A is\"\n     [,1] [,2] [,3]\n[1,]    0    1    5\n[2,]    1   -2   -1\n[3,]    5   -1    2\n[1] \"The transpose of B is\"\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n[1] \"The transpose of C is\"\n     [,1] [,2]\n[1,]    1    3\n[2,]    1   -2\n[1] \"The transpose of D is\"\n     [,1] [,2] [,3] [,4]\n[1,]    0    5    2    1\n[2,]    1    1    4    1\n[3,]    2   -1    0    0\n\nExercise 4\nGiven the matrices in the problem, perform the following calculations.\n\n\nA = rbind(c(5,1,2), c(6,2,3))\nB = rbind(c(3,4,5), c(-2,-3,6))\nC = cbind(c(1,-5,-3), c(2,3,1))\nD = rbind(c(2,1),c(4,3))\nA\n\n     [,1] [,2] [,3]\n[1,]    5    1    2\n[2,]    6    2    3\n\nB\n\n     [,1] [,2] [,3]\n[1,]    3    4    5\n[2,]   -2   -3    6\n\nC\n\n     [,1] [,2]\n[1,]    1    2\n[2,]   -5    3\n[3,]   -3    1\n\nD\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    4    3\n\n\\(A + C\\) is not possible because there are not the same dimensions.\n\\(A-B\\)\n\n     [,1] [,2] [,3]\n[1,]    2   -3   -3\n[2,]    8    5   -3\n\n\\(A + 5B\\)\n\n     [,1] [,2] [,3]\n[1,]   20   21   27\n[2,]   -4  -13   33\n\n\\(3A\\)\n\n     [,1] [,2] [,3]\n[1,]   15    3    6\n[2,]   18    6    9\n\n\\(2B - 5A\\)\n\n     [,1] [,2] [,3]\n[1,]  -19    3    0\n[2,]  -34  -16   -3\n\n\\(B^T - C\\)\n\n     [,1] [,2]\n[1,]    2   -4\n[2,]    9   -6\n[3,]    8    5\n\n\\(BA\\) is not possible because the dimensions are not correct \\(2 \\times 3\\) and \\(2 \\times 3\\)\n\\(DA\\)\n\n     [,1] [,2] [,3]\n[1,]   16    4    7\n[2,]   38   10   17\n\n\\(AD\\) is not possible because the dimensions are not correct.\n\\(CD\\)\n\n     [,1] [,2]\n[1,]   10    7\n[2,]    2    4\n[3,]   -2    0\n\n\\(BC\\)\n\n     [,1] [,2]\n[1,]  -32   23\n[2,]   -5   -7\n\n\\(CB\\)\n\n     [,1] [,2] [,3]\n[1,]   -1   -2   17\n[2,]  -21  -29   -7\n[3,]  -11  -15   -9\n\nNote that \\(BC \\neq CB\\) This is common with matrix multiplication whenever both \\(BC\\) and \\(CB\\) exist.\nExercise 6\nFind the inverse of the following two matrices or explain why it does not exist.\n\n     [,1] [,2]\n[1,]    4    2\n[2,]    6    3\n     [,1] [,2]\n[1,]    1    4\n[2,]    3    2\n\n\\(A\\) does not have full rank because the first column is just the second column muliplied by 2. As a result, the matrix is not square (and singular) so there is no inverse.\n\\(B^{-1}\\) does have an inverse. The determinant is \\(ad - bc = 2 - 12 = -10\\). We swap the two diagonal elements and flip the signs on the off diagonal and then multiply by 1 over the determinant \\(\\frac{-1}{10}\\).\n\n     [,1] [,2]\n[1,] -0.2  0.4\n[2,]  0.3 -0.1\n\nExercise 9\nTrue or False?\n\\(BA = AB\\) for all matrices \\(A,B\\).\nFalse. We’ve already shown this to be the case in 4 so the matrices B and C from that problem serve as a counter example.\n\\(XX^-1 \\neq I\\). False. Plug in the identity matrix for X as a counter example. More generally, we can produce the identity matrix by either left or right multiplying any matrix \\(X\\) with the inverse matrix provided the inverse matrix exists.\n\\(M_{i \\times j}N_{j\\times k} = (MN)_{i \\times k}\\). True.\nExercise 10\nWhy are the following useful?\nThe determinant is useful because it determines whether a matrix is invertible. If the determinant is 0, then a matrix is not invertible.\nThe inverse is useful because we can use it to solve the system of equations represented by the matrix. An inverse indicates whether the system has a solution.\nExercise 11\nWhat does it mean if you have a singular matrix?\nIt means that the matrix is not invertible and has less thank full rank. For most applied political science research, it means that you have a perfect collinearity between two of your variables, and the error will show up when you try to run a regression via OLS.\nExercise 12\nFill in the blanks:\nAB indicates that you right-multiply A by B.\nAB indicates that you left-multiply B by A\nM+S Chapter 13: Vector Spaces and Systems of Equations\nExercises\nExercise 1\nLet \\(\\textbf{a} = \\begin{pmatrix} 10\\\\ 2\\\\ 5 \\\\ 2\\end{pmatrix}\\), \\(\\textbf{b} = \\begin{pmatrix} 4\\\\ 15\\\\ 6 \\\\ 8\\end{pmatrix}\\), \\(\\textbf{e} = (14, 17, 11, 10)^T, \\textbf{f} = (20, 4, 10, 4)^T\\). Calculate each of the following, indicating that it’s not possible if there is a calculation you cannot perform.\nWrite the most general vector that is a lineaer combination of \\(\\textbf{a}\\) and \\(\\textbf{b}\\).\n\\[\\begin{aligned}\ns\\textbf{a} + t\\textbf{b} &= \\begin{pmatrix} 10s + 4t \\\\ 2s + 15t \\\\ 5s + 6t \\\\ 2s + 8t \\end{pmatrix}\n\\end{aligned}\\]\nNo. At most there are 2 that linearly dependent. For example \\(\\textbf{e} = \\textbf{a} + \\textbf{b}\\), while \\(\\textbf{f} = 2\\textbf{a}\\)\nSince there are only two linearly independent vectors, the vectors span a two-dimensional space.\nExercise 2\nSolve the following systems of equations using substitution or elimination or both.\nExercise 4\nFrom the previous chapter, consider the following matrices.\n\n[1] \"D\"\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[1] \"E\"\n     [,1] [,2]\n[1,]    3    1\n[2,]    6    2\n[1] \"g\"\n     [,1]\n[1,]    2\n[2,]    3\n[3,]    1\n[1] \"h\"\n     [,1] [,2] [,3]\n[1,]    1    2    3\n\nRank of D is 2.\nRank of E is 1 because the first column is 3 times the second.\nSolve \\(D\\textbf{x} = \\textbf{g}\\) for \\(\\textbf{x}\\) using matrix inversion.\nAs stated, this is not possible because \\(\\textbf{g}\\) has dimension 3 and \\(D\\) is of rank 2.\nSolve \\(\\textbf{x}E = \\textbf{h}\\) for \\(\\textbf{x}\\) using matrix inversion.\nOnce again, E is singular which means it is not a full rank matrix so we cannot use matrix inversion. There is in fact an infinite number of solutions to the system of equations.\nExercise 6\nWhy is Cramer’s rule useful?\nCramer’s rule is useful for solving for just one of the variables in a system of equations without having to solve the entire system of equations. It is a method of solving systems of linear equations by dividing the value of two determinants. To find whichever variable we are interested in, just evaluate the determinant quotient $.\nFor example, consider the system of equations\n\\[\\begin{pmatrix}\n2x + y + z = 1 \\\\\nx - y + 4z = 0 \\\\\nx + 2y - 2z = 3\n\\end{pmatrix}\\]\nand imagine we only are interested in solving for \\(x\\). First, form find the value of the coefficient determinant.\n\\[D = \\begin{vmatrix}\n2 & 1 & 1 \\\\\n1 & -1 & 4 \\\\\n1 & 2 & 2\n\\end{vmatrix}\\]\nwhich here becomes \\(4 + 4 + 2 +1 -16 + 2 = -3\\)\nNext, form \\(D_z\\) by replacing the third column of values with the answer column.\n\\[D_z = \\begin{vmatrix}\n2 & 1 & 1 \\\\\n1 & -2 & 0 \\\\\n1 & 2 & 3\n\\end{vmatrix}\\]\nwhich comes out to \\(-6 + 0 + 2 +1 - 0 -3 = -6\\).\nNow just divide the two values \\(z = \\frac{D_z}{D} = \\frac{-6}{-3} = 2\\).\nNote that in a world in which \\(D = 0\\), we cannot use Cramer’s Rule. Incidentally, Cramer’s Rule is rarely used in computation because it is often unstable.\nExercise 7\nWhat does it mean if you have a singular matrix?\nSee the answer in Exercise 11 from Chapter 12\nCalculus Sequence\nThe Calculus sequence suggested timeline is 2-4 weeks. It is entirely composed of chapters from M+S and comprises mathematical preliminaries, functions, limits and continuity, sequences and series and sets, introduction to derivatives and integrals, and an introduction to extrema and multivariate calculus.\nM+S Chapter 3: Functions\nM+S Chapter 4: Limits and Continuity, Sequences and Series, and More on Sets\nM+S Chapter 5: Introduction to Derivatives\nM+S Chapter 6: Rules of Derivatives\nM+S Chapter 7: Integrals\nM+S Chapter 8: Extrema in One Dimension\nM+S Chapter 15: Multivariate Calculus\nRandom Variables and Probability Sequence\nThe Random Variables and Probability sequence suggested timeline is 1-2 weeks. It is entirely composed of chapters from M+S and FPP and comprises mathematical preliminaries, introduction to probability, and an introduction to discrete distributions and continuous distributions.\nM+S Chapter 1: Preliminaries\nThis material is already discussed in the Linear Algebra Sequence\nFPP Chapter 4,6\nExercises\nFPP Chapter 13, 14\nExercises\n13 Exercise Set A (2,4,5)\n13 Exercise Set B (1,4)\n13 Exercise Set C (1,4,7)\n13 Exercise Set D (1,2,8)\n14 Exercise Set A (1,4)\n14 Exercise Set B (2,3,6)\n14 Exercise Set C (1,3)\n14 Exercise Set D (3,5)\nM+S Chapter 9: Introduction to Probability\nM+S Chapter 10: Introduction to Discrete Distributions\nFPP Chapter 8, 9\nExercises\nFPP Chapter 8 Exercise Set A (1,5,6)\nFPP Chapter 8 Exercise Set B (1,2,6,9)\nFPP Chapter 8 Exercise Set C(1) and D(1)\nFPP Chapter 9 Exercise Set A (2,6,9)\nFPP Chapter 9 Exercise Set B(4), C(1,4), E(all)\nM+S Chapter 11: Continuous Distributions\nFPP Chapter 10,11,12,16,17\nExercises\nFPP Chapter 10 Exercise Set A (1,4,5), B (1,3), D(all), E(all)\nFPP Chapter 11 Exercise Set A (3,4,6), B(1), C(all), D(3,7), E(1)\nFPP Chapter 12 Exercise Set A (1,3,4), B (1,3)\nFPP Chapter 16 Exercise Set A (1,4,6,9) B (2,5,7), C(1)\nFPP Chapter 17 Exercise Set A (1), B (1,3,4), C(1,8), D(1), E(1,4,9)\nR Sequence\nThe R sequence suggested timeline is 2-4 weeks, though learning to and practicing coding will occupy much of one’s time in quantitative research. Much of the material contained in the R sequence prep is available on the Coding Notes page, which offers both R and Python coding notes and exercises.\n\n\n\n",
      "last_modified": "2023-05-07T18:15:57-07:00"
    },
    {
      "path": "lectures.html",
      "title": "Lecture Notes",
      "description": "Lecture Notes and Resources\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nSimple Linear Regression\nMultiple Linear Regression\nOLS Asympotics\n\nSimple Linear Regression\nWe are interested in studying models that take the following form:\n\\(y = \\beta_0 + \\beta_1x + u\\)\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope parameter and u is the error term. In the next set of notes, we will extend this model to situations where we have more than one covariate.\nWe can think of \\(\\beta_0 + \\beta_1x\\) as the systematic part of y whereas u is the unsystematic part of y. That is, u represents y not explained by x.\nError Term Assumptions\nIn order to make progress, we make the following assumptions about the error term.\n\\(E[u] = 0\\) as long as an intercept term is included in the equation. Note that this essentially defines the intercept.\n\\(E[u|x] = E[u] = 0\\). This is the Zero Conditional Mean Assumption for the error term.\nThe average value of the unobservables is the same across all slices of the population determined by the value of x and is equal to the average of u over the entire population\nBy EA.1 that means the average is 0\nDeriving OLS Estimates\nTo estimate the Population Regression Function (PRF), we need a sample.\nLet \\((x_i, y_i): i = 1,...,n\\) be a random sample of size n from the population. We can estimate the PRF by a model:\n\\(y = \\beta_0 + \\beta_1x_i + u_i\\) (E.2)\nError Assumption 2 implies that in the popuation x and u are uncorrelated, and the zero conditional mean assumption for the error implies that \\(E[u] = 0\\). This implies that the covariance between x and u is 0 or formally:\n\\(Cov(x,u) = E(xu) = 0\\)\nWe can rewrite previous equations as follows\n\\(E[u] = E[ y - \\beta_0 + \\beta_1x]\\) (E.3)\n\\(Cov(x,u) = E[x(y - \\beta_0 + \\beta_1x)]\\) (E.4)\nOur goal is to choose sample \\(\\hat{\\beta_0}\\),\\(\\hat{\\beta_1}\\) to solve the sample equations:\n\\(\\frac{1}{n}\\sum_{i=1}^n y - \\hat{\\beta_0} + \\hat{\\beta_1}x = 0\\) (E.5)\n\\(\\frac{1}{n}\\sum_{i=1}^n x_i(y - \\hat{\\beta_0} + \\hat{\\beta_1}x) = 0\\) (E.6)\nRewrite E.4\n\\(\\bar{y} = \\hat{\\beta_0} + \\hat{\\beta_1}\\bar{x}\\) which implies\n\\(\\beta_0 = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\nEstimating The Slope Parameter\nDrop the \\(\\frac{1}{n}\\) in E.5 because it does not affect the solution. Plug in \\(\\bar{y} - \\hat{\\beta_1}\\bar{x}\\) for \\(\\beta_0\\) which yields the equation\n\\(\\sum_{i=1}^n x_i(\\bar{y} - \\hat{\\beta_1}\\bar{x}) - \\hat{\\beta_1}x) = 0\\)\nRearrange terms to get the y’s and the x’s on opposite sides of the equation.\n\\(\\sum_{i=1}^n x_i(y_i - \\bar{y})\\)\n\\(\\hat{\\beta_1}\\sum x_i(x_i - \\bar{x})\\)\nSetting these equal to each other and using properties of the sum operator, we can rewrite the the top sum to be \\(Cov(x,y)\\) and the bottom sum to \\(V(x)\\). As long as \\(V(x) > 0\\),\n\\(\\hat{\\beta_1} = \\frac{\\hat{Cov(x,y)}}{\\hat{V(x)}}\\)\nIn words, the slope parameter estimate is the sample covariance of x and y divided by the sample variance of x. We refer to this as the OLS procedure and the OLS regression line as\n\\(\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x\\)\nAlgebraic Properties of OLS on Any Sample of Data\nThe following hold by construction for any sample of data estimated by OLS\nThe sum and therefore sample average of the residuals is 0. This is because the OLS estimates are chosen to make the residuals sum to 0.\nSample covariance between regressors and OLS residuals is 0\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the OLS regression line\nVariation in Y\nWe can view OLS as decomposing each \\(y_i\\) into two parts, a fitted value and a residual. There are three parts of this decomposition: the total sum of squares (SST), the explained sum of squares (SSE), and the residual sum of squares (SSR).\n\\(SST = \\sum_{i=1}^n (y_i -\\bar{y})^2\\)\n\\(SSE = \\sum_{i=1}^n (\\hat{y_i} -\\bar{y})^2\\)\n\\(SSR = \\sum_{i=1}^n \\hat{u}^2\\)\nSST is a measure of total sample variation in the \\(y_i\\)’s. Dividing SST by n-1 gets us the sample variance of y.\nThe Total Variation in y is SST = SSE + SSR.\nTo derive\n\\(\\sum_{i=1}^n (y_i -\\bar{y})^2\\)\n\\(\\sum_{i=1}^n [(y_i - \\hat{y_i}) + (\\hat{y_i}-\\bar{y})]^2\\)\n\\(\\sum_{i=1}^n \\hat{u_i} + (\\hat{y_i}-\\bar{y})]^2\\)\nExpand out the sum and replace with definitions to get\n\\(SSR + 2Cov(\\hat{u}, \\hat{y}) + SSE\\)\nSince the covariance between u and y is 0, that term drops out.\nGoodness of Fit\nThe ratio of the explained sample variation in y by x is known as \\(R^2\\) and defined:\n\\(R^2 = 1 - \\frac{SSR}{SST}\\)\nExpected Values and Unbiasedness of OLS Estimators\nOLS is an unbiased estimator of the population model provided the following assumptions hold. These assumptions are also known as Gauss-Markov assumptions.\nA1. Linear in paramters\nIn the population model, y is related to x and u\n$y = _0 + _1x + u $\nA2. Random Sample\nWe have a random sample of size n from the population model\nA3. Sample variation in x\nThe sample outcomes \\(x_i: i = 1,2,..., n\\) are not all the same value. If they are, there is no variance of X and so \\(\\beta_1\\) cannot be estimated.\nA4. Zero Conditional Mean of the Error\nFor a random sample, this assumption implies\n\\(E(u_i|x_i) = 0: \\forall i \\in [0,1,...n]\\)\nA4 is violated whenever we think that u and x are correlated. In the simple bivariate case, an example might be using the variable education to predict salary. education is correlated with many variables, including income and family history. These may affect salary and therefore will give us biased results.\nNote: We can write the slope estimator \\(\\beta_1\\) in a slightly different way\n\\(\\hat{\\beta_1} = \\frac{\\sum\\_{i=1}^n (x_i - \\bar{x})*(\\beta_0 - \\beta_1x + u_i)}{SST_x}\\)\n\\(\\hat{\\beta_1} = \\beta\\_0 \\sum\\_{i=1}^n\\) + _1_{i=1}^n x_i(x_i -{x}) + _{i=1}^n u_i(x_i - {x})$\nThe first term sums to 0 and drops out. Thus:\n\\(\\hat{\\beta_1} = \\beta_1 + \\frac{\\sum\\_{i=1}^n u_i(x_i - \\bar{x})}{SST_x}\\)\nWe now have all the information we need to prove that OLS is unbiased. Unbiasedness is a feature of the sampling distributions of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). Unbiasedness says nothing about the estimates for any given sample we may draw.\nTheorem 1: Using A1-A4 OLS produces unbiased estimates\n\\(E(\\hat{\\beta_0}) = \\beta_0\\) and \\(E(\\hat{\\beta_1}) = \\beta_1\\)for any values of \\(\\beta_0\\) and \\(\\beta_1\\).\nProof:\nIn this proof the expected values are conditional on sample values of the independent variable x. Because \\(SST_x\\) and \\((x_i - \\bar(x))\\) are functions on of \\(x_i\\) they are non-random once we condition on x.\n\\(E\\hat{\\beta_1}] = E\\beta_1 + \\frac{\\sum\\_{i=1}^n u_i(x_i - \\bar{x})}{SST_x}]\\)\n\\(E\\hat{\\beta_1}] = \\beta_1 + \\frac{\\sum\\_{i=1}^n E[u_i(x_i - \\bar{x})]}{SST_x}\\)\n\\(E\\hat{\\beta_1}] = \\beta_1 + \\frac{\\sum\\_{i=1}^n 0 (x_i - \\bar{x})}{SST_x}\\)\n\\(E\\hat{\\beta_1}] = \\beta_1\\)\nWe can also prove the same for \\(\\beta_0\\).\n\\(E\\hat{\\beta_0}] = \\beta_0 + E[(\\beta_1 - \\hat{\\beta_1}\\bar{x} + E\\bar{u}]\\)\n\\(E\\hat{\\beta_0}] = \\beta_0 + E[(\\beta_1 - \\hat{\\beta_1}\\bar{x} + 0\\)\n\\(E\\hat{\\beta_0}] = \\beta_0\\)\nIn the last equation, because \\(\\hat{\\beta_1} = \\beta_1\\) the second term drops out.\nVariances of OLS Estimators\nAn additional assumption we can make about the variance of the OLS estimators is that the error u has the same variances conditional on any value of the explanatory variable.\n\\(V(u|x) = \\sigma^2\\)\nBy adding this assumption, which to be clear will break down horribly if it is violated, we can prove the following theorem.\nTheorem 2: Using assumptions 1-4 and homoskedastic error assumption\n\\(V(\\hat{\\beta_1}) = \\frac{\\sigma^2}{SST_x}\\) and \\(V(\\hat{\\beta_0}) = \\frac{\\sigma^2\\frac{1}{n}\\sum\\_{i=1}^n x_i^2}{\\sum\\_{i=1}^n(x_i - \\bar{x})^2}\\)\nwhere these are conditioned on the sample values.\nProof for \\(V(\\hat{\\beta_1})\\)\n\\(V(\\hat{\\beta_1}) = \\frac{1^2}{SST_x^2}V(\\sum\\_{i=1}^n u_i(x_i - \\bar{x}))\\)\nSubstitute \\(d_i = (x_i - \\bar{x})\\)\n\\(V(\\hat{\\beta_1}) = \\frac{1^2}{SST_x^2}\\sum\\_{i=1}^n u_i d_i^2\\)\nSince \\(V(u_i) = \\sigma^2 : \\forall i\\) we can substitute that constant into the equation.\n\\(V(\\hat{\\beta_1}) = \\frac{1}{SST_x^2}\\sigma^2 \\sum\\_{i=1}^n d_i^2\\)\nObserve that the second RHS term is just \\(SST_x\\) after pulling out the constant, we can rewrite as\n\\(V(\\hat{\\beta_1}) = \\frac{\\sigma^2 SST_x}{SST_x^2}\\)\nwhich reduces to our stated result.\nNow that we know the way to estimate the variance, we can ask the following question. How does \\(V(\\hat{\\beta_1})\\) depend on error variance?\nThe larger the error variance, the larger \\(V(\\hat{\\beta_1})\\).\nThe larger the \\(V(x)\\), the smaller \\(V(\\hat{\\beta_1})\\)\nAs sample size increases, the total variation in x increases which leads to a decrease in \\(V(\\hat{\\beta_1})\\)\nEstimating the Error Variance\nErrors are never observed. Instead, we observe residuals that we can compute from our sample data. We can write the errors as a function of the residuals.\n\\(\\hat{u_i} = u_i - (\\hat{\\beta_0} - \\beta_0) - (\\hat{\\beta_1} - \\beta_1)x\\)\nOne problem that we run into is that using the residuals as an estimator is biased without correction because it does not take into account two restrictions for OLS residuals. OLS residuals have to sum to 0 and have a 0 covariance between x and u. Formally,\n\\(\\sum\\_{i=1}^n \\hat{u_i} = 0\\)\nand\n\\(\\sum\\_{i=1}^n \\hat{u_i}x_i = 0\\).\nThus we need to correct by n-2 degrees of freedom for an unbiased estimator. When we do so, we get the following.\n\\(\\hat{\\sigma}^2 = \\hat{s}^2 = \\frac{1}{n-2}\\sum\\_{i=1}^n\\hat{u_i}^2\\)\n\\(\\hat{\\sigma}^2 = \\frac{SSR}{n-2}\\)\nMultiple Linear Regression\nWe are now going to extend our previous discussion of Simple Linear Regression into the multiple variate case. Here we consider models that include more than one independent variable.\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x\\_2 + ... + \\beta_kx\\_k + u\\)\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) measures change in y with respect to \\(x_1\\) holding all other covariates fixed and so on.\nAs shorthand we refer to parameters other than the intercept as slope parameters.\nWe can generalize the zero conditional mean assumption for the errors to be\n\\(E(U|x\\_1, x\\_2, ..., x\\_k) = 0\\)\nMechanics and Interpretation of OLS\nOur goal is to get estimates \\(\\beta_0, \\beta_1,...,\\beta_k\\). We choose these to minimize the sum of squared residuals\n\\(\\sum_{i=1}^n\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x\\_{i1}+...+ \\hat{\\beta_k}x\\_{ik}\\)\nThis minimization leads to k + 1 linear equations in k + 1 unknowns. We call these the OLS first order equations.\nOLS Fitted Values and Residuals\nFor observation i the fitted values are:\n\\(\\sum_{i=1}^n\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x\\_{i1}+...+ \\hat{\\beta_k}x\\_{ik}\\)\nRecall that OLS minimizes the average square prediction error which says nothing about any given prediction. The residual is generalized from the simple linear regression case to be:\n\\(\\hat{u_i} = y_i - \\hat{y_i}\\)\nProperties of fitted values and residuals\nThe sample average of the residuals is 0. \\(\\bar{y} = \\hat{\\bar{y}}\\)\nSample covariance between each independent variable and the residuals is 0 by construction\nThe average point is always on the regression line by construction.\nGoodness of Fit\nLike before SST = SSE + SSR and \\(R^2 = 1 - \\frac{SSR}{SST}\\)\nExpected Value of OLS Estimators\nWe restate the assumptions needed for OLS regressions\nLinear in Parameters\nThe model can be written as \\(y = \\beta_0 + \\beta_1x\\_1 + \\.\\.\\. + \\beta_kx\\_k\\)\nRandom sampling\nWe have a random sample of n observations \\({(x\\_{i1}, x\\_{i2}, ..., x\\_{ik}): i = 1,2,..., n}\\)\nNo perfect collinearity\nIn the sample (and therefore population) none of the independent variables is constant and there are no exact linear relationships among the independent variables\nZero Conditional Mean of Error\n\\(E(u|x_1, x_2, ..., x_k) = 0\\)\nThis implies that none of the independent variables are correlated with the error term\nIf these four assumptions hold\nTheorem 3.1: OLS is unbiased\n\\(E(\\hat{\\beta_j} = \\beta_j : j = 1,2,\\.\\.\\.,k)\\)\nfor any values of the population parameter \\(\\beta_j\\)\nNote: Adding irrelevant independent variables does not effect unbiasedness. These terms will on average be 0 across many samples. Adding irrelevant independent variables will hurt the estimator’s variances.\nOmitted Variable Bias\nA major problem that will lead to bias in our estimates is omitting a relevant variable from our model. To see why suppose we have the following population model\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + u\\)\nbut we only run the regression\n\\(y = \\hat{\\beta_0} + \\beta_1x\\)\nFor example, we want to estimate the effect of education on wages but do not include some measure of innate ability.\nSince the model is misspecified, we can define bias as:\n\\(\\tilde{\\beta_1} = \\hat{\\beta_1} + \\hat{\\beta_2}\\tilde{\\delta_1}\\)\nwhere \\(\\hat{\\beta_1}, \\hat{\\beta_2}\\) are slope estimator from the regression $y_i $ on \\(x_1, x_2\\) and \\(\\tilde{\\delta_1}\\) is the slope from the regression \\(x\\_{i2}\\) on \\(x\\_{i1}\\).\n\\(\\tilde{\\delta_1}\\) depends only on the independent variables in the sample so we can treat it as a fixed quanitty when computing \\(E[\\tilde{\\beta_1}]\\)\n\\(E[\\tilde{\\beta_1}] = E[\\hat{\\beta_1} + \\hat{\\beta_2}\\tilde{\\delta_1}]\\)\n\\(E[\\tilde{\\beta_1}] = E[\\hat{\\beta_1} + \\tilde\\delta_1\\hat{\\beta_2}]\\)\n\\(E[\\tilde{\\beta_1}] = \\beta_1 + \\tilde\\delta_1\\beta_2\\)\nwhich implies that the omitted variable bias is \\(\\beta_2\\tilde{\\delta_1}\\)\nThe bias in the model is 0 if:\n\\(\\beta_2 = 0\\)\n\\(\\tilde{\\delta_1}=0\\)\nwhich occurs if and only if \\(x_1\\) and \\(x_2\\) are uncorrelated in the sample.\nVariance of OLS Estimators\nKeeping our previous Multiple Linear Regression assumptions we add\nAssumption 5: Homoskedasticity of errors\nThe error u has the same variance given any values of the explanatory variables.\n\\(V(u| x_1, x_2, ..., x_k)=\\sigma^2\\)\nTheorem 2\nUnder assumptions 1-5 conditional on the sample values of the independent variables\n\\(v(\\hat{\\beta_j}) = \\frac{\\sigma^2}{{SST}_j(1-R_j^2)}\\)\nand\n\\(E[\\hat{\\sigma%^2}= \\sigma^2]\\)\nThe unbiased estimator of \\(\\sigma^2\\) is \\(\\hat{\\sigma^2}= \\frac{1}{n-k-1}\\sum_{i=1}^n \\hat{u_i}^2\\) where n is the number of observations and k + 1 is the number of parameters.\nThe standard error of \\(\\hat{\\beta_j}\\) is\n\\(se(\\hat{\\beta_j})= \\frac{\\hat{\\sigma}}{\\sqrt{{SST}_j(1-\\hat{R_j}^2)}}\\)\nEfficiency Properties of OLS\nThe OLS estimator \\(\\hat{\\beta_j}\\) for \\(\\beta_j\\) is BLUE: The Best Linear Unbiased Estimator.\nestimator: rule that can be applied to data to generate an estimate\nunbiased \\(E[\\hat{\\beta_j}]=\\beta_j\\) for any estimator\nlinear: An estimator is linear if it can be expressed as a linear function of the data on the dependent variable\nbest: Gauss-Markov holds that for any estimator \\(\\tilde{\\beta_j}\\) that is linear and unbiased \\(V(\\hat{\\beta_j}) \\leq V(\\tilde{\\beta_j})\\). That is the OLS estimator has at least as small if not smaller variance than any other linear unbiased estimator.\nFrom our previous assumptions we add an additional assumption.\nAssumption 6: Normality\nThe population error u is independent of the explanatory variables \\(x_1, x_2, ..., x_k\\) and is normally distributed as \\(u \\sim N(0, \\sigma^2)\\)\nAssumption 6 is strong and amounts to also assuming MLR Assumption 4 and MLR Assumption 5. Taken together, the six assumptions we’ve made so far collectively are the classical linear model (CLM) assumptions.\nWe can summarise the CLM as:\n\\(y|\\textbf{x} \\sim N(\\beta_0 + \\beta_1x\\_{ik} + ... + \\beta_kx\\_{ik}, \\sigma^2)\\)\nwhere \\(\\textbf{x}\\) is shorthand for \\(x_1, x_2, ..., x_k\\)\nNote: Problems with Normal Error Assumption\nFactors in u can have very different distributions in the population. While central limit theorems can still hold, the normal approximation can be poor.\nCentral limit theorem arguments assume all error affects y in separate additive fashion which has no guarantee of truth. Any breakdown in this assumption will break this assumption\nIn any given application, normal error assumptions are an empirical matter. In general the assumption will be false if y takes on just a few possible values.\nTheorem 4.1: Normality of error terms lead to normality of sampling distributions of OLS estimators\nUnder the CLM assumptions (MLR Assumptions 1-6) conditional on the sample values of the independent variables\n\\(\\hat{\\beta_j} \\sim N(\\beta_j, V(\\beta_j))\\) which implies\n\\(\\frac{\\hat{\\beta_j} - \\beta_j}{V(\\hat{\\beta_j})} \\sim N(0,1)\\)\nTesting hypotheses about a single population parameter: The t-test\nTheorem 4.2: t distribution for standard errors\nUnder the CLM assumptions (MLR Assumptions 1-6)\n\\(\\frac{\\hat{\\beta_j} - \\beta_j}{se(\\hat{\\beta_j})} \\sim t\\_{n-k-1} = t\\_{df}\\)\nwhere k+1 is the number of paramters in the population model and n-k-1 is the degrees of freedom.\nTheorem 2 is important because it allows us to test hypotheses involving \\(\\beta_j\\). The test statistic we use is the t-statistic.\n\\(t_{\\hat{\\beta_j}} = \\frac{\\hat{\\beta_j}}{se(\\hat{\\beta_j})}\\)\nNull Hypothesis Tests\nThe most common hypothesis test is a two-sided alternative\n\\(H_0: \\beta_j = 0\\)\n\\(H_a: \\beta_j \\neq 0\\)\nWe can also test against one sided alternatives where we expect\n\\(H_0: \\beta_j \\leq 0\\)\n\\(H_a: \\beta_j > 0\\)\nor the reverse.\nA p-value is the probability of observing a test statistic, in this case a t-statistic, as extreme as the one we observed given that the null hypothesis is true.\nTesting Hypotheses about a single linear combination of Parameters\nOur t-statistic keeps the same general format but changes slightly.\n\\(t = \\frac{\\hat{\\beta_j} - \\hat{\\beta_k}}{se(\\hat{\\beta_j} - \\hat{\\beta_k})}\\)\nwhere\n\\(se(\\hat{\\beta_j} - \\hat{\\beta_k}) = \\sqrt{se(\\hat{\\beta_j})^2 + se(\\hat{\\beta_k})^2 + 2Cov(\\hat{\\beta_j},\\hat{\\beta_k})}\\)\nSome guidelines for discussing signficance of a variable in a MLR model\nCheck for statistical significance. If yes, discuss the magnitude of the coefficient to get an idea of its practical importance.\nIf variable isn’t significant, look at whether the variable has the expected effect and if it is practically large.\nConfidence Intervals\nA confidence interval is if random samples were obtained infinitely many times with \\(\\tilde{\\beta_j}, \\hat{\\beta_j}\\) computed each time then the (unknown) \\(\\beta_j\\) population value would lie in the interval \\((\\tilde{\\beta_j}, \\hat{\\beta_j})\\) for 95% of the samples.\nUnder the CLM assumptions, a confidence interval is\n\\(\\hat{\\beta_j} \\pm \\alpha SE(\\hat{\\beta_j})\\)\nwhere \\(\\alpha\\) is a critical value\nTesting Multiple Linear Restrictions: The F-test\nOften we want to test whether a group of variables have no effect on the dependenet variables.\nThe null hypothesis is that a set of variables has no effect on y, once another set of variables have been controlled.\nIn contest to hypothesis testing:\nThe restricted model is the model without hte groups of variables we are testing\nThe unrestricted model is the model of all the parameters\nFor the general case:\nThe unrestricted model with k independent variables \\(y = \\beta_0 + \\beta_1x_1 +...+ \\beta_k x_k + u\\)\nThe null hypothesis is \\(H_0: \\beta\\_{k-q-1} = 0,..., \\beta_k = 0\\) which puts q exclusion restrictions on the model\nThe F-statistic is:\n\\(F = \\frac{\\frac{SSR_r - SSR\\_{ur}}{q}}{\\frac{SSR\\_{ur}}{n-k-1}}\\)\nwhere \\(SSR_r\\) is the sum of squared residuals from the restricted model and \\(SSR\\_{ur}\\) is the sum of squared residuals from the unrestricted model and q is the numerator degrees of freedom which is the degrees of freedom in the restricted model minus the degrees of freedom in the unrestricted model.\nThe F-statistic is always non-negative. If \\(H_0\\) is rejected then we say that \\(x\\_{k-q+1},...x_k\\) are jointly statistically significant. If we fail to reject then the variables are jointly statistically insignificant.\nThe R-Squared Form of the F-statistic\nUsing the fact that \\(SSR_r = SST(1-R_r^2)\\) and \\(SSR\\_{ur} = SST(1 - R\\_{ur}^2)\\) we can substitute in to the F-statistic to get\n\\(F = \\frac{\\frac{R\\_{ur}^2 - R_r^2}{q}}{\\frac{1-R\\_{ur}^2}{n-k-1}}\\)\nThis is often more convenient for testing exclusion restrictions in models but cannot test all linear restrictions.\nOLS Asympotics\nWe know under certain assumptions that OLS estimators are unbiased, but unbiasedness cannot always be achieved for an estimator. Another property that we are interested in is whether an estimator is consistent.\nTheorem 5.1: OLS is a consistent estimator\nUnder MLR Assumptions 1-4, the OLS estimator \\(\\hat{\\beta_j}\\) is consistent for \\(\\beta_j \\forall \\\\ j \\in 1,2,...,k\\).\nInformally, as n tends to infinitythe distribution of \\(\\hat{\\beta_j}\\) collapses to the single point \\(\\beta_j\\)\nWe can add an assumption MLR 4’:\n\\(E(u) = 0,Cov(x_j, u) = 0 \\forall j \\in 1,2,...,k\\)\nThis is a weaker assumption than MLR 4. MLR 4’ requires only that \\(x_j\\) is uncorrelated with u and that u has zero mean in the population. Indeed MLR 4 implies MLR 4’.\nWe use MLR4 as an assumption because OLS is biased but consistent under MLR 4’ if \\(E[u| x_1, ..., x_k]\\) depends on any of the \\(x_j\\). Second, if MLR 4 holds, then we have properly modeled the population regression function.\nDeriving Inconsistency of OLS\nCorrelation between u and any of the \\(x_1, ..., x_k]\\) generally causes all of the OLS estimators to be inconsistent. If the error is correlated with any of the independent variables then OLS is biased and inconsistent.\nThere is an asymptotic analogue to Omitted Variable Bias. Suppose the model \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + v\\) satisfies MLR assumptions 1-4. If we omit \\(x_2\\) then:\n\\(plim\\tilde{\\beta_1} = \\beta_1 + \\beta_2\\delta_1\\)\n\\(plim\\tilde{\\beta_1} = \\beta_1 + \\beta_2\\frac{Cov(x_1,x_2)}{V(x_1)}\\)\nIf the covariance term is zero then the estimator is still consistent. Otherwise, the inconsistency takes on the same sign as the covariance term.\nAsymptotic Normality and Large Sample Inference\nIn cases where the \\(y_i\\) do not follow normal distributions we can still get asymptotic normality.\nTheorem 5.2: Asymptotic Normality\nUnder MLR Assumptions 1-5\n\\(\\sqrt{n}(\\hat{\\beta_j} - \\beta_j) \\xrightarrow{a} N(0, \\frac{\\sigma^2}{a_j^2}\\) where \\(a_j^2\\) is the asymptotic variance of \\(\\sqrt{n}(\\hat{\\beta_j} - \\beta_j)\\). For the slope coefficients \\(a\\_j^2 = plim(\\frac{1}{n} \\sum\\_{i=1}^n \\hat{r\\_{ij}^2})\\) where the \\(r\\_{ij}\\) are the residuals from regressing \\(x_j\\) on the other independent variables.\n\\(\\hat{\\sigma^2}\\) is a consistent estimator of \\(\\sigma^2\\)\nFor each j\n\\(\\frac{\\hat{\\beta_j} - \\beta_j}{sd(\\hat{\\beta_j})} \\xrightarrow{a} N(0,1)\\) which we cannot compute from data and\n\\(\\frac{\\hat{\\beta_j} - \\beta_j}{se(\\hat{\\beta_j})} \\xrightarrow{a} N(0,1)\\) which we can compute from data.\nThis theorem does not require MLR 6 from the list of required assupmtions. What this theorem says is that regardless of the population distribution of u, the OLS estimators when properly standardized have approximate standard normal distributions.\nFurther,\n\\(\\frac{\\hat{\\beta_j} - \\beta_j}{se(\\hat{\\beta_j})} \\xrightarrow{a} t\\_{df}\\)\nbecause \\(t\\_{df}\\) approaches $ N(0,1)$ as the degrees of freedom gets large so we can carry out t-tests and confidence intervals in the same way as the CLM assumptions.\nRecall that \\(\\widehat{V(\\hat{\\beta_j})} = \\frac{\\hat{\\sigma^2}}{SST_j(1-R_j^2)}\\) where \\(SST_j\\) is the total sum of squares of \\(x_j\\) in the sample and \\(R_j^2\\) is the R-squared from regressing \\(x_j\\) on all other independent variables. As the sample size increases:\n\\(\\hat{\\sigma^2} \\xrightarrow{d} \\sigma^2\\)\n\\(R_j^2 \\xrightarrow{d} c\\) which is some number between 0 and 1\nThe sample variance \\(\\frac{SST_j}{n} \\xrightarrow{d} V(x_j)\\)\nThese imply that \\(\\widehat{V(\\hat{\\beta_j})}\\) shrinks to 0 at the rate of \\(\\frac{1}{n}\\) and \\(se(\\hat{\\beta_j}) = \\frac{c_j}{\\sqrt{n}}\\) where \\(c_j = \\frac{\\sigma}{\\sigma\\sqrt{1 - \\rho_j^2}}\\).\nThis last equation is an approximation. A good rule of thumb is that standard eerrors can be expected to shrink at a rate that is the inverse of the square root of the sample size.\nAsymptotic Efficiency of OLS\nTheorem 5.3\nUnder Gauss-Markov assumptions, let \\(\\tilde{\\beta_j}\\) denote estimators that solve the equation\n\\(\\sum\\_{i=1}^n g_j(\\textbf{x}_i)(y_i - \\tilde{\\beta_0}-\\tilde{\\beta_1}x\\_{i1} - ... - \\tilde{\\beta_k}x\\_{ik}) = 0\\)\nLet \\(\\hat{\\beta_j}\\) denote the OLS estimators. The OLS estimators have the smallest asymptotic variance.\n\\(AVar(\\sqrt{n}(\\hat{\\beta_j} - \\beta_j)) \\leq AVar(\\sqrt{n}(\\tilde{\\beta_j} - \\beta_j))\\)\n\n\n\n",
      "last_modified": "2023-05-07T18:15:58-07:00"
    },
    {
      "path": "matrix.html",
      "title": "Linear Algebra",
      "description": "A Crash Course in Linear Algebra using R and Python\n",
      "author": [
        {
          "name": "Alex Stephenson",
          "url": {}
        }
      ],
      "contents": "\n\nContents\nVectors\nBlock and stacked vectors\nSome special vectors\n\nVector Addition and Multiplication\nScalar Multiplication and division\nUsing what we’ve learned to confirm the distributive property\nInner Product\n\nMatrices\nAdditional Definitions\nMatrix Arithmetic\n\nNorms\nOrdinary Least Squares\nCovariance Matrices and Standard Errors\nClassical Standard Errors\nSandwich Standard Errors\n\nPrincipal Components Analysis\n\nThis page provides code in R and Python for doing linear algebra. In Python, we make use of the NumPy library.\n\nimport numpy as np \n\nVectors\nThe simplest way to represent vectors in R is by using the vector data structure.\n\n\nx = c(-1.1, 0.0, 3.6, -7.2)\nlength(x) ## 4 \n\n[1] 4\n\nIn python:\n\nx = np.array([-1.1, 0.0, 3.6, -7.2])\nlen(x)\n4\n\nBlock and stacked vectors\nIn addition to creating vectors, we can concatenate vectors together to produce blocked and stacked vectors using the c() function.\n\n\nx = c(1,-2)\ny = c(1,1,0)\nz = c(x,y)\nz\n\n[1]  1 -2  1  1  0\n\nIn python:\n\nx = np.array([1,-2])\ny = np.array([1,1,0])\nz = np.concatenate((x,y))\nprint(z)\n[ 1 -2  1  1  0]\n\nSome special vectors\nThe Zeros vector is a default behavior of creating a vector with a given length.\n\n\nz = numeric(3)\nz\n\n[1] 0 0 0\n\nIn python:\n\nz = np.zeros(3)\nz\narray([0., 0., 0.])\n\nThe Ones vector can be made by way of the rep() function.\n\n\no = rep(1,3)\no\n\n[1] 1 1 1\n\nIn python:\n\no = np.ones(3)\no\narray([1., 1., 1.])\n\nVector Addition and Multiplication\nIf x and y are vectors of the same size, then x+y and x-y give their element wise sum and difference respectively. R by default computes most vector operations element wise.\n\n\nx = c(1,2,3)\ny = c(100,200,300)\nx+y\n\n[1] 101 202 303\n\nIn python:\n\nx = np.array([1,2,3])\ny = np.array([100,200,300])\nx + y\narray([101, 202, 303])\nnp.add(x,y)\narray([101, 202, 303])\n\nScalar Multiplication and division\nIf a is a number and x is a vector, then we can express the scalar vector product as either a*x or x*a\n\n\na = 2 \nx = c(1,2,3)\na*x\n\n[1] 2 4 6\n\nx*a\n\n[1] 2 4 6\n\nIn python:\n\na = 2\nx = np.array([1,2,3])\na*x\narray([2, 4, 6])\nx*a\narray([2, 4, 6])\n\nUsing what we’ve learned to confirm the distributive property\nThe distributive property \\(\\beta(a+b) = \\beta a + \\beta b\\) holds for any two n-vector a and b and any scalar \\(\\beta\\).\n\n\na = c(3,5,6)\nb = c(2,4,9)\nbeta = 5\nlhs = beta*(a+b)\nrhs = beta*a + beta*b\nprint(lhs)\n\n[1] 25 45 75\n\nprint(rhs)\n\n[1] 25 45 75\n\nlhs == rhs\n\n[1] TRUE TRUE TRUE\n\nIn python:\n\na = np.array([3,5,6])\nb = np.array([2,4,9])\nbeta = 5 \nlhs = beta*(a+b)\nrhs = beta*a + beta*b\nprint('lhs:', lhs)\nlhs: [25 45 75]\nprint('rhs:', rhs)\nrhs: [25 45 75]\nlhs == rhs\narray([ True,  True,  True])\n\nInner Product\nThe inner product of n-vector x and y is denoted \\(x^Ty\\)\n\n\nx = c(1,2,3,4)\ny = c(3,4,6,7)\n\n## t() is the transpose function in R\nt(x)%*% y\n\n     [,1]\n[1,]   57\n\nIn python:\n\nx = np.array([1,2,3,4])\ny = np.array([3,4,6,7])\nnp.inner(x,y) \n\n# Alternatively \n57\nx @ y\n57\n\nMatrices\nA matrix \\(\\textbf{X}\\) is an \\(m\\) x \\(n\\) data structure that is a rectangular array of scalar numbers. The numbers \\(x_{ij}\\) are components or elements of \\(\\textbf{X}\\). The transpose of a matrix is the \\(n\\) x \\(m\\) matrix \\(\\textbf{X}'\\)\n\n\n## Creating a matrix in R \nX = matrix(seq(1,16,1), \n           nrow = 4, \n           byrow = T)\nX\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\nWe can also create matrices from vectors or from data frames\n\n\n## Equivalent to above but with vectors \nX2 = rbind(1:4, 5:8,9:12,13:16)\nX2\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\n\n\n## via a data frame \ndf = data.frame(\n  x = 1:4,\n  y = 5:8,\n  z = 9:12,\n  w = 13:16\n)\nX3 = as.matrix(df)\nX3\n\n     x y  z  w\n[1,] 1 5  9 13\n[2,] 2 6 10 14\n[3,] 3 7 11 15\n[4,] 4 8 12 16\n\nIn python:\n\nX = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12], [13,14,15,16]])\nX\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12],\n       [13, 14, 15, 16]])\nX.shape\n(4, 4)\n\nSome other useful matrices\n\nnp.identity(4) \narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\nnp.zeros((4,4))\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\nnp.ones((4,4))\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\nAdditional Definitions\nThe transpose in R\n\n\nX_transpose = t(X)\nX_transpose\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nIn python:\n\nX_transpose = X.T\nX_transpose\narray([[ 1,  5,  9, 13],\n       [ 2,  6, 10, 14],\n       [ 3,  7, 11, 15],\n       [ 4,  8, 12, 16]])\n\nA diagonal matrix with all elements not on the diagonal equal to zero is a diagonal matrix. By default, R creates an identity matrix with the diag() function.\n\n\ndM = diag(4)\ndM\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n\nThe trace of a matrix is the sum of the diagonal elements\n\\[trace(X) = \\sum_{i=1}^n x_{ii}\\]\n\n\nmatrix_trace = function(mat){\n  return(sum(diag(mat)))\n}\n\nmatrix_trace(X)\n\n[1] 34\n\nMatrix Arithmetic\nAddition and subtraction of matrices of the same order are performed element by element. Scalar multiplication is element by element\n\n\nA = matrix(data = seq(1,9,1), nrow = 3, byrow = T)\nB = matrix(data = seq(1,9,1), nrow = 3, byrow = T)\nA+B\n\n     [,1] [,2] [,3]\n[1,]    2    4    6\n[2,]    8   10   12\n[3,]   14   16   18\n\nProvided that the number of columns of A equals the number of rows of B, we can multiple A by B.\n\n\n## To get the appropriate multiplication, we wrap * in %*%\nA%*%B\n\n     [,1] [,2] [,3]\n[1,]   30   36   42\n[2,]   66   81   96\n[3,]  102  126  150\n\nNote that we can perform X’X in one of two ways.\n\n\nt(A)%*%A \n\n     [,1] [,2] [,3]\n[1,]   66   78   90\n[2,]   78   93  108\n[3,]   90  108  126\n\n## same but can be slightly faster \ncrossprod(A)\n\n     [,1] [,2] [,3]\n[1,]   66   78   90\n[2,]   78   93  108\n[3,]   90  108  126\n\nIn python:\n\nA = np.array([[1,2,3], [4,5,6], [7,8,9]])\nB = np.array([[1,2,3], [4,5,6], [7,8,9]])\nA+B\narray([[ 2,  4,  6],\n       [ 8, 10, 12],\n       [14, 16, 18]])\nnp.add(A,B)\n\n# Multiplication \narray([[ 2,  4,  6],\n       [ 8, 10, 12],\n       [14, 16, 18]])\nnp.matmul(A,B)\narray([[ 30,  36,  42],\n       [ 66,  81,  96],\n       [102, 126, 150]])\nnp.matmul(A.T, A)\narray([[ 66,  78,  90],\n       [ 78,  93, 108],\n       [ 90, 108, 126]])\n\nNorms\nInformally, the norm of a vector tells us how big it is. Formally, a norm is a function \\(\\lVert \\cdot \\rVert\\) that maps a vector to a scalar which satisfies:\nGiven any vector x \\(\\lVert \\alpha x \\rVert = \\lVert \\alpha \\rVert \\lvert x \\rVert\\).\nFor any vectors x, y, norms satisfy the triangle inequality \\(\\lVert x + y \\rVert \\leq \\lVert x \\rVert + \\lVert y \\rVert\\)\nThe norm of a vector \\(\\lVert x \\rVert > 0,\\forall x \\neq 0\\)\nThe most common norm is the \\(\\ell_2\\) norm or Euclidean norm which is defined as \\(\\lVert x \\rVert_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\\)\n\n\nx = c(3,-4)\nl2 = sqrt(crossprod(x,x))\nl2\n\n     [,1]\n[1,]    5\n\n## We can also call R's built in norm() function \nnorm(x, \"2\")\n\n[1] 5\n\nIn python\n\nx = np.array([3,-4])\nnp.linalg.norm(x)\n5.0\n\nWe sometimes take the Manhattan norm (\\(\\ell_1\\)) which sums the absolute values of a vectors elements \\(\\lVert x \\rVert_1 = \\sum_{i=1}^n|x_i|\\)\n\n\nx = c(3,-4)\nl1 = function(x){\n  return(sum(abs(x)))\n}\nl1(x)\n\n[1] 7\n\nIn python\n\nx = np.array([3,-4])\nnp.linalg.norm(x, 1)\n7.0\n\nThe infinity norm or max norm (\\(\\ell_\\infty\\)) is common in machine learning applications defined as \\(\\lVert x \\rVert_\\infty = \\max_i |x_i|\\), which simplifies to the absolute value of the element with the largest magnitude in the vector.\n\n\nx = matrix(c(3,-4), byrow = T, nrow = 2)\nnorm(x, type =\"I\")\n\n[1] 4\n\nl_inf = function(x){\n  return(max(abs(x)))\n}\nl_inf(c(3,-4))\n\n[1] 4\n\nIn python\n\nx = np.array([3,-4])\nmax(abs(x))\n4\n\nOrdinary Least Squares\nThe simplest linear model expresses the dependence of a dependent or response variable y on independent variables \\(x_1,.., x_p\\) and is usually written \\(y = X\\beta + \\epsilon\\). See the Lecture Notes for more details on the properties of this model.\nDefine the design matrix as the \\(n \\times p\\) matrix of independent variables \\(x_1,..,x_p\\) and assume that the first columns is a column of ones and that the design matrix has full rank. Then the usual OLS estimator is defined as \\((X'X)^{-1}X'Y\\)\n\n\nbeta_estimator = function(X,y){\n  X = cbind(rep(1,nrow(X)), X)\n  betas = solve(t(X)%*%X)%*%t(X)%*%y\n  return(betas)\n}\n\n## example data \nset.seed(123)\nx1 = rnorm(10000)\nx2 = rnorm(10000)\ny = 2*x1 + 4*x2 + runif(10000)\n\nX = cbind(x1, x2)\nbeta_estimator(X,y)\n\n        [,1]\n   0.4997833\nx1 1.9971656\nx2 4.0020879\n\nIn python:\n\nx1 = np.random.default_rng(seed=123).normal(0, 1, size =1000)\nx2 = np.random.default_rng().normal(0, 1, size =1000)\nones = np.ones(1000)\ny = 2*x1 + 4*x2 + np.random.default_rng().uniform(size = 1000)\n\nX = np.concatenate((ones, x1, x2)).reshape((-1,3), order = 'F')\n\n# Alternatively we can make a matrix with column_stack()\nX = np.column_stack((ones, x1, x2))\n\n# (X'X)^-1X'y\nnp.linalg.inv(X.T @ X) @ X.T @ y\narray([0.48985085, 2.011836  , 3.99624952])\n\nCovariance Matrices and Standard Errors\nClassical Standard Errors\nThe least squares solution gives us point estimates for coefficients, but if we want to do inference, we need to get standard errors. See the Lecture Notes for more details.\nTo get standard errors, we must first calculate the covariance matrix of our estimates and then take the square root of the diagonal.\n\n\nbeta_estimator = function(X,y){\n  X = cbind(rep(1,nrow(X)), X)\n  betas = solve(t(X)%*%X)%*%t(X)%*%y\n  return(betas)\n}\n\nbetas_and_std_errors = function(X,y){\n  betas = beta_estimator(X,y)\n  ## get the design matrix again\n  X = cbind(rep(1,nrow(X)), X)\n  residuals = y - X %*% betas \n  \n  ## Degree of freedom calculation \n  p = ncol(X) - 1 \n  df = nrow(X) - p - 1 \n  \n  ## Residual variance \n  res_var = sum(residuals^2) / df \n  \n  ## Covariance matrix of estimate \n  ## cov(\\hat{\\beta}|X) = (X'X)^-1X'cov(\\epsilon|X)X(X'X)^-1\n  beta_cov = res_var * solve(t(X)%*%X)\n  \n  ## Standard errors are square root of diagonal \n  return(list(beta = betas, se = sqrt(diag(beta_cov))))\n}\n\n## example data \n## To keep consistent with python examples I use pre-generated \n## random variables created in python with:\n# x1 = np.random.default_rng(seed=123).normal(0, 1, size =1000)\n# x2 = np.random.default_rng().normal(0, 1, size =1000)\n# ones = np.ones(1000)\n# y = 2*x1 + 4*x2 + np.random.default_rng().uniform(size = 1000)\n\nx1 = read.csv(\"x1.csv\", header = F) |>\n  unlist()\nx2 = read.csv(\"x2.csv\", header = F) |>\n  unlist()\ny = read.csv(\"y.csv\", header = F) |>\n  unlist()\nX = cbind(x1, x2)\n\nbetas_and_std_errors(X,y)\n\n$beta\n       [,1]\n   0.484434\nx1 2.005286\nx2 3.994715\n\n$se\n                     x1          x2 \n0.009244982 0.009198621 0.009201766 \n\nIn python\n\ndef betas_and_se(X,y):\n  betas = np.linalg.inv(X.T @ X) @ X.T @ y\n  residuals = y - X @ betas\n  df = X.shape[0] - X.shape[1]\n  res_var = np.sum(residuals**2) / df\n  cov_mat = res_var * np.linalg.inv(X.T @ X)\n  se = np.sqrt(np.diag(cov_mat))\n  return betas, se\n\n\nx1 = np.loadtxt(\"x1.csv\", delimiter = \",\", dtype = float)\nx2 = np.loadtxt(\"x2.csv\", delimiter = \",\", dtype = float)\ny = np.loadtxt(\"y.csv\", delimiter = \",\", dtype = float)\n\n# One way to make a matrix from vectors \n# X = np.concatenate((ones, x1, x2)).reshape((-1,3), order = 'F')\n\n# Alternatively we can make a matrix with column_stack()\nX = np.column_stack((ones, x1, x2))\n\nbetas, se = betas_and_se(X,y)\nprint(\"betas:\", betas)\nbetas: [0.48443399 2.00528572 3.99471469]\nprint(\"SE:\", se)\nSE: [0.00924498 0.00919862 0.00920177]\n\nSandwich Standard Errors\n\n\nbetas_and_std_errors_sandwich = function(X,y){\n  betas = beta_estimator(X,y)\n  ## get the design matrix again\n  X = cbind(rep(1,nrow(X)), X)\n  residuals = y - X %*% betas \n\n  ## Degree of freedom calculation \n  p = ncol(X) - 1 \n  df = nrow(X) - p - 1 \n  \n  \n  ## HC1 or Eicker-Huber_White Variance Estimator \n  ## This is a way of creating a diagonal matrix from a matrix\n  ## with one column in R. \n  u2 = matrix(diag(as.vector(residuals^2)), ncol = nrow(X))\n  beta_cov = (nrow(X)/df) * solve(t(X)%*%X) %*% t(X) %*% u2 %*% X  %*% solve(t(X)%*%X)\n  \n  ## Standard errors are square root of diagonal \n  return(list(beta = betas, se = sqrt(diag(beta_cov))))\n}\n\n## example data \nx1 = read.csv(\"x1.csv\", header = F) |>\n  unlist()\nx2 = read.csv(\"x2.csv\", header = F) |>\n  unlist()\ny = read.csv(\"y.csv\", header = F) |>\n  unlist()\nX = cbind(x1, x2)\n\nbetas_and_std_errors_sandwich(X,y)\n\n$beta\n       [,1]\n   0.484434\nx1 2.005286\nx2 3.994715\n\n$se\n                     x1          x2 \n0.009248444 0.009296868 0.009105095 \n\nIn python\n\ndef betas_and_std_errors_sandwich(X,y):\n  ## Calculate Beta coefficients\n  betas = np.linalg.inv(X.T @ X) @ X.T @ y\n  \n  ## Get residuals, degree of freedom, and squared residuals\n  residuals = y - X @ betas\n  df = X.shape[0] - X.shape[1]\n  u2 = residuals**2\n\n  ## apply the HC1 formula with appropriate correction \n  beta_cov = (X.shape[0]/df) * np.linalg.inv(X.T @ X) @ X.T @ np.diag(u2) @ X @ np.linalg.inv(X.T @ X)\n  se = np.sqrt(np.diag(beta_cov))\n  return betas, se\n\nx1 = np.loadtxt(\"x1.csv\", delimiter = \",\", dtype = float)\nx2 = np.loadtxt(\"x2.csv\", delimiter = \",\", dtype = float)\ny = np.loadtxt(\"y.csv\", delimiter = \",\", dtype = float)\n\nX = np.column_stack((ones, x1, x2))\n\nbetas, se = betas_and_std_errors_sandwich(X,y)\nprint(\"betas:\", betas)\nbetas: [0.48443399 2.00528572 3.99471469]\nprint(\"SE:\", se)\nSE: [0.00924844 0.00929687 0.00910509]\n\nPrincipal Components Analysis\nSuppose we have a collection of points in \\(\\mathbb{R}^n\\) and we want to encode these points to represent a lower-dimensional version of them. If \\(X'\\) is a \\(n \\times p\\) matrix, then the first principal component of \\(X'\\) is the linear combination of the \\(p\\) variables \\(y'_1 = (X-\\bar{X})'a_1\\) s.t \\(V(y_1)'\\) is maximized subject to the constraint that \\(a_1'a_1 =1\\). Subsequent principal components are defined successively in a similar way.\n\n\noptions(scipen=999)\n\nscale_and_center = function(x){\n  ## center columns \n  x_s = x - mean(x)\n  \n  ## return scaled columns \n  return(x_s/sd(x))\n}\n\nprcomp_by_hand = function(A) {\n  ## Calculate mean of each column\n  C = apply(A, 2, scale_and_center)\n  \n  ## Calculate covariance matrix of centered matrix\n  V = cov(C)\n  ## Eigendecomposition of covariance matrix\n  eig = eigen(V, symmetric = F)\n  ## Transpose eigenvectors\n  eig.t = t(eig$vectors)\n  ## calculate new dataset\n  A.new = eig.t %*% t(C)\n  df.new = t(A.new)\n  return(list(points = df.new, vectors = eig$vectors))\n}\n\nresults = prcomp_by_hand(USArrests)\nresults$vectors \n\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\nhead(results$points)\n\n                 [,1]       [,2]        [,3]         [,4]\nAlabama     0.9756604  1.1220012 -0.43980366  0.154696581\nAlaska      1.9305379  1.0624269  2.01950027 -0.434175454\nArizona     1.7454429 -0.7384595  0.05423025 -0.826264240\nArkansas   -0.1399989  1.1085423  0.11342217 -0.180973554\nCalifornia  2.4986128 -1.5274267  0.59254100 -0.338559240\nColorado    1.4993407 -0.9776297  1.08400162  0.001450164\n\nUsing built-in function in R.\n\n\n## As Brian Ripley pointed out on R-help back in 2003\n## using different compilers on the same machine and the same version of R may give different signs for the eigenvectors. \n## The moral is, don't rely on the signs of eigenvectors! \n## (This is on the help page.)\nt = prcomp(USArrests, center = T,scale = T)\nhead(-1*t$rotation)\n\n               PC1        PC2        PC3         PC4\nMurder   0.5358995  0.4181809 -0.3412327 -0.64922780\nAssault  0.5831836  0.1879856 -0.2681484  0.74340748\nUrbanPop 0.2781909 -0.8728062 -0.3780158 -0.13387773\nRape     0.5434321 -0.1673186  0.8177779 -0.08902432\n\nhead(-1*t$x) \n\n                  PC1        PC2         PC3          PC4\nAlabama     0.9756604  1.1220012 -0.43980366 -0.154696581\nAlaska      1.9305379  1.0624269  2.01950027  0.434175454\nArizona     1.7454429 -0.7384595  0.05423025  0.826264240\nArkansas   -0.1399989  1.1085423  0.11342217  0.180973554\nCalifornia  2.4986128 -1.5274267  0.59254100  0.338559240\nColorado    1.4993407 -0.9776297  1.08400162 -0.001450164\n\nIn python\n\nnp.set_printoptions(suppress=True)\n\ndef scale(mat):\n  center = mat - np.mean(mat, axis = 0)\n  scale = center / np.std(mat, axis = 0, ddof = 1)\n  return scale\n\ndef pca(mat):\n  center = mat - np.mean(mat, axis = 0)\n  scale = center / np.std(mat, axis = 0, ddof = 1)\n  cov_mat = np.cov(scale, rowvar = False)\n  vals, vec = np.linalg.eig(cov_mat)\n  \n  ## Sort eigen vectors and eigen values in order \n  idx = (-vals).argsort()\n  vals = vals[idx]\n  vec = vec[:, idx]\n  ## Calculate new dataset \n  A_new = (vec.T @ scale.T).T\n  return vec, A_new\n\n## Test with the same UArrests dataset \n\n## We need to do a bit of cleaning of the raw dataset to \n## turn it into an appropriate matrix\nusarrests = np.loadtxt(\"https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/USArrests.csv\", delimiter = \",\", dtype = str, skiprows = 1)\nusarrests = np.delete(usarrests, (0), axis = 1)\nusarrests = usarrests.astype(dtype = \"float\")\nvec, transformed = pca(usarrests)\nprint('Principal Components:', vec)\nPrincipal Components: [[ 0.53589947  0.41818087 -0.34123273  0.6492278 ]\n [ 0.58318363  0.1879856  -0.26814843 -0.74340748]\n [ 0.27819087 -0.87280619 -0.37801579  0.13387773]\n [ 0.54343209 -0.16731864  0.81777791  0.08902432]]\ntransformed[:6]\narray([[ 0.97566045,  1.12200121, -0.43980366,  0.15469658],\n       [ 1.93053788,  1.06242692,  2.01950027, -0.43417545],\n       [ 1.74544285, -0.73845954,  0.05423025, -0.82626424],\n       [-0.13999894,  1.10854226,  0.11342217, -0.18097355],\n       [ 2.49861285, -1.52742672,  0.592541  , -0.33855924],\n       [ 1.49934074, -0.97762966,  1.08400162,  0.00145016]])\n\n\n\n\n",
      "last_modified": "2023-05-07T18:16:05-07:00"
    }
  ],
  "collections": []
}
