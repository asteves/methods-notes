---
title: "Linear Algebra"
description: |
  Notes on Linear Algebra.
author:
  - name: Alex Stephenson
output:
  distill::distill_article:
    toc: true
    toc_depth: 3
    hightlight: haddock
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This page provides notes on linear algebra with a bias towards results that are particularly useful to doing statistics and machine learning. Most of the notation comes from Greene (2017) and Boyd and Vandenberghe (2018). 

## Matrices {#top}

### Matrix Definition

First we need some terminology. A **matrix** is a rectangular array of numbers. 

$$\begin{aligned}
A &= [a_{jk}]  \\
A &= [A]_{ik} \\
A &= \begin{pmatrix} a_{11} & a_{12} & ... & a_{1k} \\
... & ... & ... & ... \\
a_{n1} & a_{n2} & ... & a_{nk}
\end{pmatrix}
\end{aligned}$$

We always read any subscripted element of a matrix as "a row, column". For example 

$$\begin{aligned}
A &= \begin{pmatrix} 1 & 2 & 3 \\
4 & 5 & 6  \\
7 & 8 & 9 
\end{pmatrix}
\end{aligned}$$

Then $A_{11} = 1$. 

A matrix is made up of a vectors. A vector is an ordered set of numbers arranged either in a row or column. A column vector can be thought of as a matrix with one column. A row vector can be thought of a matrix with one row.^[Unless otherwise noted, a vector is always a column vector.] 

### Square Matrix 

A **square matrix** is a matrix where the number of rows is equal to the number of columns. 

### Symmetric Matrix 

There are a few matrices that are very common in statistics. The first is a **symmetric matrix** which is where $a_{ik} = a_{ki}, \forall i, k$

$$\begin{aligned}
 \begin{pmatrix}
  1 & 2 & 5 \\
  2 & 3 & 6 \\
  5 & 6 & 4
 \end{pmatrix}
\end{aligned}$$ is symmetric. 

### Diagonal Matrix 

A **diagonal matrix** is a square matrix where all off-diagonal elements are 0, and the only non-zero elements are on the main diagonal. 

$$\begin{aligned}
\begin{pmatrix}
  1 & 0 & 0 \\
  0 & 3 & 0 \\
  0 & 0 & 4
 \end{pmatrix}
\end{aligned}$$

### Scalar Matrix 

A **scalar matrix** is a square matrix where every diagonal element is the same. 

$$\begin{aligned}
\begin{pmatrix}
  2 & 0 & 0 \\
  0 & 2 & 0 \\
  0 & 0 & 2
 \end{pmatrix}
\end{aligned}$$

### Triangular

A **triangular matrix** is a matrix that has only zeros either above or below the main diagonal. If the zeros are above the diagonal, it is *lower triangular.* If the zeros are below the diagonal, it is *upper triangular*. 

$$\begin{aligned}
 \begin{pmatrix}
  1 & 0 & 0 \\
  2 & 3 & 0 \\
  5 & 6 & 4
 \end{pmatrix}
\end{aligned}$$

### Identity Matrix 

A special kind of scalar, diagonal, symmetric, and triangular matrix is the **idential matrix**. We always denote this as $\textbf{I}$ with a subscript to denote the number of columns (the order). 

$$\begin{aligned}
\textbf{I}_3\begin{pmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1
 \end{pmatrix}
\end{aligned}$$

[Return to Top](#top)

## Matrix Manipulation 

Two matrices $\textbf{A}$ and $\textbf{B}$ are equal if and only if they have the same dimensions *and* each element of $\textbf{A}$ is the same as each element in $\textbf{B}$. 

For example, 

$$\begin{aligned}
\textbf{A} &= \begin{pmatrix}
  1 & 2 & 5 \\
  2 & 3 & 6 \\
  5 & 6 & 4
 \end{pmatrix} \\
 \textbf{B} &= \begin{pmatrix}
  1 & 2 & 5 \\
  2 & 3 & 6 \\
  5 & 6 & 4
 \end{pmatrix}
\end{aligned}$$

are equal to each other. 

### Transpose 

The transpose of a matrix is denoted $\textbf{A}^T$ or $\textbf{A}^{'}$ is the matrix that we get by rotating the rows and columns of $\textbf{A}$. Formally $\textbf{B} = \textbf{A}^T \iff b_{ik} = a_{ki}$. 

An immediate implication of this definition is that any symmetric matrix $\textbf{A}$ is equal to $\textbf{A}^T$ *and* any matrix $\textbf{A} = (\textbf{A}^{T})^T$. 

$$\begin{aligned}
\textbf{A} &= \begin{pmatrix}
  1 & 2 & 3 \\
  4 & 5 & 6 \\
  7 & 8 & 9
 \end{pmatrix} \\
\textbf{A}^T &= \begin{pmatrix}
  1 & 4 & 7 \\
  2 & 5 & 8 \\
  3 & 6 & 9
 \end{pmatrix}
\end{aligned}$$

### Matrix Addition 

We can add and subtract any two matrices of equal sizes (**conformable**). $\textbf{C} = \textbf{A} + \textbf{B} = [a_{ik} + b_{ik}]$. A special matrix known as the **zero matrix** is one where all elements are zero and can be any order. It plays the same role as zero does in regular addition. 

#### Properties of Matrix Addition 

1. Matrix addition is commutative. $\textbf{A} + \textbf{B} = \textbf{B} + \textbf{A}$

The proof is straight forward. Suppose $\textbf{A}_{m \times n}$ and $\textbf{B}_{m \times n}$. Let $\textbf{C}_{m \times n}$ be the result of adding $\textbf{A} + \textbf{B}$. Let $\textbf{D}_{m \times n} = \textbf{B} + \textbf{A}$. Then:

$$\begin{aligned}
c_{ij} &= a_{ij} + b_{ij} \\
d_{ij} &= b_{ij} + a_{ij} \\
c_{ij} &= d_{ij}
\end{aligned}$$

2. Matrix addition is associative. $(\textbf{A} + \textbf{B}) + \textbf{B} = \textbf{A} + (\textbf{B} + \textbf{C})$

The proof is slightly more involved, but only slightly. Assume all three matrices are the same order. Let $\textbf{D} = \textbf{B} + \textbf{C}$, $\textbf{E} = \textbf{A} + \textbf{B}$, $\textbf{F} = \textbf{A} + \textbf{D}$, and $\textbf{G} = \textbf{E} + \textbf{C}$. Then: 

$$\begin{aligned}
d_{ij} &= b_{ij} + c_{ij} \\
e_{ij} &= a_{ij} + b_{ij} \\
f_{ij} &= a_{ij} + d_{ij} \rightarrow a_{ij} + b_{ij} + c_{ij}\\
g_{ij} &= e_{ij} + c_{ij} \rightarrow a_{ij} + b_{ij} + c_{ij}
\end{aligned}$$. 

We observe that both the new matrices **F** and **G** are the same order and that their elements are the same, completing the proof. 

3. Matrix addition with transposes follows $(\textbf{A} +\textbf{B})^T = \textbf{A}^T + \textbf{B}^T$. 

The proof is straight forward.  The $(i,j)$ entry of $(\textbf{A} +\textbf{B})^T$ is the sum of $(i,j)$ entries of $\textbf{A}^T + \textbf{B}^T$. which are $(j,i)$ entries of **A** and **B**. That means that the $(i,j)$ entry of $\textbf{A}^T + \textbf{B}^T$ is the $(j,i)$ entry of the sum of $\textbf{A} +\textbf{B}$ , which is equal to the $(i,j)$ entry of the transpose $(\textbf{A} +\textbf{B})^T$.  

### Inner Product 

We multiply matrices and vector by using the inner product (also known as the dot product). The **inner product** of two vectors **a** and **b** is $a^Tb = a_1b_1 + ... + a_nb_n$. Of course provided they are conformable it is also the case that $a^Tb = b^Ta$. 

### Matrix Multiplication 

For an $n \times k$ matrix **A** and a $k \times m$ matrix **B**, the product matrix $\textbf{C} = \textbf{A}\textbf{B}$ is a $n \times m$ matrix whose $ik^{th}$ element is the inner product of row *i* in **A** and column *k* in **B**. 

$$\begin{aligned}
\textbf{C} = \textbf{A}\textbf{B} \rightarrow c_{ik} = a_i^Tb_k
\end{aligned}$$

Matrices can only be multiplied if they are conformable. In simple terms, the number of columns in **A** must be equal to the number of rows in **B**. It is not generally the case that matrix multiplication is commutative. For example consider the two matrices: 

$$\begin{aligned}
A &= \begin{pmatrix}
1 & 2 \\ 
3 & 4 
\end{pmatrix} \\

B &= \begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{pmatrix}
\end{aligned}$$

It is possible to multiply **AB**, but not possible to multiply **BA** because the latter has non-conformable arguments. Even when **A** and **B** have the same dimensions, it is generally the case that they will not be equal. Here's a simple example. 

$$\begin{aligned}
AB &= \begin{pmatrix}
1 & 2 \\ 
3 & 4 
\end{pmatrix} 
\begin{pmatrix}
 5 & 6 \\
 7 & 8
\end{pmatrix} = \begin{pmatrix} 19 & 22 \\ 43 &50 \end{pmatrix} \\
BA &= \begin{pmatrix}
5 & 6 \\ 
7 & 8 
\end{pmatrix} 
\begin{pmatrix}
1 & 2 \\
4 & 5 
\end{pmatrix} = \begin{pmatrix} 23 & 34 \\ 31 & 46 \end{pmatrix}
\end{aligned}$$

Because it is not the case that matrix multiplication is commutative, we have two different ways of multiplying. We can **premultiply** or **postmultiply**. In **AB**, **B** is *premultiplied* by **A** and **A** is *postmultiplied* by **B**. 

### Scalar Multiplication 

For any matrix (or vector), **scalar multiplication** is the operation of multiplying every element by a given scalar $c\textbf{A} = [ca_{ik}]$. 

The product of a matrix and a vector is written $\textbf{c} = \textbf{A}\textbf{b}$. In order to perform this operation, the number of elements in **b** must be equal to the number of columns in **A**. It's the equivalent of multiplying a $n \times k$ matrix by a $k \times 1$ matrix.

#### Properties of Matrix Multiplication 

1. Associative Law: **(AB)C = A(BC)**

Proof: 

2. Distributive Law: **A(B + C) = AB + AC**

Proof: 

3. Transpose of a Product: **(AB)' = B'A'** 

Proof:

4. Transpose of an extended product: **(ABC)' = C'B'A'**

Proof:

### Vector Sums 

There are some common summations that are useful to see in matrix form.

Let **i** be a column vector of ones. $\textbf{ix} = \sum_{i=1}^nx_i$.  

For any constant *c* then $\textbf{x} = c\textbf{i} = \sum_{i=1}^ncx_i = c\sum_{i=1}^nx_i = a\textbf{i}^T\textbf{x}$. 
The *sum of squares* of the elements of **x** is $\textbf{x}^T\textbf{x}$. 

### Centering Matrix 

A fundamental matrix that we use to transform data to deviations from their mean is the *centering matrix* 

$$\begin{aligned}
\textbf{i}\bar{x} &= \textbf{i}\frac{1}{n}\textbf{i}^T\textbf{x} \\
\textbf{i}\bar{x} &= \begin{pmatrix} \bar{x} \\ \bar{x} \\\cdot \\ \cdot \\ \bar{x}\end{pmatrix} \\
\textbf{i}\bar{x} &= \frac{1}{n}\textbf{i}\textbf{i}^T\textbf{x}
\end{aligned}$$

The first matrix $\frac{1}{n}ii^T$ is an $n \times n$ matrix with every element equal to $\frac{1}{2}$. This means that the set of values is $\textbf{x} - \frac{1}{2}ii^Tx$. 

Since $x = \mathbb{I}x$ this implies that we can write this as $[\mathbb{I} - \frac{1}{n}ii^T]x = \textbf{M}^0\textbf{x}$. $M^0$ is an idempotent matrix, which means that it is equal to its square, and it is symmetric so $(M^0)^TM^0 = M^0$. 

The matrix is particularly useful when computing sums of squared deviations. For example, for a single variable **x** the sum of squared deviations about the mean is $\sum_{i=1}^n(x_i - \bar{x})^2 = (\sum_{i=1}^nx_i^2) - n\bar{x}^2$. 

The equivalent in matrix notation is: 

$$\begin{aligned}
\sum_{i=1}^n(x_i - \bar{x})^2 &= (\textbf{x} - \bar{x}\textbf{i})(\textbf{x} - \bar{x}\textbf{i}) \\
\sum_{i=1}^n(x_i - \bar{x})^2 &= (M^0x)^T(M^0x) \\
\sum_{i=1}^n(x_i - \bar{x})^2 &= x^TM^0M^0x \\
\sum_{i=1}^n(x_i - \bar{x})^2 &= x^TM^0x
\end{aligned}$$

## Matrix Geometry 

### Vector Spaces 



