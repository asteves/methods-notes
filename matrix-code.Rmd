---
title: "Linear Algebra Code"
description: |
  A Crash Course in Linear Algebra using R and Python
author: 
  - name: Alex Stephenson
output:
  distill::distill_article:
    toc: true
    toc_depth: 3
    hightlight: haddock
site: distill::distill_website
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This page provides code in R and Python for doing linear algebra. In Python, we make use of the [NumPy](https://numpy.org/doc/stable/index.html) library and the PyTorch library.

```{python}
import numpy as np 
import torch
```



For tensor processing in R, we'll use the R port of `torch` 

```{r}
library(torch)
```

## Vectors 

The simplest way to represent vectors in R is by using the vector data structure. 

```{r}
x = c(-1.1, 0.0, 3.6, -7.2)
length(x) ## 4 
```

In python:

```{python}
x = np.array([-1.1, 0.0, 3.6, -7.2])
len(x)
```

### Block and stacked vectors 

In addition to creating vectors, we can concatenate vectors together to produce blocked and stacked vectors using the `c()` function.

```{r}
x = c(1,-2)
y = c(1,1,0)
z = c(x,y)
z
```

In python: 

```{python}
x = np.array([1,-2])
y = np.array([1,1,0])
z = np.concatenate((x,y))
print(z)

```

### Some special vectors 

The Zeros vector is a default behavior of creating a vector with a given length. 

```{r}
z = numeric(3)
z
```

In python: 

```{python}
z = np.zeros(3)
z

```


The Ones vector can be made by way of the `rep()` function. 

```{r}
o = rep(1,3)
o
```

In python: 

```{python}
o = np.ones(3)
o
```


## Vector Addition and Multiplication 

If x and y are vectors of the same size, then x+y and x-y give their element wise sum and difference respectively. R by default computes most vector operations element wise. 

```{r}
x = c(1,2,3)
y = c(100,200,300)
x+y
```

In python: 

```{python}
x = np.array([1,2,3])
y = np.array([100,200,300])
x + y
np.add(x,y)
```


### Scalar Multiplication and division 

If a is a number and x is a vector, then we can express the scalar vector product as either `a*x` or `x*a`

```{r}
a = 2 
x = c(1,2,3)
a*x
x*a
```

In python: 

```{python}
a = 2
x = np.array([1,2,3])
a*x
x*a
```


### Using what we've learned to confirm the distributive property 

The distributive property $\beta(a+b) = \beta a + \beta b$ holds for any two n-vector *a* and *b* and any scalar $\beta$. 

```{r}
a = c(3,5,6)
b = c(2,4,9)
beta = 5
lhs = beta*(a+b)
rhs = beta*a + beta*b
print(lhs)
print(rhs)
lhs == rhs
```

In python: 

```{python}
a = np.array([3,5,6])
b = np.array([2,4,9])
beta = 5 
lhs = beta*(a+b)
rhs = beta*a + beta*b
print('lhs:', lhs)
print('rhs:', rhs)
lhs == rhs
```


### Inner Product 

The inner product of n-vector x and y is denoted $x^Ty$ 

```{r}
x = c(1,2,3,4)
y = c(3,4,6,7)

## t() is the transpose function in R
t(x)%*% y
```

In python: 

```{python}
x = np.array([1,2,3,4])
y = np.array([3,4,6,7])
np.inner(x,y) 

# Alternatively 
x @ y
```


## Matrices 

A matrix $\textbf{X}$ is an $m$ x $n$ data structure that is a rectangular array of scalar numbers. The numbers $x_{ij}$ are components or elements of $\textbf{X}$. The transpose of a matrix is the $n$ x $m$ matrix $\textbf{X}'$

```{r}
## Creating a matrix in R 
X = matrix(seq(1,16,1), 
           nrow = 4, 
           byrow = T)
X
```
We can also create matrices from vectors or from data frames

```{r}
## Equivalent to above but with vectors 
X2 = rbind(1:4, 5:8,9:12,13:16)
X2
```

```{r}
## via a data frame 
df = data.frame(
  x = 1:4,
  y = 5:8,
  z = 9:12,
  w = 13:16
)
X3 = as.matrix(df)
X3
```

In python: 

```{python}
X = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12], [13,14,15,16]])
X
X.shape
```

Some other useful matrices 

```{python}
np.identity(4) 
np.zeros((4,4))
np.ones((4,4))
```

### Additional Definitions 

The transpose in R 

```{r}
X_transpose = t(X)
X_transpose
```

In python: 

```{python}
X_transpose = X.T
X_transpose
```
A diagonal matrix with all elements not on the diagonal equal to zero is a diagonal matrix. By default, R creates an identity matrix with the `diag()` function.

```{r}
dM = diag(4)
dM
```

The trace of a matrix is the sum of the diagonal elements 

$$trace(X) = \sum_{i=1}^n x_{ii}$$ 

```{r}
matrix_trace = function(mat){
  return(sum(diag(mat)))
}

matrix_trace(X)
```


### Matrix Arithmetic 

Addition and subtraction of matrices of the same order are performed element by element. Scalar multiplication is element by element 

```{r}
A = matrix(data = seq(1,9,1), nrow = 3, byrow = T)
B = matrix(data = seq(1,9,1), nrow = 3, byrow = T)
A+B

```

Provided that the number of columns of A equals the number of rows of B, we can multiple A by B. 

```{r}
## To get the appropriate multiplication, we wrap * in %*%
A%*%B
```

Note that we can perform X'X in one of two ways. 

```{r}
t(A)%*%A 

## same but can be slightly faster 
crossprod(A)

```

In python: 

```{python}
A = np.array([[1,2,3], [4,5,6], [7,8,9]])
B = np.array([[1,2,3], [4,5,6], [7,8,9]])
A+B
np.add(A,B)

# Multiplication 
np.matmul(A,B)

np.matmul(A.T, A)
```
## Norms 

Informally, the norm of a vector tells us how *big* it is. Formally, a norm is a function $\lVert \cdot \rVert$ that maps a vector to a scalar which satisfies:

1. Given any vector **x** $\lVert \alpha x \rVert = \lVert \alpha \rVert \lvert x \rVert$. 
2. For any vectors **x**, **y**, norms satisfy the triangle inequality $\lVert x + y \rVert \leq \lVert x \rVert + \lVert y \rVert$
3. The norm of a vector $\lVert x \rVert > 0,\forall x \neq 0$

The most common norm is the $\ell_2$ norm or Euclidean norm which is defined as $\lVert x \rVert_2 = \sqrt{\sum_{i=1}^n x_i^2}$

```{r}
x = c(3,-4)
l2 = sqrt(crossprod(x,x))
l2

## We can also call R's built in norm() function 
norm(x, "2")
```
In python 

```{python}
x = np.array([3,-4])
np.linalg.norm(x)
```
We sometimes take the Manhattan norm ($\ell_1$) which sums the absolute values of a vectors elements $\lVert x \rVert_1 = \sum_{i=1}^n|x_i|$

```{r}
x = c(3,-4)
l1 = function(x){
  return(sum(abs(x)))
}
l1(x)

```
In python 

```{python}
x = np.array([3,-4])
np.linalg.norm(x, 1)

```

The infinity norm or max norm ($\ell_\infty$) is common in machine learning applications defined as $\lVert x \rVert_\infty = \max_i |x_i|$, which simplifies to the absolute value of the element with the largest magnitude in the vector. 

```{r}
x = matrix(c(3,-4), byrow = T, nrow = 2)
norm(x, type ="I")

l_inf = function(x){
  return(max(abs(x)))
}
l_inf(c(3,-4))
```
In python 

```{python}
x = np.array([3,-4])
max(abs(x))
```

## Ordinary Least Squares

The simplest linear model expresses the dependence of a dependent or response variable y on independent variables $x_1,.., x_p$ and is usually written $y = X\beta + \epsilon$. See the [Lecture Notes](lectures.html) for more details on the properties of this model. 

Define the design matrix as the $n \times p$ matrix of independent variables $x_1,..,x_p$ and assume that the first columns is a column of ones and that the design matrix has full rank. Then the usual OLS estimator is defined as $(X'X)^{-1}X'Y$

```{r}
beta_estimator = function(X,y){
  X = cbind(rep(1,nrow(X)), X)
  betas = solve(t(X)%*%X)%*%t(X)%*%y
  return(betas)
}

## example data 
set.seed(123)
x1 = rnorm(10000)
x2 = rnorm(10000)
y = 2*x1 + 4*x2 + runif(10000)

X = cbind(x1, x2)
beta_estimator(X,y)

```
 
In python: 

```{python}
x1 = np.random.default_rng(seed=123).normal(0, 1, size =1000)
x2 = np.random.default_rng().normal(0, 1, size =1000)
ones = np.ones(1000)
y = 2*x1 + 4*x2 + np.random.default_rng().uniform(size = 1000)

X = np.concatenate((ones, x1, x2)).reshape((-1,3), order = 'F')

# Alternatively we can make a matrix with column_stack()
X = np.column_stack((ones, x1, x2))

# (X'X)^-1X'y
np.linalg.inv(X.T @ X) @ X.T @ y
```

## Covariance Matrices and Standard Errors

### Classical Standard Errors 

The least squares solution gives us point estimates for coefficients, but if we want to do inference, we need to get standard errors. See the [Lecture Notes](lectures.html) for more details. 

To get standard errors, we must first calculate the covariance matrix of our estimates and then take the square root of the diagonal. 

```{r}
beta_estimator = function(X,y){
  X = cbind(rep(1,nrow(X)), X)
  betas = solve(t(X)%*%X)%*%t(X)%*%y
  return(betas)
}

betas_and_std_errors = function(X,y){
  betas = beta_estimator(X,y)
  ## get the design matrix again
  X = cbind(rep(1,nrow(X)), X)
  residuals = y - X %*% betas 
  
  ## Degree of freedom calculation 
  p = ncol(X) - 1 
  df = nrow(X) - p - 1 
  
  ## Residual variance 
  res_var = sum(residuals^2) / df 
  
  ## Covariance matrix of estimate 
  ## cov(\hat{\beta}|X) = (X'X)^-1X'cov(\epsilon|X)X(X'X)^-1
  beta_cov = res_var * solve(t(X)%*%X)
  
  ## Standard errors are square root of diagonal 
  return(list(beta = betas, se = sqrt(diag(beta_cov))))
}

## example data 
## To keep consistent with python examples I use pre-generated 
## random variables created in python with:
# x1 = np.random.default_rng(seed=123).normal(0, 1, size =1000)
# x2 = np.random.default_rng().normal(0, 1, size =1000)
# ones = np.ones(1000)
# y = 2*x1 + 4*x2 + np.random.default_rng().uniform(size = 1000)

x1 = read.csv("x1.csv", header = F) |>
  unlist()
x2 = read.csv("x2.csv", header = F) |>
  unlist()
y = read.csv("y.csv", header = F) |>
  unlist()
X = cbind(x1, x2)

betas_and_std_errors(X,y)
```

In python 

```{python}
def betas_and_se(X,y):
  betas = np.linalg.inv(X.T @ X) @ X.T @ y
  residuals = y - X @ betas
  df = X.shape[0] - X.shape[1]
  res_var = np.sum(residuals**2) / df
  cov_mat = res_var * np.linalg.inv(X.T @ X)
  se = np.sqrt(np.diag(cov_mat))
  return betas, se


x1 = np.loadtxt("x1.csv", delimiter = ",", dtype = float)
x2 = np.loadtxt("x2.csv", delimiter = ",", dtype = float)
y = np.loadtxt("y.csv", delimiter = ",", dtype = float)

# One way to make a matrix from vectors 
# X = np.concatenate((ones, x1, x2)).reshape((-1,3), order = 'F')

# Alternatively we can make a matrix with column_stack()
X = np.column_stack((ones, x1, x2))

betas, se = betas_and_se(X,y)
print("betas:", betas)
print("SE:", se)
```

### Sandwich Standard Errors 

```{r}
betas_and_std_errors_sandwich = function(X,y){
  betas = beta_estimator(X,y)
  ## get the design matrix again
  X = cbind(rep(1,nrow(X)), X)
  residuals = y - X %*% betas 

  ## Degree of freedom calculation 
  p = ncol(X) - 1 
  df = nrow(X) - p - 1 
  
  
  ## HC1 or Eicker-Huber_White Variance Estimator 
  ## This is a way of creating a diagonal matrix from a matrix
  ## with one column in R. 
  u2 = matrix(diag(as.vector(residuals^2)), ncol = nrow(X))
  beta_cov = (nrow(X)/df) * solve(t(X)%*%X) %*% t(X) %*% u2 %*% X  %*% solve(t(X)%*%X)
  
  ## Standard errors are square root of diagonal 
  return(list(beta = betas, se = sqrt(diag(beta_cov))))
}

## example data 
x1 = read.csv("x1.csv", header = F) |>
  unlist()
x2 = read.csv("x2.csv", header = F) |>
  unlist()
y = read.csv("y.csv", header = F) |>
  unlist()
X = cbind(x1, x2)

betas_and_std_errors_sandwich(X,y)
```

In python 

```{python}
def betas_and_std_errors_sandwich(X,y):
  ## Calculate Beta coefficients
  betas = np.linalg.inv(X.T @ X) @ X.T @ y
  
  ## Get residuals, degree of freedom, and squared residuals
  residuals = y - X @ betas
  df = X.shape[0] - X.shape[1]
  u2 = residuals**2

  ## apply the HC1 formula with appropriate correction 
  beta_cov = (X.shape[0]/df) * np.linalg.inv(X.T @ X) @ X.T @ np.diag(u2) @ X @ np.linalg.inv(X.T @ X)
  se = np.sqrt(np.diag(beta_cov))
  return betas, se

x1 = np.loadtxt("x1.csv", delimiter = ",", dtype = float)
x2 = np.loadtxt("x2.csv", delimiter = ",", dtype = float)
y = np.loadtxt("y.csv", delimiter = ",", dtype = float)

X = np.column_stack((ones, x1, x2))

betas, se = betas_and_std_errors_sandwich(X,y)
print("betas:", betas)
print("SE:", se)
```


## Principal Components Analysis 

Suppose we have a collection of points in $\mathbb{R}^n$ and we want to encode these points to represent a lower-dimensional version of them. If $X'$ is a $n \times p$ matrix, then the first principal component of $X'$ is the linear combination of the $p$ variables $y'_1 = (X-\bar{X})'a_1$ s.t $V(y_1)'$ is maximized subject to the constraint that $a_1'a_1 =1$. Subsequent principal components are defined successively in a similar way. 

```{r}
options(scipen=999)

scale_and_center = function(x){
  ## center columns 
  x_s = x - mean(x)
  
  ## return scaled columns 
  return(x_s/sd(x))
}

prcomp_by_hand = function(A) {
  ## Calculate mean of each column
  C = apply(A, 2, scale_and_center)
  
  ## Calculate covariance matrix of centered matrix
  V = cov(C)
  ## Eigendecomposition of covariance matrix
  eig = eigen(V, symmetric = F)
  ## Transpose eigenvectors
  eig.t = t(eig$vectors)
  ## calculate new dataset
  A.new = eig.t %*% t(C)
  df.new = t(A.new)
  return(list(points = df.new, vectors = eig$vectors))
}

results = prcomp_by_hand(USArrests)
results$vectors 
head(results$points)
```

Using built-in function in R.

```{r}
## As Brian Ripley pointed out on R-help back in 2003
## using different compilers on the same machine and the same version of R may give different signs for the eigenvectors. 
## The moral is, don't rely on the signs of eigenvectors! 
## (This is on the help page.)
t = prcomp(USArrests, center = T,scale = T)
head(-1*t$rotation)
head(-1*t$x) 
```


In python 

```{python}
np.set_printoptions(suppress=True)

def scale(mat):
  center = mat - np.mean(mat, axis = 0)
  scale = center / np.std(mat, axis = 0, ddof = 1)
  return scale

def pca(mat):
  center = mat - np.mean(mat, axis = 0)
  scale = center / np.std(mat, axis = 0, ddof = 1)
  cov_mat = np.cov(scale, rowvar = False)
  vals, vec = np.linalg.eig(cov_mat)
  
  ## Sort eigen vectors and eigen values in order 
  idx = (-vals).argsort()
  vals = vals[idx]
  vec = vec[:, idx]
  ## Calculate new dataset 
  A_new = (vec.T @ scale.T).T
  return vec, A_new

## Test with the same UArrests dataset 

## We need to do a bit of cleaning of the raw dataset to 
## turn it into an appropriate matrix
usarrests = np.loadtxt("https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/USArrests.csv", delimiter = ",", dtype = str, skiprows = 1)
usarrests = np.delete(usarrests, (0), axis = 1)
usarrests = usarrests.astype(dtype = "float")
vec, transformed = pca(usarrests)
print('Principal Components:', vec)
transformed[:6]

```

## Tensors 

Tensors are generic $n^{th}$ order arrays. Vectors are a 1st order tensor. Matrices are a second order tensor. 

```{r, eval = F}
t1 = torch_tensor(1)
```

In python using `PyTorch`

```{python}
t1 = torch.tensor(1.0)
t1
```